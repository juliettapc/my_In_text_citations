{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import gzip\n",
    "import os,glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import random\n",
    "from  scipy import stats\n",
    "#sys.path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MongoConnection(object):\n",
    "    def __init__(self, cxnSettings, **kwargs):\n",
    "        self.settings = cxnSettings\n",
    "        self.mongoURI = self._constructURI()\n",
    "        self.connect(**kwargs)\n",
    "        self.ensure_index()\n",
    "\n",
    "    def _constructURI(self):\n",
    "        '''\n",
    "        Construct the mongo URI\n",
    "        '''\n",
    "        mongoURI = 'mongodb://'\n",
    "        #User/password handling\n",
    "        if 'user'in self.settings and 'password' in self.settings:\n",
    "            mongoURI += self.settings['user'] + ':' + self.settings['password']\n",
    "            mongoURI += '@'\n",
    "        elif 'user' in self.settings:\n",
    "            print('Missing password for given user, proceeding without either')\n",
    "        elif 'password' in self.settings:\n",
    "            print('Missing user for given passord, proceeding without either')\n",
    "        #Host and port\n",
    "        try:\n",
    "            mongoURI += self.settings['host'] + ':'\n",
    "        except KeyError:\n",
    "            print('Missing the hostname. Cannot connect without host')\n",
    "            sys.exit()\n",
    "        try:\n",
    "            mongoURI += str(self.settings['port'])\n",
    "        except KeyError:\n",
    "            print('Missing the port. Substituting default port of 27017')\n",
    "            mongoURI += str('27017')\n",
    "        return mongoURI\n",
    "\n",
    "    def connect(self, **kwargs):\n",
    "        '''\n",
    "        Establish the connection, database, and collection\n",
    "        '''\n",
    "        self.connection = MongoClient(self.mongoURI, **kwargs)\n",
    "        #########\n",
    "        try:\n",
    "            self.db = self.connection[self.settings['db']]\n",
    "        except KeyError:\n",
    "            print(\"Must specify a database as a 'db' key in the settings file\")\n",
    "            sys.exit()\n",
    "        #########\n",
    "        try:\n",
    "            self.collection = self.db[self.settings['collection']]\n",
    "        except KeyError:\n",
    "            print('Should have a collection.', end='')\n",
    "            print('Starting a collection in database', end='')\n",
    "            print(' for current connection as test.')\n",
    "            self.collection = self.db['test']\n",
    "\n",
    "    def tearDown(self):\n",
    "        '''\n",
    "        Closes the connection\n",
    "        '''\n",
    "        self.connection.close()\n",
    "\n",
    "    def ensure_index(self):\n",
    "        '''\n",
    "        Ensures the connection has all given indexes.\n",
    "        indexes: list of (`key`, `direction`) pairs.\n",
    "            See docs.mongodb.org/manual/core/indexes/ for possible `direction`\n",
    "            values.\n",
    "        '''\n",
    "        if 'indexes' in self.settings:\n",
    "            for index in self.settings['indexes']:\n",
    "                self.collection.ensure_index(index[0], **index[1])\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "merged_papers_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"merged_papers\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "\n",
    "papers_con = MongoConnection(merged_papers_settings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "\n",
    "dais_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"ut_dais_all\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "\n",
    "dais_con = MongoConnection(dais_settings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = papers_con.collection.find({\"UT\":\"000429763700010\"})  \n",
    "#for s in cursor:\n",
    "    #print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# cursor = papers_con.collection.find().sort([(\"issue.PY\",-1)]).limit(10) # i look for the 10 youngest papers (write +1 instead of -1 for the oldest)\n",
    "# # for s in cursor:\n",
    "# #     print(s)\n",
    "# #     print('--------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                          # df_reference_cite_plos_merged     (original with ALL columns and all entries, without removing any references)\n",
    "\n",
    "#%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl', 'rb'))\n",
    "\n",
    "#%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns_no_self-cit_one_ref_per_sect_ONLY_ARTICLES.pkl', 'rb'))\n",
    "%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_with_team_expertise_and_recycled_ref.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"done loading pickles\", df_merged.shape)\n",
    "\n",
    "\n",
    "#%time plos_df = pickle.load(open('../data/plos_paper_dataframe_more_columns.pkl', 'rb'))\n",
    "\n",
    "#%time plos_df = pickle.load(open('../data/plos_paper_dataframe_ONLY_ARTICLES.pkl', 'rb'))\n",
    "%time plos_df = pickle.load(open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect_young_old.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "#%time plos_df = pickle.load(open('../data/plos_paper_dataframe.pkl', 'rb'))\n",
    "print (\"done loading pickles\", plos_df.shape)\n",
    "plos_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### OJOOOO!new!! if i load the original df, i need to remove duplicate references used more than once per section and per plos paper:  (numbers go down from 8.9M to 6.9M)\n",
    "# print (df_merged.shape)  \n",
    "# df_merged=df_merged.drop_duplicates(subset=['paper_UT','reference_UT','reference_rank','regex_sect_index','cite_count','ref_pub_year','paper_cite_count','total_refs','plos_pub_year','plos_field'])\n",
    "# print (df_merged.shape)\n",
    "\n",
    "\n",
    "print (sorted(df_merged.columns))\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plos_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(df_merged.columns)\n",
    "sorted(plos_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_plosone = df_merged[df_merged['plos_j1']==\"PLOS ONE\"]\n",
    "# list_plos_one = list(df_plosone.paper_UT.unique())\n",
    "# len(list_plos_one)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# done loading pickles (6924269, 23)\n",
    "# ['cite_count',\n",
    "#  'diff_year_plos_ref',\n",
    "#  'isolated_citation',\n",
    "#  'log10_num_cit_paper',\n",
    "#  'log10_num_cit_ref',\n",
    "#  'log2_num_cit_paper',\n",
    "#  'log2_num_cit_ref',\n",
    "#  'occurence',\n",
    "#  'paper_UT',\n",
    "#  'paper_cite_count',\n",
    "#  'plos_field',\n",
    "#  'plos_j1',\n",
    "#  'plos_pub_year',\n",
    "#  'ref_field',\n",
    "#  'ref_j1',\n",
    "#  'ref_pub_year',\n",
    "#  'reference_UT',\n",
    "#  'reference_rank',\n",
    "#  'regex_sect_index',\n",
    "#  'rel_loc_in_sect',\n",
    "#  'sect_char_pos',\n",
    "#  'sect_char_total',\n",
    "#  'total_refs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_plos = list(df_merged.paper_UT.unique())\n",
    "len(list_plos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################  i add the plos one category (coded as numbers) to those rows corresponding to plos one paper_UTs\n",
    "\n",
    "\n",
    "try:\n",
    "    \n",
    "    dict_UT_plos_categ = pickle.load(open('../data/dict_UT_plos_categ.pkl', 'rb'))\n",
    "    print (\"loaded pickle dict\", len(dict_UT_plos_categ))\n",
    "\n",
    "except:\n",
    "    \n",
    "    cont_found_categ=0\n",
    "    start=0\n",
    "    stop=1000\n",
    "    dict_UT_plos_categ={}\n",
    "    while stop <=159000:\n",
    "\n",
    "\n",
    "        lista=list_plos[start:stop]\n",
    "        cursor = papers_con.collection.find({\"UT\":{\"$in\":lista}},{\"UT\":1,\"plos_categories\":1}, no_cursor_timeout=True)  \n",
    "\n",
    "\n",
    "        print (start, stop)\n",
    "\n",
    "        for item in cursor:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict\n",
    "\n",
    "            UT=item[\"UT\"]\n",
    "            try:\n",
    "                list_subjects=item['plos_categories']\n",
    "                cont_found_categ +=1\n",
    "            except KeyError:\n",
    "                list_subjects =[]\n",
    "            dict_UT_plos_categ[UT]=list_subjects\n",
    "\n",
    "        stop +=1000\n",
    "        start +=1000\n",
    "\n",
    "\n",
    "        cursor.close()  # because i am using the no_cursor_timeout=True, i need also this, or cursor keeps waiting so ur resources are used up\n",
    "\n",
    "    print (\"done, num. UTs with category info:\", cont_found_categ, \"   tot UTs\", len(dict_UT_plos_categ))\n",
    "\n",
    "\n",
    "    with open('../data/dict_UT_plos_categ.pkl', 'wb') as handle:\n",
    "         pickle.dump(dict_UT_plos_categ, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/dict_UT_plos_categ.pkl')   \n",
    "\n",
    "\n",
    "    \n",
    "##########################    \n",
    "    \n",
    "    \n",
    "    \n",
    "lista_values = dict_UT_plos_categ.values()\n",
    "lista_diff_categ=[]\n",
    "for item in lista_values:\n",
    "    lista_diff_categ += item\n",
    "\n",
    "lista_diff_categ = sorted(list(set(lista_diff_categ)))\n",
    "print (len(lista_diff_categ), lista_diff_categ)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "code = 0\n",
    "dict_categ_code={}\n",
    "for categ in lista_diff_categ:\n",
    "    dict_categ_code[categ] = code\n",
    "    code +=1    \n",
    "print (dict_categ_code)\n",
    "with open('../data/dict_categ_code.pkl', 'wb') as handle:\n",
    "     pickle.dump(dict_categ_code, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_categ_code.pkl')   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_dict_UT_plos_categ_codes={}\n",
    "for UT in dict_UT_plos_categ:\n",
    "    new_combined_code=''\n",
    "    for categ in dict_UT_plos_categ[UT]:\n",
    "        code = dict_categ_code[categ]\n",
    "        new_combined_code += (str(code) +\" \")\n",
    "        \n",
    "    new_dict_UT_plos_categ_codes[UT] = new_combined_code.strip()  # i just remove the last space\n",
    "    \n",
    "print (len(new_dict_UT_plos_categ_codes))\n",
    "\n",
    "\n",
    "\n",
    "###################### I convert the info UT-categ into a dataframe for merging with the df_merged one:\n",
    "\n",
    "lista_UTs=[]\n",
    "lista_categ_codes=[]\n",
    "for UT in new_dict_UT_plos_categ_codes:\n",
    "    lista_UTs.append(UT)\n",
    "    lista_categ_codes.append(new_dict_UT_plos_categ_codes[UT])\n",
    "   \n",
    "data_tuples = list(zip(lista_UTs,lista_categ_codes))\n",
    "df_categ = pd.DataFrame(data_tuples, columns=['paper_UT','categ_codes'])\n",
    "\n",
    "path = '../data/df_UT_plosone_categ_codes.pkl'\n",
    "%time df_categ.to_pickle(path, compression='infer', protocol=2)\n",
    "print (\"written little df with plos\")\n",
    "\n",
    "\n",
    "##########################\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_categ, on='paper_UT', how='left')\n",
    "df_merged.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "%time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "print (\"written:\",path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_categ_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fix_paper_UT(row):\n",
    "    paper_UT = row.paper_UT\n",
    "    return '000'+str(paper_UT)\n",
    "\n",
    "#df_testing = df_merged.head(1000)\n",
    "\n",
    "df_merged['new_paper_UT'] = df_merged.apply (lambda row: fix_paper_UT(row),axis=1)\n",
    "df_merged.drop(['paper_UT'], axis=1, inplace=True)\n",
    "df_merged.rename(columns={'new_paper_UT': 'paper_UT'}, inplace=True)\n",
    "\n",
    "df_merged[['paper_UT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged[['paper_UT']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_testing=df_merged.copy(deep=True)\n",
    "# print (df_merged.shape, df_testing.shape)\n",
    "\n",
    "\n",
    "# ###### if i upload the original merged df, then i need to select some columns, or it will be too long\n",
    "# print (sorted(df_merged.columns)) \n",
    "# df_merged = df_merged[['occurence','paper_UT','reference_UT','reference_rank','regex_sect_index','cite_count','ref_pub_year',\\\n",
    "#                                   'paper_cite_count','total_refs','plos_pub_year' , 'sect_char_pos', 'sect_char_total' , 'plos_field','plos_j1','ref_field', 'ref_j1', ]]\n",
    "# df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lista_all_plos_UTs = pickle.load(open('../data/lista_all_plos_UTs.pkl', 'rb'))\n",
    "new_lista_all_plos_UTs = ['000'+str(item) for item in lista_all_plos_UTs]   # i need to add the 000 so it matches the db UTs\n",
    "\n",
    "print (len(lista_all_plos_UTs))\n",
    "\n",
    "lista_all_reference_UTs = pickle.load(open('../data/lista_all_reference_UTs.pkl', 'rb'))\n",
    "print (len(lista_all_reference_UTs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_UTs_plos_and_ref = new_lista_all_plos_UTs+ lista_all_reference_UTs\n",
    "lista_UTs_plos_and_ref =  list(set(lista_UTs_plos_and_ref))\n",
    "len(lista_UTs_plos_and_ref)   #2706287 without UT repetitions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_num_cit_of_ref(row):\n",
    "    \n",
    "    try:\n",
    "        log_num_cit=np.log10(float(row.cite_count))\n",
    "    except:\n",
    "        print (row.plos_pub_year)\n",
    "        log_num_cit_ref=0.\n",
    "    return log_num_cit\n",
    "\n",
    "###################\n",
    "\n",
    "def get_log_num_cit_of_paper(row):\n",
    "    \n",
    "    try:\n",
    "        log_num_cit=np.log10(float(row.paper_cite_count))\n",
    "    except:\n",
    "        print (row.plos_pub_year)\n",
    "        log_num_cit=0.\n",
    "    return log_num_cit\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['log10_num_cit_ref'] = df_merged.apply (lambda row: get_log_num_cit_of_ref(row),axis=1)\n",
    "%time df_merged['log10_num_cit_paper'] = df_merged.apply (lambda row: get_log_num_cit_of_paper(row),axis=1)\n",
    "\n",
    "#df_merged.rel_loc_in_sect.min()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log2_num_cit_of_ref(row):\n",
    "    \n",
    "    try:\n",
    "        log_num_cit=np.log2(float(row.cite_count))\n",
    "    except:\n",
    "        print (row.plos_pub_year)\n",
    "        log_num_cit_ref=0.\n",
    "    return log_num_cit\n",
    "\n",
    "###################\n",
    "\n",
    "def get_log2_num_cit_of_paper(row):\n",
    "    \n",
    "    try:\n",
    "        log_num_cit=np.log2(float(row.paper_cite_count))\n",
    "    except:\n",
    "        print (row.plos_pub_year)\n",
    "        log_num_cit=0.\n",
    "    return log_num_cit\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['log2_num_cit_ref'] = df_merged.apply (lambda row: get_log2_num_cit_of_ref(row),axis=1)\n",
    "%time df_merged['log2_num_cit_paper'] = df_merged.apply (lambda row: get_log2_num_cit_of_paper(row),axis=1)\n",
    "\n",
    "#df_merged.rel_loc_in_sect.min()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diff_publ_plos_publ_ref(row):\n",
    "    \n",
    "    try:\n",
    "        delta_publ_year=float(row.plos_pub_year)- float(row.ref_pub_year) \n",
    "    except:\n",
    "        delta_publ_year=np.nan\n",
    "    return delta_publ_year\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['diff_year_plos_ref'] = df_merged.apply (lambda row: get_diff_publ_plos_publ_ref(row),axis=1)\n",
    "\n",
    "#df_merged.rel_loc_in_sect.min()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_position_within_section(row):\n",
    "    \n",
    "    try:\n",
    "        rel_pos=float(row.sect_char_pos) / float(row.sect_char_total) \n",
    "    except:\n",
    "        rel_pos=np.nan\n",
    "    return rel_pos \n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['rel_loc_in_sect'] = df_merged.apply (lambda row: get_relative_position_within_section(row),axis=1)\n",
    "\n",
    "#df_merged.rel_loc_in_sect.min()\n",
    "\n",
    "df_merged.drop(['sect_char_pos','sect_char_total','total_refs'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_section(row):\n",
    "    \n",
    "    sect_num = row.regex_sect_index\n",
    "    section_name = \"NA\"\n",
    "    \n",
    "    if sect_num == 0:\n",
    "        section_name = \"0:Intro\"\n",
    "    elif sect_num == 1:\n",
    "        section_name = \"1:Methods\"\n",
    "    elif sect_num == 2:\n",
    "        section_name = \"2:Results\"\n",
    "    elif sect_num == 3:\n",
    "        section_name = \"3:Discussion\"\n",
    "    elif sect_num == 4:\n",
    "        section_name = \"4:Results/Discussion\"\n",
    "    elif sect_num == 5:\n",
    "        section_name = \"5:Conclusion\"\n",
    "    elif sect_num == 6:\n",
    "        section_name = \"6:Mx\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    return section_name \n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['section'] = df_merged.apply (lambda row: get_section(row),axis=1)\n",
    "\n",
    "\n",
    "print (\"done. writing pickle.....\")\n",
    "\n",
    "\n",
    "# path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "# %time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "# print (\"written:\",path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "lista_options_fields= ['D RO MULTIDISCIPLINARY SCIENCES','D RO MULTIDISCIPLINARY SCIENCES','D PY MEDICINE, GENERAL & INTERNAL','D TI PARASITOLOGY', 'D YU TROPICAL MEDICINE',\\\n",
    "'D CU BIOLOGY', 'D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY','D CU BIOLOGY', 'D PY MEDICINE, GENERAL & INTERNAL','D QU MICROBIOLOGY', 'D TI PARASITOLOGY', 'D ZE VIROLOGY',\\\n",
    "'D CU BIOLOGY', 'D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY','D KM GENETICS & HEREDITY','D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY',\\\n",
    "'D TI PARASITOLOGY', 'D ZE VIROLOGY', 'D QU MICROBIOLOGY','D KM GENETICS & HEREDITY','D CO BIOCHEMICAL RESEARCH METHODS', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS',\\\n",
    "'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY','D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']\n",
    "\n",
    "\n",
    "lista_options_fields= list(set(lista_options_fields))\n",
    "###################\n",
    "\n",
    "def get_new_plos_field(row, lista_options_fields):\n",
    "    \n",
    "    new_field=[]\n",
    "    old_field=str(row.plos_field)\n",
    "    #print (old_field,)\n",
    "    for field in lista_options_fields:\n",
    "        if field in old_field:\n",
    "            new_field.append(field)\n",
    "    \n",
    "    return str(new_field)\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['new_plos_field'] = df_merged.apply (lambda row: get_new_plos_field(row, lista_options_fields),axis=1)\n",
    "\n",
    "#df_merged.rel_loc_in_sect.min()\n",
    "\n",
    "\n",
    "\n",
    "df_merged.drop(['plos_field'], axis=1, inplace=True)\n",
    "df_merged.rename(columns={'new_plos_field': 'plos_field'}, inplace=True)\n",
    "\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = df_merged[['occurence','paper_UT','reference_UT','reference_rank','regex_sect_index','cite_count','ref_pub_year',\\\n",
    "#                                   'paper_cite_count','total_refs','plos_pub_year' , 'sect_char_pos', 'sect_char_total' ,'plos_field']]\n",
    "\n",
    "\n",
    "df_merged[['occurence', 'paper_UT', 'reference_UT','regex_sect_index','sect_char_pos', 'isolated_citation']].sort_values(by=['paper_UT','regex_sect_index','sect_char_pos']).head(10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isolated_or_group_citation(row):\n",
    "    \n",
    "    window=4  # max distance (in units of characters) to determine if a reference is cluster with anothers or not\n",
    "    \n",
    "    paper_UT = row.paper_UT\n",
    "    reference_UT = row.reference_UT\n",
    "    regex_sect_index = row.regex_sect_index\n",
    "    sect_char_pos = row.sect_char_pos\n",
    "    #print (\"\\n\",paper_UT,reference_UT,regex_sect_index,sect_char_pos)\n",
    "    \n",
    "    df_selection=df_merged[ (df_merged['paper_UT'] == paper_UT) &  (df_merged['regex_sect_index'] == regex_sect_index) ]   \n",
    "    \n",
    "    #print (df_selection.shape)\n",
    "    list_positions=sorted(list(df_selection.sect_char_pos))\n",
    "    \n",
    "    #print (list_positions)\n",
    "    lista_diff_pos= sorted([abs(item - sect_char_pos ) for item in list_positions])[1:]\n",
    "    #print (lista_diff_pos)\n",
    "     \n",
    "    if len(lista_diff_pos)>0:\n",
    "        if min(lista_diff_pos) > window:\n",
    "            #print (min(lista_diff_pos), paper_UT,reference_UT)#\"isolated ref!\")\n",
    "            return 1   # isolated reference\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "#df_testing=df_merged.head(10000)\n",
    "\n",
    "%time df_merged['isolated_citation'] = df_merged.apply (lambda row: isolated_or_group_citation(row),axis=1)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_new_paper_UT(row):\n",
    "    \n",
    "    new_UT= '000'+str(row.paper_UT)\n",
    "   \n",
    "    return new_UT\n",
    "\n",
    "###################\n",
    "\n",
    "\n",
    "%time df_merged['new_paper_UT'] = df_merged.apply (lambda row: get_new_paper_UT(row),axis=1)\n",
    "\n",
    "df_merged.drop(['paper_UT'], axis=1, inplace=True)\n",
    "df_merged.rename(columns={'new_paper_UT': 'paper_UT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "%time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "print (\"written:\",path)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns_.pkl'\n",
    "# %time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "# print (\"written:\",path)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "######## to re-add the number_of_references field that got dropped:\n",
    "\n",
    "%time df_merged_OLD = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_cols.pkl', 'rb'))\n",
    "print (\"done loading pickles\", df_merged_OLD.shape)\n",
    "\n",
    "\n",
    "df_plos_num_ref = df_merged_OLD[['paper_UT', 'total_refs']].drop_duplicates()\n",
    "\n",
    "\n",
    "## ojo! the old df\n",
    "df_plos_num_ref['new_paper_UT'] = df_plos_num_ref.apply (lambda row: fix_paper_UT(row),axis=1)\n",
    "df_plos_num_ref.drop(['paper_UT'], axis=1, inplace=True)\n",
    "df_plos_num_ref.rename(columns={'new_paper_UT': 'paper_UT'}, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_plos_num_ref, on='paper_UT', how='left')\n",
    "df_merged.head()\n",
    "\n",
    "\n",
    "df_merged.drop(['total_refs_x'], axis=1, inplace=True)\n",
    "df_merged.rename(columns={'total_refs_y': 'total_refs'}, inplace=True)\n",
    "\n",
    "\n",
    "path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "%time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "print (\"written little df with plos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_merged[['paper_UT','total_refs']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############  for classifying self-citations non self-citations:\n",
    "#####  i read the gzip files with the disambig author info from WOS, and filter out only the ones i care about (those papers included in the PLOS db):\n",
    "\n",
    "\n",
    "\n",
    "try:       \n",
    "                                 \n",
    "    %time df_disamb = pickle.load(open('../data/df_disambig_filtered.pkl', 'rb'))\n",
    "    print (\"done loading pickles\", df_disamb.shape)\n",
    "    df_disamb.head()\n",
    "\n",
    "\n",
    "except:\n",
    "\n",
    "\n",
    "    path_disamb='/home/workspace/scibio/resources/rbusa/rbusa_main_v1_1/disambiguation/wos_dais/'\n",
    "\n",
    "    lista_files=[]\n",
    "\n",
    "    for i in range(320):\n",
    "        i += 1\n",
    "        file_name=path_disamb+'wos_dais_all_batch'+str(i)+'.csv.gz'    \n",
    "        lista_files.append(file_name)\n",
    "    ##############\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cont_tot=0\n",
    "    cont_filtered=0\n",
    "    ########### concatenate a list of (gzip) files into one single pandas dataframe:\n",
    "    frame = pd.DataFrame()\n",
    "    list_ = []\n",
    "    for file_ in lista_files:\n",
    "        print (file_)\n",
    "        df = pd.read_csv(file_, compression = 'gzip', dtype=object, index_col=None, header=0)   # OJO!!!!!!!!!! i need to force it to read it as object (that is, as strings) so it doesnt remove the 000 at the beginning of some UTS!!!\n",
    "        cont_tot += len(df)\n",
    "        df = df[df['WOS'].isin(lista_UTs_plos_and_ref)]  # i directly filter out what i dont need, to save space\n",
    "        cont_filtered += len(df)\n",
    "        list_.append(df)\n",
    "\n",
    "    df_disamb = pd.concat(list_)\n",
    "    print (cont_tot, cont_filtered)    # 208,042,832         14,382,344\n",
    "\n",
    "\n",
    "    df_disamb.rename(columns={'WOS': 'UT'}, inplace=True)\n",
    "    print (df_disamb.shape)     # (14382344, 3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    path = '../data/df_disambig_filtered.pkl'\n",
    "    %time df_disamb.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "    print (\"written:\",path)  \n",
    "    df_disamb.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#####################\n",
    "\n",
    "\n",
    "###### i build the dict UT-list_authors that i will need for classifying citations as self or not self citations:\n",
    "try:\n",
    "    \n",
    "    %time dict_UT_list_authors = pickle.load(open('../data/dict_UT_list_authors.pkl', 'rb'))\n",
    "    print (\"done loading dict pickle\", len(dict_UT_list_authors))\n",
    "\n",
    "except:\n",
    "\n",
    "    dict_UT_list_authors={}    \n",
    "\n",
    "    for item in df_disamb.groupby(['UT']):  ### OJO!!!! IF I TRY TO DO IT WITH : for DAIS in df_disamb.DAIS.unique():      df_select= df_disamb [df_disamb ['DAIS']== DAIS]  IT would TAKE A MONTH TO RUN!!!\n",
    "        #print (item, \"  ---\",len(item[1].UT))\n",
    "        UT=item[0]\n",
    "        dict_UT_list_authors[UT]=list(item[1].DAIS)\n",
    "\n",
    "\n",
    "    print (\"done. now writing pickle.....\")\n",
    "\n",
    "    with open('../data/dict_UT_list_authors.pkl', 'wb') as handle:\n",
    "         pickle.dump(dict_UT_list_authors, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/dict_UT_list_authors.pkl')   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "##############\n",
    "\n",
    "\n",
    "\n",
    "def get_self_citation(row, dict_UT_list_authors, cont_missing_records):\n",
    "    \n",
    "    paper_UT = row.paper_UT\n",
    "    ref_UT = row.reference_UT\n",
    "    \n",
    "    self_citation=0\n",
    "    try:\n",
    "        lista_DAIS_paper = dict_UT_list_authors[paper_UT]\n",
    "    except KeyError:\n",
    "        lista_DAIS_paper = []\n",
    "        cont_missing_records +=1\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        lista_DAIS_ref = dict_UT_list_authors[ref_UT]\n",
    "    except KeyError:\n",
    "        lista_DAIS_ref = []\n",
    "        cont_missing_records +=1\n",
    "        \n",
    "#     print (paper_UT,lista_DAIS_paper ,\" -----\",ref_UT, lista_DAIS_ref, set(lista_DAIS_paper) & set(lista_DAIS_ref))\n",
    "#     input()\n",
    "    \n",
    "    if len(set(lista_DAIS_paper) & set(lista_DAIS_ref)) >0:  # if the citing paper and the reference paper share at least one author\n",
    "        self_citation = 1\n",
    "       # print (\"self-cit:\",paper_UT,\" --> \", ref_UT)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    return self_citation\n",
    "\n",
    "###################\n",
    "\n",
    "cont_missing_records=0\n",
    "%time df_merged['self_citation'] = df_merged.apply (lambda row: get_self_citation(row, dict_UT_list_authors, cont_missing_records), axis=1)\n",
    "\n",
    "print (\"done. missing records (plos and/or ref):\", cont_missing_records, \"\\nwriting pickle.....\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## print out the new dataframe with extra columns:\n",
    "#################################\n",
    "df_merged.drop([ 'log10_num_cit_paper', 'log10_num_cit_ref' ], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "%time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "print (\"written:\",path)  \n",
    "sorted(df_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### adding number of team members in a paper\n",
    "\n",
    "# def get_number_authors(row):\n",
    "     \n",
    "#     paper_UT = row.paper_UT\n",
    "       \n",
    "  \n",
    "#     #### i get the list of disambiguated author ID for the paper\n",
    "#     result_query = papers_con.collection.find_one({\"UT\":paper_UT},{\"AU\":1})  \n",
    "#    # print (result_query) ##  example: {'_id': ObjectId('54d3bdcdec29bd464368e4a8'), 'AU': [{'AU': 'Loffler, S', 'DAIS': 19993061}, {'AU': 'Jessen, J', 'DAIS': 37335135}, {'AU': 'Schmid, T', 'DAIS': 30805206}, {'AU': 'Porksen, U', 'DAIS': 63190680}]}\n",
    "\n",
    "#     list_dict_authors = result_query['AU']  # the result of find_one is a dictionary, NOT an iterator!  And the, result_query['AU'] is a list with all the authors of the UT paper        \n",
    "#     number_authors = len(list_dict_authors) \n",
    "        \n",
    "    \n",
    "#     return number_authors\n",
    "\n",
    "# ###################\n",
    "\n",
    "# %time plos_df['number_authors'] = plos_df.apply (lambda row: get_number_authors(row), axis=1)\n",
    "\n",
    "# ######################\n",
    "\n",
    "\n",
    "# path = '../data/plos_paper_dataframe_more_columns.pkl'\n",
    "# %time plos_df.to_pickle(path, compression='infer', protocol=2)\n",
    "# print (\"written little df with plos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### i get a dictionary for paper_UT  - publication year:\n",
    "\n",
    "plos_simple = plos_df[['paper_UT','plos_pub_year']]\n",
    "dict_aux_UT_year = dict(zip(plos_simple.paper_UT, plos_simple.plos_pub_year))\n",
    "for UT in dict_aux_UT_year:\n",
    "    dict_aux_UT_year[UT] = int(dict_aux_UT_year[UT])\n",
    "dict_aux_UT_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    ###   DONE!   i add the number of authors in each plos paper\n",
    "\n",
    "\n",
    "    try:\n",
    "        dict_UT_paper_number_authors = pickle.load(open('../data/dict_UT_paper_number_authors.pkl', 'rb'))    \n",
    "\n",
    "    except:    \n",
    "\n",
    "\n",
    "        dict_UT_paper_number_authors = {}\n",
    "        cont = 1\n",
    "        for paper_UT in list_paper_UT:\n",
    "\n",
    "\n",
    "\n",
    "            ######### i get the number of team members for each plos paper UT:\n",
    "\n",
    "            result_query = papers_con.collection.find_one({\"UT\":paper_UT},{\"AU\":1})  \n",
    "           # print (result_query) ##  example: {'_id': ObjectId('54d3bdcdec29bd464368e4a8'), 'AU': [{'AU': 'Loffler, S', 'DAIS': 19993061}, {'AU': 'Jessen, J', 'DAIS': 37335135}, {'AU': 'Schmid, T', 'DAIS': 30805206}, {'AU': 'Porksen, U', 'DAIS': 63190680}]}\n",
    "\n",
    "            list_dict_authors = result_query['AU']  # the result of find_one is a dictionary, NOT an iterator!  And the, result_query['AU'] is a list with all the authors of the UT paper        \n",
    "            number_authors = len(list_dict_authors) \n",
    "            dict_UT_paper_number_authors[paper_UT] = number_authors#get_number_authors(paper_UT)\n",
    "\n",
    "            print (cont)\n",
    "            cont += 1\n",
    "\n",
    "\n",
    "\n",
    "        with open('../data/dict_UT_paper_number_authors.pkl', 'wb') as handle:\n",
    "                 pickle.dump(dict_UT_paper_number_authors, handle, protocol = 2)\n",
    "        print (\"written:\",'../data/dict_UT_paper_number_authors.pkl', len(dict_UT_paper_number_authors))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    plos_df = plos_df.sort_values(by=['paper_UT'])\n",
    "    list_paper_UT = list(plos_df.paper_UT.values)    \n",
    "\n",
    "    list_number_authors = []\n",
    "    for paper_UT in list_paper_UT:\n",
    "        list_number_authors.append(dict_UT_paper_number_authors[paper_UT])\n",
    "\n",
    "    plos_df['number_authors'] = list_number_authors\n",
    "\n",
    "\n",
    "\n",
    "    plos_simple = plos_df[['paper_UT','number_authors']]\n",
    "    df_merged = pd.merge(df_merged, plos_simple, on='paper_UT', how='left')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # df_merged = df_merged.drop('number_authors_y', axis=1)\n",
    "    # df_merged = df_merged.rename(columns={'number_authors_x': 'number_authors'})\n",
    "    # sorted(df_merged.columns)\n",
    "\n",
    "\n",
    "\n",
    "    path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "    %time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "    print (\"written df_reference_cite_plos_merged_simplified_added_more_columns\")\n",
    "\n",
    "\n",
    "    plos_df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -rtl ../data/dict*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ########  DONE   (no need to re-run)!!!!!!     i get the dict plos paper_UT list of DAIS (disambiguated authors)\n",
    "\n",
    "\n",
    "list_paper_UT = list(plos_df.paper_UT.values)    \n",
    "\n",
    "\n",
    "\n",
    "##### i get the list of all DAIS (disambiguated authors) for the list of PLOS papers\n",
    "try:\n",
    "    tot_list_DAIS = pickle.load(open('../data/tot_list_DAIS.pkl', 'rb'))    \n",
    "    dict_plos_paper_UT_list_DAIS = pickle.load(open('../data/dict_plos_paper_UT_list_DAIS.pkl', 'rb'))    \n",
    "    \n",
    "except:    \n",
    "\n",
    "    cont  = 1\n",
    "    tot_list_DAIS=[]\n",
    "    dict_plos_paper_UT_list_DAIS = {}\n",
    "    for paper_UT in list_paper_UT:\n",
    "\n",
    "        if cont % 10000 == 0:\n",
    "           print (cont)\n",
    "        #### i get the list of disambiguated author IDs for the paper plos\n",
    "        result_query = papers_con.collection.find_one({\"UT\":paper_UT},{\"AU\":1})  \n",
    "         #  print (result_query) ##  example: {'_id': ObjectId('54d3bdcdec29bd464368e4a8'), 'AU': [{'AU': 'Loffler, S', 'DAIS': 19993061}, {'AU': 'Jessen, J', 'DAIS': 37335135}, {'AU': 'Schmid, T', 'DAIS': 30805206}, {'AU': 'Porksen, U', 'DAIS': 63190680}]}\n",
    "\n",
    "\n",
    "\n",
    "        list_DAIS=[]\n",
    "        for dict_author in result_query['AU']:  # the result of find_one is a dictionary, NOT an iterator!  And the, result_query['AU'] is a list of dict with info on all the authors of the paper_UT        \n",
    "            try:\n",
    "                DAIS = dict_author['DAIS']\n",
    "                #list_DAIS.append(DAIS)\n",
    "                tot_list_DAIS.append(DAIS)\n",
    "                list_DAIS.append(DAIS)\n",
    "            except :\n",
    "                pass#print (\"author without DAIS\",dict_author, \"  in paper:\", paper_UT)\n",
    "                #input()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ########## alternative, more pythonic \n",
    "        #### example: [d['value'] for d in l if 'value' in d]\n",
    "        #lista_of_dict =  result_query['AU'] \n",
    "        #list_DAIS = [d['DAIS'] for d in lista_of_dict if 'DAIS' in d]\n",
    "            \n",
    "                \n",
    "        dict_plos_paper_UT_list_DAIS[paper_UT] = list_DAIS\n",
    "        cont += 1\n",
    "\n",
    "\n",
    "    tot_list_DAIS = list(set(tot_list_DAIS))  # i remove duplicates\n",
    "    with open('../data/tot_list_DAIS.pkl', 'wb') as handle:\n",
    "             pickle.dump(tot_list_DAIS, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/tot_list_DAIS.pkl')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    with open('../data/dict_plos_paper_UT_list_DAIS.pkl', 'wb') as handle:\n",
    "             pickle.dump(dict_plos_paper_UT_list_DAIS, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/dict_plos_paper_UT_list_DAIS.pkl')\n",
    "                   \n",
    "\n",
    "print (\"# unique DAIS: \",len(tot_list_DAIS), len(dict_plos_paper_UT_list_DAIS))    #  697,993    158,813\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#### i get dict of DAIS list of all authored papers by that DAIS (disambiguated authors from all PLOS papers) \n",
    "try:\n",
    "    tot_list_papers_authored = pickle.load(open('../data/tot_list_papers_authored.pkl', 'rb'))    \n",
    "    dict_DAIS_list_papers = pickle.load(open('../data/dict_DAIS_list_papers.pkl', 'rb'))    \n",
    "except:        \n",
    "    \n",
    "    cont  = 1   \n",
    "    tot_list_papers_authored=[]    \n",
    "    dict_DAIS_list_papers = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for DAIS in tot_list_DAIS:    # tot:  697993\n",
    "\n",
    "                  ######for DAIS in aux_list_DAIS:  #  ooooooooooooooojo!!!!!!!!!! this is temporary, remove line after it is done running, and un-comment the previous lines!!!!\n",
    "       \n",
    "\n",
    "        if cont % 10000 == 0:\n",
    "            print (cont)\n",
    "\n",
    "\n",
    "        #####  i get all the papers by a given disambiguated author\n",
    "        cursor = dais_con.collection.find({\"DAIS\":DAIS},{\"UT\":1}) \n",
    "\n",
    "        list_papers=[]\n",
    "        for item in cursor:  # I iterate over all papers by all the authors of paper_UT\n",
    "            UT=item[\"UT\"]\n",
    "            tot_list_papers_authored.append(UT)\n",
    "            list_papers.append(UT)\n",
    "\n",
    "        dict_DAIS_list_papers[DAIS] = list_papers\n",
    "\n",
    "        cont += 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    tot_list_papers_authored = list(set(tot_list_papers_authored))  # i remove duplicates\n",
    "#     with open('../data/tot_list_papers_authored.pkl', 'wb') as handle:  # I dont really need this list\n",
    "#              pickle.dump(tot_list_papers_authored, handle, protocol = 2)\n",
    "#     print (\"written:\",'../data/tot_list_papers_authored.pkl')\n",
    " \n",
    "\n",
    "\n",
    "with open('../data/dict_DAIS_list_papers.pkl', 'wb') as handle:\n",
    "         pickle.dump(dict_DAIS_list_papers, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_DAIS_list_papers.pkl')\n",
    " \n",
    "         \n",
    "print (\"# unique authored papers by all those DAIS:  \",len(tot_list_papers_authored), len(dict_DAIS_list_papers))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### now i get the publication year of all papers author by the list of DAIS (from the list of all plos papers)\n",
    "try:\n",
    "    dict_all_papers_authored_publ_year = pickle.load(open('../data/dict_all_papers_authored_publ_year.pkl', 'rb'))  \n",
    "except:\n",
    "    \n",
    "    cont  = 1  \n",
    "    dict_all_papers_authored_publ_year = {}\n",
    "    for paper in tot_list_papers_authored:\n",
    "        if cont % 10000 == 0:\n",
    "            print (cont)\n",
    "            \n",
    "        dict_result_query = papers_con.collection.find_one({\"UT\":paper},{\"issue.PY\":1})  \n",
    "        \n",
    "        try:\n",
    "            year = dict_result_query['issue']['PY']\n",
    "            dict_all_papers_authored_publ_year[paper]=year\n",
    "        except: pass # if the paper does not exist or doesnt have a publication year\n",
    "            \n",
    "        cont += 1\n",
    "                \n",
    "        \n",
    "    with open('../data/dict_all_papers_authored_publ_year.pkl', 'wb') as handle:\n",
    "                 pickle.dump(dict_all_papers_authored_publ_year, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/dict_all_papers_authored_publ_year.pkl',len(dict_all_papers_authored_publ_year))   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "########################### next, i get the dictionary of all authored paper UTs vs the list of the R9 references they use  (later i need to convert R9s into UTs)\n",
    "\n",
    "\n",
    "try:  ####   OJOOOO !  replace by the final dict and list once it is done (instead of the partial)\n",
    "    #dict_all_authored_paper_UTs_list_R9_references = pickle.load(open('../data/dict_all_authored_paper_UTs_list_R9_references.pkl', 'rb'))  \n",
    "    #list_all_R9s = pickle.load(open('../data/list_all_R9s.pkl', 'rb'))  \n",
    "    \n",
    "    \n",
    "    \n",
    "    dict_all_authored_paper_UTs_list_R9_references = pickle.load(open('../data/dict_all_authored_paper_UTs_list_R9_references_partial.pkl', 'rb'))      \n",
    "    list_all_R9s = pickle.load(open('../data/list_all_R9s_partial.pkl', 'rb'))  \n",
    "    \n",
    "    aux_list_authored_papers= []\n",
    "    for llave in dict_all_papers_authored_publ_year:\n",
    "        if llave not in dict_all_authored_paper_UTs_list_R9_references:\n",
    "            aux_list_authored_papers.append(llave)\n",
    "\n",
    "#except:\n",
    "\n",
    "\n",
    "    dict_all_authored_paper_UTs_list_R9_references={}   \n",
    "    list_all_R9s = []\n",
    "\n",
    "    cont =1\n",
    "    ###for paper_UT in dict_all_papers_authored_publ_year:     # tot:   12,357,336  with publ year info\n",
    "\n",
    "\n",
    "\n",
    "    for paper_UT in aux_list_authored_papers:   ##################  ojo!!! this is temporary, remove after done running and un-coment previous lines\n",
    "\n",
    "        result_query = papers_con.collection.find_one({\"UT\":paper_UT},{\"CR\":1})  \n",
    "        if cont % 10000 == 0:\n",
    "            print (cont)\n",
    "\n",
    "        #    result_query['CR']  ### keys of the resulting find_one dictionary:   '_id', 'CR', 'UT'\n",
    "            ##### result_query['CR'] is a list of dict (one element per reference that the paper UT lists):  \n",
    "            #  [ {'/A': '*DHHS PAN CLIN PRA', '/W': 'GUID US ANT AG HIV 1', '/Y': '2006'}, \n",
    "            #    {'/A': 'BARTLETT, JA',  '/P': '1369',  '/V': '15',  '/W': 'AIDS', '/Y': '2001',  'R9': '0081245001'}]\n",
    "\n",
    "\n",
    "        try:        \n",
    "            lista_dict_references =  result_query['CR']  # i access the references used by a given paper_UT\n",
    "\n",
    "            list_ref_R9 = [d['R9'] for d in lista_dict_references if 'R9' in d]\n",
    "\n",
    "            list_all_R9s  += list_ref_R9\n",
    "\n",
    "        except : pass # if the paper doesnt have a list of references\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        dict_all_authored_paper_UTs_list_R9_references[paper_UT]=list_ref_R9\n",
    "\n",
    "        cont +=1\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "with open('../data/dict_all_authored_paper_UTs_list_R9_references.pkl', 'wb') as handle:\n",
    "             pickle.dump(dict_all_authored_paper_UTs_list_R9_references, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_all_authored_paper_UTs_list_R9_references.pkl',len(dict_all_authored_paper_UTs_list_R9_references))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_all_R9s = list(set(list_all_R9s))\n",
    "\n",
    "with open('../data/list_all_R9s_partial.pkl', 'wb') as handle:\n",
    "             pickle.dump(list_all_R9s, handle, protocol = 2)\n",
    "print (\"written:\",'../data/list_all_R9s_partial.pkl',len(list_all_R9s))   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################  i collect the keys ffrom all the partial dict so far, to continue where i left off:\n",
    "import pickle\n",
    "\n",
    "partial_list_keys=[]\n",
    "i = 1\n",
    "\n",
    "while i < 11:\n",
    "    try:\n",
    "        aux_dict = pickle.load(open('../data/dict_all_authored_paper_UT_list_R9_references_partial'+str(i*1000000)+'.pkl', 'rb'))      \n",
    "        partial_list_keys += list(aux_dict.keys())\n",
    "        print (i)\n",
    "    except: pass\n",
    "    i += 1\n",
    "\n",
    "print (len(partial_list_keys))\n",
    "partial_list_keys = list(set(partial_list_keys))\n",
    "    \n",
    "with open('../data/partial_list_keys.pkl', 'wb') as handle:\n",
    "             pickle.dump(partial_list_keys, handle, protocol = 2)\n",
    "print (\"written:\",'../data/partial_list_keys.pkl',len(partial_list_keys))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(partial_list_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux_list_authored_papers= []\n",
    "#     for llave in dict_all_papers_authored_publ_year:\n",
    "#         if llave not in dict_all_authored_paper_UTs_list_R9_references:\n",
    "#             aux_list_authored_papers.append(llave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(aux_list_authored_papers)   # 11965212\n",
    "  \n",
    "# len(dict_all_papers_authored_publ_year)  # 12357336\n",
    "\n",
    "# len (dict_all_authored_paper_UTs_list_R9_references)  # 392124\n",
    "\n",
    "#len(list_all_R9s)   11101075\n",
    "\n",
    "\n",
    "# list_all_R9s = list(set(list_all_R9s))\n",
    "\n",
    "# print (len(list_all_R9s))    #   6292503\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -rtl ../data/dict*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##########################  FALTA POR CORRER!        transform R9 from list of references of papers into UT\n",
    "\n",
    "dict_R9_UT = {}\n",
    "dict_UT_R9 = {}\n",
    "cont = 1\n",
    "list_missing_R9s =[]\n",
    "for R9 in list_all_R9s:\n",
    "    print (cont)\n",
    "    \n",
    "    dict_result = papers_con.collection.find_one({\"T9\":R9},{\"UT\":1})    ### keys of the resulting dict_result dictioray:   '_id', 'UT'\n",
    "    try:\n",
    "        UT = dict_result['UT']\n",
    "        dict_R9_UT[R9] = UT\n",
    "        dict_UT_R9[UT] = R9\n",
    "\n",
    "    except : \n",
    "        list_missing_R9s.append(R9)\n",
    "    \n",
    "    if cont % 10000 == 0:\n",
    "        print (cont)\n",
    "    cont +=1        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "with open('../data/dict_R9_UT_partial.pkl', 'wb') as handle:\n",
    "             pickle.dump(dict_R9_UT, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_R9_UT_partial.pkl',len(dict_R9_UT))   \n",
    "\n",
    "\n",
    "    \n",
    "with open('../data/dict_UT_R9_partial.pkl', 'wb') as handle:\n",
    "             pickle.dump(dict_UT_R9, handle, protocol = 2)\n",
    "print (\"written:\",'../data/dict_UT_R9_partial.pkl',len(dict_UT_R9))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_missing_R9s =  set(list(list_missing_R9s))\n",
    "print (len(list_missing_R9s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_all_authored_paper_UTs_list_R9_references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##################### FALTA POR CORRER!      i create a new dict of paper_UT : list of ref_UT from the auxiliary dict_all_authored_paper_UTs_list_R9_references:\n",
    "\n",
    "# dict_all_authored_paper_UT_list_UT_references = {}\n",
    "# cont = 1\n",
    "# for paper_UT in dict_all_authored_paper_UTs_list_R9_references:\n",
    "#     print (cont)\n",
    "#     list_ref_R9 = dict_all_authored_paper_UTs_list_R9_references[paper_UT]\n",
    "    \n",
    "#     list_aux_ref_UT = []\n",
    "#     for R9 in list_ref_R9:\n",
    "#         try:\n",
    "#             UT = dict_R9_UT[R9]\n",
    "#             list_aux_ref_UT.append(UT)\n",
    "#         except: pass\n",
    "        \n",
    "#     dict_all_authored_paper_UT_list_UT_references[paper_UT] = list_aux_ref_UT\n",
    "        \n",
    "#     if cont % 10000 == 0:\n",
    "#         print (cont)\n",
    "#     cont +=1  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with open('../data/dict_all_authored_paper_UT_list_UT_references_partial.pkl', 'wb') as handle:\n",
    "#              pickle.dump(dict_all_authored_paper_UT_list_UT_references, handle, protocol = 2)\n",
    "# print (\"written:\",'../data/dict_all_authored_paper_UT_list_UT_references_partial.pkl',len(dict_all_authored_paper_UT_list_UT_references))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plos_df.head()    # 156610\n",
    "sorted(df_merged.columns) #   5787634\n",
    "sorted(plos_df.columns) #   5787634\n",
    "\n",
    "#df_merged.regex_sect_index.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add new column from list\n",
    "#plos_df['new_paper_UT_copy'] = lista_plos_UT\n",
    "plos_df\n",
    "len(lista_plos_UT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####  DONE. the get the number of references used in each section for every plos paper:\n",
    "\n",
    "\n",
    "\n",
    "plos_df = plos_df.sort_values(by=['paper_UT'])    ### i make sure the order by plos paper UT is fixed, so I add the new columns in their proper order \n",
    "lista_plos_UT = list(plos_df.paper_UT.unique())\n",
    "\n",
    "\n",
    "dict_plos_UT_num_ref_sect0 ={}\n",
    "dict_plos_UT_num_ref_sect1 ={}\n",
    "dict_plos_UT_num_ref_sect2 ={}\n",
    "dict_plos_UT_num_ref_sect3 ={}\n",
    "dict_plos_UT_num_ref_sect4 ={}\n",
    "dict_plos_UT_num_ref_sect5 ={}\n",
    "dict_plos_UT_num_ref_sect6 ={}\n",
    "dict_plos_UT_num_ref_sect7 ={}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_plos_UT_eff_num_ref = {}\n",
    "\n",
    "\n",
    "for paper_UT in lista_plos_UT:     \n",
    "    dict_plos_UT_num_ref_sect0[paper_UT] = 0   # i initialize all values to 0, to cover the cases where thre are NO records for a given paper-section\n",
    "    dict_plos_UT_num_ref_sect1[paper_UT] = 0\n",
    "    dict_plos_UT_num_ref_sect2[paper_UT] = 0\n",
    "    dict_plos_UT_num_ref_sect3[paper_UT] = 0\n",
    "    dict_plos_UT_num_ref_sect4[paper_UT] = 0\n",
    "    dict_plos_UT_num_ref_sect5[paper_UT] = 0\n",
    "    dict_plos_UT_num_ref_sect6[paper_UT] = 0\n",
    "    dict_plos_UT_num_ref_sect7[paper_UT] = 0\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "num_records = 0\n",
    "cont = 1\n",
    "#for paper_UT in lista_plos_UT:\n",
    "for paper_UT, group_df in df_merged.groupby(['paper_UT']):  #### OJO!!!! THIS LOOP IS WAY FASTER THAN DOING:  for   paper_UT in list_paper_UT    !!!!    \n",
    "    print (cont)   \n",
    "    \n",
    "   \n",
    "    dict_plos_UT_eff_num_ref[paper_UT] =len(group_df)\n",
    "            \n",
    "    for section in range(8):\n",
    "        df_selection_by_sect = group_df[ group_df['regex_sect_index'] == section]\n",
    "                        \n",
    "        if section ==0:\n",
    "              dict_plos_UT_num_ref_sect0[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==1:\n",
    "              dict_plos_UT_num_ref_sect1[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==2:\n",
    "              dict_plos_UT_num_ref_sect2[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==3:\n",
    "              dict_plos_UT_num_ref_sect3[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==4:\n",
    "              dict_plos_UT_num_ref_sect4[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==5:\n",
    "              dict_plos_UT_num_ref_sect5[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==6:\n",
    "              dict_plos_UT_num_ref_sect6[paper_UT] = len(df_selection_by_sect)\n",
    "        elif section ==7:\n",
    "              dict_plos_UT_num_ref_sect7[paper_UT] = len(df_selection_by_sect)\n",
    "                                                                              \n",
    "        \n",
    "    cont += 1\n",
    "    num_records  += len(group_df)                                  \n",
    "                                            \n",
    "                                            \n",
    "print (\"done creating dictionaries\")                                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plos_df = plos_df.sort_values(by=['paper_UT'])    ### i make sure the order by plos paper UT is fixed, so I add the new columns in their proper order \n",
    "lista_plos_UT = list(plos_df.paper_UT.unique())\n",
    "print (len(lista_plos_UT))\n",
    "\n",
    "lista_section0 = []\n",
    "lista_section1 = []\n",
    "lista_section2 = []\n",
    "lista_section3 = []\n",
    "lista_section4 = []\n",
    "lista_section5 = []\n",
    "lista_section6 = []\n",
    "lista_section7 = []\n",
    "\n",
    "lista_eff_num_papers = []\n",
    "\n",
    "lista_missing_paper_UT = []\n",
    "for paper_UT in lista_plos_UT: ### there are 52  paper_UT that have NO references \n",
    "    try:\n",
    "        lista_section0.append(dict_plos_UT_num_ref_sect0[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section0.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "        \n",
    "    try:\n",
    "        lista_section1.append(dict_plos_UT_num_ref_sect1[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section1.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "            \n",
    "    try:\n",
    "        lista_section2.append(dict_plos_UT_num_ref_sect2[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section2.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "            \n",
    "    try:\n",
    "        lista_section3.append(dict_plos_UT_num_ref_sect3[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section3.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "            \n",
    "    try:\n",
    "        lista_section4.append(dict_plos_UT_num_ref_sect4[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section4.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "            \n",
    "    try:\n",
    "        lista_section5.append(dict_plos_UT_num_ref_sect5[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section5.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "            \n",
    "    try:\n",
    "        lista_section6.append(dict_plos_UT_num_ref_sect6[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section6.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "        \n",
    "    try:\n",
    "        lista_section7.append(dict_plos_UT_num_ref_sect7[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_section7.append(np.nan)\n",
    "        lista_missing_paper_UT.append(paper_UT)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    try:    \n",
    "        lista_eff_num_papers.append(dict_plos_UT_eff_num_ref[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_eff_num_papers.append(np.nan)\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "print (len(plos_df), len(lista_section0), len(lista_section1), len(lista_section2), len(lista_section3), len(lista_section4), len(lista_section5), len(lista_section6), len(lista_section7), len(lista_eff_num_papers))    \n",
    "\n",
    "plos_df['num_ref_section0'] = lista_section0\n",
    "plos_df['num_ref_section1'] = lista_section1\n",
    "plos_df['num_ref_section2'] = lista_section2\n",
    "plos_df['num_ref_section3'] = lista_section3\n",
    "plos_df['num_ref_section4'] = lista_section4\n",
    "plos_df['num_ref_section5'] = lista_section5\n",
    "plos_df['num_ref_section6'] = lista_section6\n",
    "plos_df['num_ref_section7'] = lista_section7\n",
    "\n",
    "plos_df['eff_num_ref'] = lista_eff_num_papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## example of plos not included:  000207444200009  (does it have 0 references in each section???)   --->>  check df_merged\n",
    "\n",
    "print (\"num missing plos papers:\",len(set(lista_missing_paper_UT)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plos_df['fract_ref_section0'] = plos_df.apply(lambda row: row.num_ref_section0 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section1'] = plos_df.apply(lambda row: row.num_ref_section1 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section2'] = plos_df.apply(lambda row: row.num_ref_section2 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section3'] = plos_df.apply(lambda row: row.num_ref_section3 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section4'] = plos_df.apply(lambda row: row.num_ref_section4 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section5'] = plos_df.apply(lambda row: row.num_ref_section5 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section6'] = plos_df.apply(lambda row: row.num_ref_section6 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_ref_section7'] = plos_df.apply(lambda row: row.num_ref_section7 / row.eff_num_ref, axis=1)\n",
    "\n",
    "\n",
    "plos_df['tot_frac_eff_ref'] = plos_df.apply(lambda row: row.fract_ref_section0 +  row.fract_ref_section1 +  row.fract_ref_section2 +  row.fract_ref_section3 +  row.fract_ref_section4 +  row.fract_ref_section5 + row.fract_ref_section6 +  row.fract_ref_section7 , axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect.pkl', 'wb') as handle:\n",
    "                 pickle.dump(plos_df, handle, protocol = 2)\n",
    "print (\"written:\",'../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect.pkl',len(plos_df))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                                            \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(plos_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####  i get the number of YOUNG AND OLF references used in each section for every plos paper:\n",
    "\n",
    "\n",
    "\n",
    "time_window_young = 1\n",
    "time_window_old = 10\n",
    "\n",
    "\n",
    "# if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window_young))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_young) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window_old))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_old) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "\n",
    "\n",
    "plos_df = plos_df.sort_values(by=['paper_UT'])    ### i make sure the order by plos paper UT is fixed, so I add the new columns in their proper order \n",
    "lista_plos_UT = list(plos_df.paper_UT.unique())\n",
    "\n",
    "\n",
    "dict_plos_UT_num_old_ref_sect0 = {}  # number of old ref per paper and per section\n",
    "dict_plos_UT_num_old_ref_sect1 = {}\n",
    "dict_plos_UT_num_old_ref_sect2 = {}\n",
    "dict_plos_UT_num_old_ref_sect3 = {}\n",
    "dict_plos_UT_num_old_ref_sect4 = {}\n",
    "dict_plos_UT_num_old_ref_sect5 = {}\n",
    "dict_plos_UT_num_old_ref_sect6 = {}\n",
    "dict_plos_UT_num_old_ref_sect7 = {}\n",
    "\n",
    "\n",
    "\n",
    "dict_plos_UT_num_young_ref_sect0 = {}  # number of old ref per paper and per section\n",
    "dict_plos_UT_num_young_ref_sect1 = {}\n",
    "dict_plos_UT_num_young_ref_sect2 = {}\n",
    "dict_plos_UT_num_young_ref_sect3 = {}\n",
    "dict_plos_UT_num_young_ref_sect4 = {}\n",
    "dict_plos_UT_num_young_ref_sect5 = {}\n",
    "dict_plos_UT_num_young_ref_sect6 = {}\n",
    "dict_plos_UT_num_young_ref_sect7 = {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for paper_UT in lista_plos_UT:                 \n",
    "    \n",
    "    dict_plos_UT_num_old_ref_sect0[paper_UT] = 0   # i initialize all values to 0, to cover the cases where thre are NO records for a given paper-section\n",
    "    dict_plos_UT_num_old_ref_sect1[paper_UT] = 0\n",
    "    dict_plos_UT_num_old_ref_sect2[paper_UT] = 0\n",
    "    dict_plos_UT_num_old_ref_sect3[paper_UT] = 0\n",
    "    dict_plos_UT_num_old_ref_sect4[paper_UT] = 0\n",
    "    dict_plos_UT_num_old_ref_sect5[paper_UT] = 0\n",
    "    dict_plos_UT_num_old_ref_sect6[paper_UT] = 0\n",
    "    dict_plos_UT_num_old_ref_sect7[paper_UT] = 0\n",
    "\n",
    "    \n",
    "          \n",
    "    dict_plos_UT_num_young_ref_sect0[paper_UT] = 0   # i initialize all values to 0, to cover the cases where thre are NO records for a given paper-section\n",
    "    dict_plos_UT_num_young_ref_sect1[paper_UT] = 0\n",
    "    dict_plos_UT_num_young_ref_sect2[paper_UT] = 0\n",
    "    dict_plos_UT_num_young_ref_sect3[paper_UT] = 0\n",
    "    dict_plos_UT_num_young_ref_sect4[paper_UT] = 0\n",
    "    dict_plos_UT_num_young_ref_sect5[paper_UT] = 0\n",
    "    dict_plos_UT_num_young_ref_sect6[paper_UT] = 0\n",
    "    dict_plos_UT_num_young_ref_sect7[paper_UT] = 0  \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "cont = 1\n",
    "#for paper_UT in lista_plos_UT:\n",
    "for paper_UT, group_df in df_merged.groupby(['paper_UT']):  #### OJO!!!! THIS LOOP IS WAY FASTER THAN DOING:  for   paper_UT in list_paper_UT    !!!!    \n",
    "    print (cont)   \n",
    "    \n",
    "    \n",
    "    year = int(group_df.plos_pub_year.unique())\n",
    "    \n",
    "    preselection_df_young = group_df[group_df['ref_pub_year'] >= (year-time_window_young) ]   \n",
    "    #print (\"  size of preselection (only young references):\", group_df.shape, preselection_df_young.shape)\n",
    " \n",
    "\n",
    "    preselection_df_old = group_df[group_df['ref_pub_year'] <= (year-time_window_old) ]   \n",
    "   # print (\"  size of preselection (only young references):\", group_df.shape, preselection_df_old.shape)\n",
    " \n",
    "    \n",
    "        \n",
    "            \n",
    "    for section in range(8):\n",
    "        df_selection_young_by_sect = preselection_df_young[ preselection_df_young['regex_sect_index'] == section]\n",
    "        df_selection_old_by_sect = preselection_df_old[ preselection_df_old['regex_sect_index'] == section]\n",
    "       \n",
    "    \n",
    "    \n",
    "        if section ==0:\n",
    "            dict_plos_UT_num_young_ref_sect0[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect0[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==1:\n",
    "            dict_plos_UT_num_young_ref_sect1[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect1[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==2:\n",
    "            dict_plos_UT_num_young_ref_sect2[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect2[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==3:\n",
    "            dict_plos_UT_num_young_ref_sect3[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect3[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==4:\n",
    "            dict_plos_UT_num_young_ref_sect4[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect4[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==5:\n",
    "            dict_plos_UT_num_young_ref_sect5[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect5[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==6:\n",
    "            dict_plos_UT_num_young_ref_sect6[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect6[paper_UT] = len(df_selection_old_by_sect)\n",
    "        elif section ==7:\n",
    "            dict_plos_UT_num_young_ref_sect7[paper_UT] = len(df_selection_young_by_sect)\n",
    "            dict_plos_UT_num_old_ref_sect7[paper_UT] = len(df_selection_old_by_sect)\n",
    "                                                                              \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    cont += 1\n",
    "                          \n",
    "                                            \n",
    "                                            \n",
    "print (\"done creating dictionaries\")                                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plos_df = plos_df.sort_values(by=['paper_UT'])    ### i make sure the order by plos paper UT is fixed, so I add the new columns in their proper order \n",
    "lista_plos_UT = list(plos_df.paper_UT.unique())\n",
    "print (len(lista_plos_UT))\n",
    "\n",
    "lista_young_section0 = []\n",
    "lista_young_section1 = []\n",
    "lista_young_section2 = []\n",
    "lista_young_section3 = []\n",
    "lista_young_section4 = []\n",
    "lista_young_section5 = []\n",
    "lista_young_section6 = []\n",
    "lista_young_section7 = []\n",
    "\n",
    "\n",
    "\n",
    "lista_old_section0 = []\n",
    "lista_old_section1 = []\n",
    "lista_old_section2 = []\n",
    "lista_old_section3 = []\n",
    "lista_old_section4 = []\n",
    "lista_old_section5 = []\n",
    "lista_old_section6 = []\n",
    "lista_old_section7 = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_missing_paper_UT = []\n",
    "for paper_UT in lista_plos_UT: ### there are 52  paper_UT that have NO references \n",
    "    \n",
    "    \n",
    "    #### for young ref\n",
    "    try:\n",
    "        lista_young_section0.append(dict_plos_UT_num_young_ref_sect0[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section0.append(np.nan)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        lista_young_section1.append(dict_plos_UT_num_young_ref_sect1[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section1.append(np.nan)\n",
    "       \n",
    "            \n",
    "    try:\n",
    "        lista_young_section2.append(dict_plos_UT_num_young_ref_sect2[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section2.append(np.nan)\n",
    "       \n",
    "            \n",
    "    try:\n",
    "        lista_young_section3.append(dict_plos_UT_num_young_ref_sect3[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section3.append(np.nan)\n",
    "        \n",
    "            \n",
    "    try:\n",
    "        lista_young_section4.append(dict_plos_UT_num_young_ref_sect4[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section4.append(np.nan)\n",
    "        \n",
    "            \n",
    "    try:\n",
    "        lista_young_section5.append(dict_plos_UT_num_young_ref_sect5[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section5.append(np.nan)\n",
    "       \n",
    "            \n",
    "    try:\n",
    "        lista_young_section6.append(dict_plos_UT_num_young_ref_sect6[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section6.append(np.nan)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        lista_young_section7.append(dict_plos_UT_num_young_ref_sect7[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_young_section7.append(np.nan)\n",
    "        \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "    ###### for old ref\n",
    "    try:\n",
    "        lista_old_section0.append(dict_plos_UT_num_old_ref_sect0[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section0.append(np.nan)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        lista_old_section1.append(dict_plos_UT_num_old_ref_sect1[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section1.append(np.nan)\n",
    "        \n",
    "            \n",
    "    try:\n",
    "        lista_old_section2.append(dict_plos_UT_num_old_ref_sect2[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section2.append(np.nan)\n",
    "       \n",
    "            \n",
    "    try:\n",
    "        lista_old_section3.append(dict_plos_UT_num_old_ref_sect3[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section3.append(np.nan)\n",
    "       \n",
    "            \n",
    "    try:\n",
    "        lista_old_section4.append(dict_plos_UT_num_old_ref_sect4[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section4.append(np.nan)\n",
    "        \n",
    "            \n",
    "    try:\n",
    "        lista_old_section5.append(dict_plos_UT_num_old_ref_sect5[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section5.append(np.nan)\n",
    "       \n",
    "            \n",
    "    try:\n",
    "        lista_old_section6.append(dict_plos_UT_num_old_ref_sect6[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section6.append(np.nan)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        lista_old_section7.append(dict_plos_UT_num_old_ref_sect7[paper_UT])\n",
    "    except KeyError:   \n",
    "        lista_old_section7.append(np.nan)\n",
    "        \n",
    "        \n",
    "     \n",
    "        \n",
    "        \n",
    "print (len(plos_df), len(lista_young_section0), len(lista_young_section1), len(lista_young_section2), len(lista_young_section3), len(lista_young_section4), len(lista_young_section5), len(lista_young_section6), len(lista_young_section7)   ) \n",
    "print (len(plos_df), len(lista_old_section0), len(lista_old_section1), len(lista_old_section2), len(lista_old_section3), len(lista_old_section4), len(lista_old_section5), len(lista_old_section6), len(lista_old_section7)   ) \n",
    "\n",
    "\n",
    "\n",
    "plos_df['num_young_ref_section0'] = lista_young_section0\n",
    "plos_df['num_young_ref_section1'] = lista_young_section1\n",
    "plos_df['num_young_ref_section2'] = lista_young_section2\n",
    "plos_df['num_young_ref_section3'] = lista_young_section3\n",
    "plos_df['num_young_ref_section4'] = lista_young_section4\n",
    "plos_df['num_young_ref_section5'] = lista_young_section5\n",
    "plos_df['num_young_ref_section6'] = lista_young_section6\n",
    "plos_df['num_young_ref_section7'] = lista_young_section7\n",
    "\n",
    "       \n",
    "\n",
    "plos_df['num_old_ref_section0'] = lista_old_section0\n",
    "plos_df['num_old_ref_section1'] = lista_old_section1\n",
    "plos_df['num_old_ref_section2'] = lista_old_section2\n",
    "plos_df['num_old_ref_section3'] = lista_old_section3\n",
    "plos_df['num_old_ref_section4'] = lista_old_section4\n",
    "plos_df['num_old_ref_section5'] = lista_old_section5\n",
    "plos_df['num_old_ref_section6'] = lista_old_section6\n",
    "plos_df['num_old_ref_section7'] = lista_old_section7\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plos_df['fract_young_ref_section0'] = plos_df.apply(lambda row: row.num_young_ref_section0 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section1'] = plos_df.apply(lambda row: row.num_young_ref_section1 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section2'] = plos_df.apply(lambda row: row.num_young_ref_section2 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section3'] = plos_df.apply(lambda row: row.num_young_ref_section3 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section4'] = plos_df.apply(lambda row: row.num_young_ref_section4 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section5'] = plos_df.apply(lambda row: row.num_young_ref_section5 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section6'] = plos_df.apply(lambda row: row.num_young_ref_section6 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_young_ref_section7'] = plos_df.apply(lambda row: row.num_young_ref_section7 / row.eff_num_ref, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plos_df['fract_old_ref_section0'] = plos_df.apply(lambda row: row.num_old_ref_section0 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section1'] = plos_df.apply(lambda row: row.num_old_ref_section1 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section2'] = plos_df.apply(lambda row: row.num_old_ref_section2 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section3'] = plos_df.apply(lambda row: row.num_old_ref_section3 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section4'] = plos_df.apply(lambda row: row.num_old_ref_section4 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section5'] = plos_df.apply(lambda row: row.num_old_ref_section5 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section6'] = plos_df.apply(lambda row: row.num_old_ref_section6 / row.eff_num_ref, axis=1)\n",
    "plos_df['fract_old_ref_section7'] = plos_df.apply(lambda row: row.num_old_ref_section7 / row.eff_num_ref, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plos_df['tot_frac_eff_young_ref'] = plos_df.apply(lambda row: row.fract_young_ref_section0 +  row.fract_young_ref_section1 +  row.fract_young_ref_section2 +  row.fract_young_ref_section3 +  row.fract_young_ref_section4 +  row.fract_young_ref_section5 + row.fract_young_ref_section6 +  row.fract_young_ref_section7 , axis=1)\n",
    "plos_df['tot_frac_eff_old_ref'] = plos_df.apply(lambda row:   row.fract_old_ref_section0 +    row.fract_old_ref_section1 +    row.fract_old_ref_section2 +  row.fract_old_ref_section3 +  row.fract_old_ref_section4 +  row.fract_old_ref_section5 + row.fract_old_ref_section6 +  row.fract_old_ref_section7 , axis=1)\n",
    "\n",
    "\n",
    "plos_df['tot_num_eff_young_ref'] = plos_df.apply(lambda row: row.num_young_ref_section0 +  row.num_young_ref_section1 +  row.num_young_ref_section2 +  row.num_young_ref_section3 +  row.num_young_ref_section4 +  row.num_young_ref_section5 + row.num_young_ref_section6 +  row.num_young_ref_section7 , axis=1)\n",
    "plos_df['tot_num_eff_old_ref'] = plos_df.apply(lambda row:   row.num_old_ref_section0 +    row.num_old_ref_section1 +    row.num_old_ref_section2 +  row.num_old_ref_section3 +  row.num_old_ref_section4 +  row.num_old_ref_section5 + row.num_old_ref_section6 +  row.num_old_ref_section7 , axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect_young_old.pkl', 'wb') as handle:\n",
    "                 pickle.dump(plos_df, handle, protocol = 2)\n",
    "print (\"written:\",'../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect_young_old.pkl',len(plos_df))   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(plos_df.columns)\n",
    "print (plos_df.shape)\n",
    "\n",
    "plos_df[['paper_UT','total_refs','eff_num_ref','tot_num_eff_young_ref','tot_num_eff_old_ref','tot_frac_eff_young_ref','tot_frac_eff_old_ref','num_ref_section0','num_ref_section1','num_ref_section2','num_ref_section3','num_ref_section4','num_ref_section5','num_ref_section6','num_ref_section7', 'num_young_ref_section0','num_young_ref_section1','num_young_ref_section2','num_young_ref_section3',  'num_old_ref_section0','num_old_ref_section1','num_old_ref_section2','num_old_ref_section3']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sorted(plos_df.columns)\n",
    "print (plos_df.shape)\n",
    "\n",
    "plos_df[['paper_UT','total_refs','eff_num_ref','tot_num_eff_young_ref','tot_num_eff_old_ref','fract_young_ref_section0','fract_young_ref_section1',  'fract_old_ref_section0','fract_old_ref_section1']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(plos_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   double check:\n",
    "print (df_merged.shape)   # (5787634, 27)\n",
    "df_merged=df_merged.drop_duplicates(subset=['paper_UT','reference_UT','regex_sect_index'])\n",
    "print (df_merged.shape)  # (5787634, 27)       ### yay!!!! df_merged EXCLUDES repetitions of a given reference in a given section of a plos paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plos_df.hist(column='eff_num_ref', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_select = df_merged[df_merged['paper_UT'] == '000207443600001']\n",
    "# print (len(df_select))\n",
    "# df_select[['paper_UT','reference_UT','regex_sect_index','occurence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plos_df.hist(column='total_refs', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print (lista_plos_UT[:10])\n",
    "#df_merged[df_merged['paper_UT'].isin(lista_plos_UT[:10])]\n",
    "\n",
    "# '000207444200009',\n",
    "#  '000207455200002',\n",
    "#  '000267347900002',\n",
    "#  '000268637600014',\n",
    "#  '000270594000017',\n",
    "#  '000278017400008',\n",
    "#  '000280065600013',\n",
    "#  '000284686500004',\n",
    "#  '000286663900015',\n",
    "#  '000287764100045',\n",
    "#  '000289058700017',\n",
    "#  '000291097600002',\n",
    "#  '000306507000030',\n",
    "#  '000309510900003',\n",
    "#  '000311997100014',\n",
    "#  '000313236200097',\n",
    "#  '000313236200126',\n",
    "#  '000315157200061',\n",
    "#  '000315159200027',\n",
    "#  '000315637900010',\n",
    "#  '000315897100001',\n",
    "#  '000320579400001',\n",
    "#  '000322167900019',\n",
    "#  '000323221500028',\n",
    "#  '000324238400040',\n",
    "#  '000324527300053'\n",
    "\n",
    "set(lista_missing_paper_UT)\n",
    "\n",
    "plos_df[plos_df['paper_UT'] == '000207444200009']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_merged.reference_UT.unique())\n",
    "# df_merged.shape\n",
    "# len(set(lista_plos_UT)), len(plos_df), len(dict_plos_UT_num_ref_sect0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dict_plos_UT_num_ref_sect0), len(dict_plos_UT_num_ref_sect1),len(dict_plos_UT_num_ref_sect2) ,len(dict_plos_UT_num_ref_sect3) \n",
    "#plos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged.groupby(['paper_UT']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(dict_DAIS_list_papers)   #  13,052,537\n",
    "\n",
    "# len(dict_all_papers_authored_publ_year)  # 12,357,336"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### adding columns regarding number of citations as of 2009, 2011, 2013 of only young references!\n",
    "\n",
    "\n",
    "                                            \n",
    "%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns_no_self-cit_one_ref_per_sect_ONLY_ARTICLES.pkl', 'rb'))\n",
    "print (\"done loading pickles\", df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)\n",
    "\n",
    "# ['categ_codes',\n",
    "#  'cite_count',\n",
    "#  'diff_year_plos_ref',\n",
    "#  'isolated_citation',\n",
    "#  'log2_num_cit_paper',\n",
    "#  'log2_num_cit_ref',\n",
    "#  'number_authors',\n",
    "#  'occurence',\n",
    "#  'paper_UT',\n",
    "#  'paper_cite_count',\n",
    "#  'plos_article_type',\n",
    "#  'plos_field',\n",
    "#  'plos_j1',\n",
    "#  'plos_pub_year',\n",
    "#  'ref_field',\n",
    "#  'ref_j1',\n",
    "#  'ref_pub_year',\n",
    "#  'reference_UT',\n",
    "#  'reference_rank',\n",
    "#  'regex_sect_index',\n",
    "#  'rel_loc_in_sect',\n",
    "#  'sect_char_pos',\n",
    "#  'sect_char_total',\n",
    "#  'self_citation',\n",
    "#  'total_refs']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_merged.plos_pub_year.max()  2005-2016\n",
    "\n",
    "#df_merged.ref_pub_year.min() #  1900-2016\n",
    "\n",
    "#df_merged.shape   # (5787634, 25)\n",
    "\n",
    "len(df_merged.paper_UT.unique())    #  156558\n",
    "\n",
    "\n",
    "len(df_merged.reference_UT.unique())    #  2320777\n",
    "\n",
    "list_all_ref_UT = df_merged.reference_UT.unique()   # this is an array\n",
    "\n",
    "\n",
    "list_all_ref_UT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ######## extract number of citations of young references early on.\n",
    "\n",
    "\n",
    "### NON-EFICIENT QUERIES\n",
    "\n",
    "# focus_plos_year = 2011  # the main figures of my paper correspond to plos papers published in 2011, and the young references are at most one year old == 2011: \n",
    "# ############# how many citations did young references have by 2011 (by focus year)???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_selection_general_young_ref = df_merged[ (df_merged['ref_pub_year'] >= (focus_plos_year - 1) )]  \n",
    "\n",
    "\n",
    "# list_UT_young_ref_by_focus_year = df_selection_general_young_ref.reference_UT.unique()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"Total # records of young ref. by\", focus_year, \": \",df_selection_general_young_ref.shape, \"  # unique ref_UTs:\", len(list_UT_young_ref_by_focus_year))   # \n",
    "\n",
    "\n",
    "# dict_UT_young_ref_num_cit_by_focus_year = {}\n",
    "\n",
    "# list_all_citing_appers_of_ref = []   \n",
    "\n",
    "\n",
    "# for ref_UT in list_all_ref_UT:  # i need to assign a value to all references  (those that were older, get a NaN, those that are young enough (one year old or younger), get whatever number of citations they had by focus_year)           \n",
    "#     dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = np.nan  # first i initialize all ref to nan  (faster than evaluating whether the reference is in the list of selected ones or not)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# cont = 0    \n",
    "# for ref_UT in list_UT_young_ref_by_focus_year:    \n",
    "#     ref_UT = str(ref_UT)\n",
    "   \n",
    "\n",
    "   \n",
    "\n",
    "# #     cursor = papers_con.collection.find({\"UT\" : ref_UT}, {\"UT\":1,\"issue.PY\":1,'citations':1})  # THIS IS AN ITERATOR\n",
    "#     item = papers_con.collection.find_one({\"UT\" : ref_UT}, {\"UT\":1,\"issue.PY\":1,'citations':1})  # THIS IS AN ITERATOR\n",
    "#     # citations:   cites received by the paper (currently updated value!)\n",
    "\n",
    "\n",
    "#     ## iteration\n",
    "# #     for item in cursor:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict.    ONLY one element in this particular cursor!!\n",
    "        \n",
    "#     #focus_ref = str(item[\"UT\"])\n",
    "#     year_ref =  item['issue']['PY']\n",
    "\n",
    "\n",
    "\n",
    "#     dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "\n",
    "#     try:\n",
    "#         list_cit = item[\"citations\"]                                \n",
    "#     except KeyError:\n",
    "#         list_cit=[]\n",
    "\n",
    "#     aux_list_citing_papers = list_cit\n",
    "\n",
    "#     cursor2 = papers_con.collection.find( { \"UT\" :{\"$in\":aux_list_citing_papers}}, {\"UT\":1,\"issue.PY\":1,'citations':1})\n",
    "\n",
    "    \n",
    "\n",
    "#     for item2 in cursor2:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict        \n",
    "\n",
    "\n",
    "#         UT_citing=str(item2[\"UT\"])\n",
    "#         year = item2['issue']['PY']\n",
    "#         #print (year)\n",
    "\n",
    "#         if year <= focus_plos_year:  # is the focus reference has recived the reference at most during the focus year, then i count it\n",
    "#             dict_UT_young_ref_num_cit_by_focus_year[ref_UT] +=1\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#     print(cont, ref_UT, \"# cit total:\", len(list_cit),\"   # cit by\",focus_year, \":\",dict_UT_young_ref_num_cit_by_focus_year[ref_UT])        \n",
    "#    # input()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "#     cont +=1  \n",
    "        \n",
    "# #   example of all citations received by '000286133900010':     \n",
    "  \n",
    "# # [  \"000312433100012\", \"000291741700004\", \"000293533000004\", \"000293533000008\", \"000293955200010\", \"000318967500010\", \"000321851900003\", \"000321851900005\", \"000321851900006\",\"000321851900007\", \"000324877000001\", \"000302254500012\", \"000326340100029\", \"000329043200014\", \"000330353000004\", \"000330641700001\", \"000317239300013\", \"000299059700011\", \"000302154300005\",  \"000291741700003\", \"000297708200027\", \"000317239300002\", \"000297708200012\", \"000343049600116\", \"000347095400001\", \"000352084800063\", \"000353713900026\", \"000354607300001\", \"000401743700010\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# print (\"done.\", len(dict_UT_young_ref_num_cit_by_focus_year),\"dumping dict ref.......\")         \n",
    "# with open('../data/dict_UT_young_ref_in'+str(focus_year)+'_num_cit_by_'+str(focus_year)+'.pkl', 'wb') as handle:\n",
    "#          pickle.dump(dict_UT_young_ref_num_cit_by_focus_year, handle, protocol = 2)\n",
    "# print (\"written:\",'../data/dict_UT_young_ref_in'+str(focus_year)+'_num_cit_by_'+str(focus_year)+'.pkl')   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dict_UT_young_ref_num_cit_by_focus_year\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "len(dict_UT_young_ref_num_cit_by_focus_year)\n",
    "\n",
    "aux_dict = dict_UT_young_ref_num_cit_by_focus_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ######## extract number of citations of young references early on.\n",
    "\n",
    "\n",
    "# focus_plos_year = 2011  # the main figures of my paper correspond to plos papers published in 2011, and the young references are at most one year old == 2011: \n",
    "# ############# how many citations did young references have by 2011 (by focus year)???\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_selection_general_young_ref = df_merged[ (df_merged['ref_pub_year'] >= (focus_plos_year - 1) )]  \n",
    "\n",
    "\n",
    "# list_UT_young_ref_by_focus_year = df_selection_general_young_ref.reference_UT.unique()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"Total # records of young ref. by\", focus_year, \": \",df_selection_general_young_ref.shape, \"  # unique ref_UTs:\", len(list_UT_young_ref_by_focus_year))   # \n",
    "\n",
    "\n",
    "\n",
    "# dict_UT_young_ref_num_cit_by_focus_year = {}\n",
    "\n",
    "# list_all_citing_appers_of_ref = []   \n",
    "\n",
    "\n",
    "# for ref_UT in list_all_ref_UT:  # i need to assign a value to all references  (those that were older, get a NaN, those that are young enough (one year old or younger), get whatever number of citations they had by focus_year)           \n",
    "#     dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = np.nan  # first i initialize all ref to nan  (faster than evaluating whether the reference is in the list of selected ones or not)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# cont = 0      # 190644#\n",
    "# #for ref_UT in list_aux_UT:\n",
    "# for ref_UT in list_UT_young_ref_by_focus_year:  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ref_UT = str(ref_UT)  # i make sure it is a string\n",
    "#    # print (\"\\n\",cont, ref_UT)       \n",
    "    \n",
    "\n",
    "# #    item = papers_con.collection.find_one({\"UT\" : ref_UT}, {\"UT\":1,\"issue.PY\":1,'citations':1})  # the output of a find_one cursor is a DICTIONARY!!!\n",
    "\n",
    "#     item = papers_con.collection.find_one({\"UT\" : ref_UT}, {'citations':1})  # the output of a find_one cursor is a DICTIONARY!!!\n",
    "\n",
    "#     # citations:   cites received by the paper (currently updated value!)\n",
    "        \n",
    "\n",
    "#     dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "\n",
    "#     try:\n",
    "#         list_citing_papers = item[\"citations\"]                                \n",
    "    \n",
    "#         aux_list_citing_papers = list_citing_papers\n",
    "\n",
    "\n",
    "\n",
    "#         ### QUERY TO GET ALL CITATIONS RECEIVED BY THE REFERENCE   (i dont really need it)\n",
    "#        # count_cursor1 = papers_con.collection.find( { \"UT\" :{\"$in\":aux_list_citing_papers} },    {\"UT\":1,\"issue.PY\":1,'citations':1}).count()  ## selects the documents where the value of the field is less than or equal to (i.e. <=) the specified value.\n",
    "#         #print(cont, ref_UT, \"# tot cit:\",count_cursor1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         ### QUERY TO GET ONLY CITATIONS RECEIVED BY THE REFERENCE AROUND THE FOCUS_YEAR\n",
    "#         #cursor2_count = papers_con.collection.find( {  \"UT\" :{\"$in\":aux_list_citing_papers }, \"issue.PY\": { '$lte': focus_plos_year }}   ,    {\"UT\":1,\"issue.PY\":1,'citations':1}).count()\n",
    "#         cursor2_count = papers_con.collection.find( {  \"UT\" :{\"$in\":aux_list_citing_papers }, \"issue.PY\": { '$lte': focus_plos_year }} ).count()  #i only care about how many fulfill the conditions\n",
    "\n",
    "#         dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = cursor2_count\n",
    "\n",
    "        \n",
    "        \n",
    "#     except :  # IF NO CITATIONS IN WOS RECORDS FOR THAT PAPER\n",
    "# #         list_citing_papers=[]\n",
    "#          dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#     print(cont)#, ref_UT, \"# tot cit:\",count_cursor1, \"# cit by\",focus_year, \":\",dict_UT_young_ref_num_cit_by_focus_year[ref_UT])       \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "#    # input()\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   \n",
    "        \n",
    "#     cont +=1  \n",
    "        \n",
    "        \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# print (\"done.\", len(dict_UT_young_ref_num_cit_by_focus_year),\"dumping dict ref.......\")         \n",
    "# with open('../data/dict_UT_young_ref_in'+str(focus_year)+'_num_cit_by_'+str(focus_year)+'.pkl', 'wb') as handle:\n",
    "#          pickle.dump(dict_UT_young_ref_num_cit_by_focus_year, handle, protocol = 2)\n",
    "# print (\"written:\",'../data/dict_UT_young_ref_in'+str(focus_year)+'_num_cit_by_'+str(focus_year)+'.pkl')   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # dict_UT_young_ref_num_cit_by_focus_year\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list_aux_UT= []\n",
    "\n",
    "# for UT in list_UT_young_ref_by_focus_year:\n",
    "#     value= dict_UT_young_ref_num_cit_by_focus_year[UT]\n",
    "#     if value >=0:    # OJO! THIS IS A COMPARISON WITH NP.NAN! (WHICH IS NEVER TRUE, EITHER FOR <= NOR FOR >=)\n",
    "#         pass\n",
    "#     else:\n",
    "#         list_aux_UT.append(UT)\n",
    "\n",
    "        \n",
    "# print (len(list_aux_UT), len(dict_UT_young_ref_num_cit_by_focus_year), len(list_UT_young_ref_by_focus_year))\n",
    "# # n = np.nan\n",
    "\n",
    "# # if n >= 0:\n",
    "# #     print (1)\n",
    "\n",
    "# # 190644 + 438100\n",
    "\n",
    "\n",
    "len(dict_UT_young_ref_num_cit_by_focus_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############  REMOVE PREVIOUS VERSION (CELL WITHOUT LOOP OVER FOCUS_YEARS) AFTER RUNNING (THIS ONE IS A GENERAL VERSION WITH A LOOK OVER FOCUS_YEARS)\n",
    "\n",
    "######## extract number of citations of young references early on.\n",
    "\n",
    "\n",
    "list_focus_years = [2009,2010,2011,2012,2013]\n",
    "\n",
    "\n",
    "def_young = 1  # young references are <= 1 year old\n",
    "\n",
    "\n",
    "\n",
    "#focus_plos_year = 2011  # the main figures of my paper correspond to plos papers published in 2011, and the young references are at most one year old == 2011: \n",
    "############# how many citations did young references have by 2011 (by focus year)???\n",
    "\n",
    "\n",
    "for focus_plos_year in list_focus_years:\n",
    "    \n",
    "    \n",
    "\n",
    "    df_selection_general_young_ref = df_merged[ (df_merged['ref_pub_year'] >= (focus_plos_year - def_young) )]  \n",
    "\n",
    "\n",
    "    list_UT_young_ref_by_focus_year = df_selection_general_young_ref.reference_UT.unique()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"\\n\\nTotal # records of young ref. by\", focus_plos_year, \": \",df_selection_general_young_ref.shape, \"  # unique ref_UTs:\", len(list_UT_young_ref_by_focus_year))   \n",
    "\n",
    "\n",
    "    dict_UT_young_ref_num_cit_by_focus_year = {}\n",
    "\n",
    "    list_all_citing_appers_of_ref = []   \n",
    "\n",
    "\n",
    "    for ref_UT in list_all_ref_UT:  # i need to assign a value to all references  (those that were older, get a NaN, those that are young enough (one year old or younger), get whatever number of citations they had by focus_year)           \n",
    "        dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = np.nan  # first i initialize all ref to nan  (faster than evaluating whether the reference is in the list of selected ones or not)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    cont = 0    \n",
    "    for ref_UT in list_UT_young_ref_by_focus_year:  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ref_UT = str(ref_UT)  # i make sure it is a string\n",
    "       # print (\"\\n\",cont, ref_UT)       \n",
    "\n",
    "\n",
    "        item = papers_con.collection.find_one({\"UT\" : ref_UT}, {'citations':1})  # the output of a find_one cursor is a DICTIONARY!!!   citations:   cites received by the paper (currently updated value!)\n",
    "\n",
    "     \n",
    "        dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "\n",
    "        try:\n",
    "            list_citing_papers = item[\"citations\"]                                \n",
    "\n",
    "            aux_list_citing_papers = list_citing_papers\n",
    "\n",
    "\n",
    "\n",
    "            ### QUERY TO GET ALL CITATIONS RECEIVED BY THE REFERENCE   (i dont really need it)\n",
    "           # count_cursor1 = papers_con.collection.find( { \"UT\" :{\"$in\":aux_list_citing_papers} },    {\"UT\":1,\"issue.PY\":1,'citations':1}).count()  ## selects the documents where the value of the field is less than or equal to (i.e. <=) the specified value.\n",
    "            #print(cont, ref_UT, \"# tot cit:\",count_cursor1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ### QUERY TO GET ONLY CITATIONS RECEIVED BY THE REFERENCE AROUND THE FOCUS_YEAR\n",
    "            #cursor2_count = papers_con.collection.find( {  \"UT\" :{\"$in\":aux_list_citing_papers }, \"issue.PY\": { '$lte': focus_plos_year }}   ,    {\"UT\":1,\"issue.PY\":1,'citations':1}).count()\n",
    "            cursor2_count = papers_con.collection.find( {  \"UT\" :{\"$in\":aux_list_citing_papers }, \"issue.PY\": { '$lte': focus_plos_year }} ).count()  #i only care about how many fulfill the conditions\n",
    "\n",
    "            #        ### MONGODB:   find with multiple conditions: find( {condition1, condition2}, {retrieve_field1, retrieve_field2})   # NO EXTRA BRACKETS, OR IT WONT WORK!!!!\n",
    "\n",
    "            \n",
    "            dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = cursor2_count\n",
    "\n",
    "\n",
    "\n",
    "        except :  # IF NO CITATIONS IN WOS RECORDS FOR THAT PAPER   \n",
    "             dict_UT_young_ref_num_cit_by_focus_year[ref_UT] = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print(cont)#, ref_UT, \"# tot cit:\",count_cursor1, \"# cit by\",focus_year, \":\",dict_UT_young_ref_num_cit_by_focus_year[ref_UT])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        cont +=1  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"done.\", len(dict_UT_young_ref_num_cit_by_focus_year),\"dumping dict ref.......\")         \n",
    "    with open('../data/dict_UT_young_ref_in'+str(focus_plos_year)+'_num_cit_by_'+str(focus_plos_year)+'.pkl', 'wb') as handle:\n",
    "             pickle.dump(dict_UT_young_ref_num_cit_by_focus_year, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/dict_UT_young_ref_in'+str(focus_plos_year)+'_num_cit_by_'+str(focus_plos_year)+'.pkl')   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 000290014500020 # tot cit: 22 # cit by 2011 : 1\n",
    "\n",
    "# 000274470200014 # tot cit: 15 # cit by 2011 : 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preselection_df = df_merged[df_merged['reference_UT'].isin(list_ref_UT_post2005)] \n",
    "\n",
    "# preselection_df\n",
    "dict_UT_young_ref_num_cit_by_focus_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_merged[df_merged['self_citation']==1][['paper_UT', 'reference_UT']]\n",
    "print (len(set(tot_list_papers)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# lista_all_plos_UTs = pickle.load(open('/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/data/lista_all_plos_UTs.pkl', 'rb'))\n",
    "# new_lista_all_plos_UTs = ['000'+str(item) for item in lista_all_plos_UTs]   # i need to add the 000 so it matches the db UTs\n",
    "\n",
    "# #print (len(new_lista_all_plos_UTs))\n",
    "\n",
    "\n",
    "# # In[15]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #lista_UTs=['000322590800016','000254928800015','000305349100026','000321341000076']\n",
    "# dict_UT_plos_DOI={}\n",
    "# dict_DOI_plos_UT={}\n",
    "\n",
    "\n",
    "# start=0\n",
    "# stop=1000\n",
    "# while stop <=159000:\n",
    "\n",
    "\n",
    "#     lista=new_lista_all_plos_UTs[start:stop]\n",
    "#     query = papers_con.collection.find({\"UT\":{\"$in\":lista}},{\"UT\":1,\"AR\":1}, no_cursor_timeout=True)  \n",
    "\n",
    "\n",
    "#     #print (start, stop)\n",
    "\n",
    "#     for item in query:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict\n",
    "\n",
    "#         UT=item[\"UT\"]\n",
    "#         doi=item['AR'][-1]\n",
    "        \n",
    "#         #string =str(UT)+\" \"+str(doi)\n",
    "#         if \".pone.\" in doi:\n",
    "#             #print ( doi)\n",
    "#             dict_UT_plos_DOI[UT]=doi\n",
    "            \n",
    "#             dict_DOI_plos_UT[doi]=UT\n",
    "\n",
    "#     stop +=1000\n",
    "#     start +=1000\n",
    "        \n",
    "     \n",
    "#     query.close()  # because i am using the no_cursor_timeout=True, i need also this, or cursor keeps waiting so ur resources are used up\n",
    "        \n",
    "# print (\"done.\", len(dict_UT_plos_DOI))   # 142723 from plos ONE\n",
    "\n",
    "# # DOI 10.1371/journal.pone.0004056\n",
    "# # DOI 10.1371/journal.pone.0004126\n",
    "# # DOI 10.1371/journal.pone.0004392\n",
    "# # DOI 10.1371/journal.pone.0004404\n",
    "# # DOI 10.1371/journal.pone.0004432\n",
    "# # DOI 10.1371/journal.pone.0004463"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dict_UT_plos_subject={}\n",
    "# lista_UT=[]\n",
    "# lista_subjects=[]\n",
    "\n",
    "# with open('/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/data/plos_subjects/plos_subjects_N100.json') as json_data:\n",
    "#     d = json.load(json_data) # a dict where the key is the doi and the value is another dict\n",
    "#     for doi in d:\n",
    "#         subject=str(d[doi]['subject_level_1']).replace(\"[\",\"\").replace(\"]\",\"\").replace(\", \",\"-\").replace(\"'\",\"\")\n",
    "       \n",
    "#         if \".pone.\" in doi:\n",
    "#             #print (doi)\n",
    "#             try:\n",
    "#                 UT = dict_DOI_plos_UT['DOI '+doi]\n",
    "#                 dict_UT_plos_subject[UT]=subject\n",
    "#                 lista_UT.append(UT)\n",
    "#                 lista_subjects.append(subject)\n",
    "#             #input()\n",
    "#                 print (UT,doi, subject)\n",
    "#             except KeyError: pass\n",
    "#     #df[df['A'].str.contains(\"Hello|Britain\")==True]     or    df[df['model'].str.contains('ac')]\n",
    "    \n",
    "# print (\"done.\", len(dict_UT_plos_subject))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in sorted(dict_UT_plos_DOI.items(), key=operator.itemgetter(1)):\n",
    "    print (item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lista_llaves=list(dict_UT_plos_subject.keys())\n",
    "# lista_values=list(dict_UT_plos_subject.values())\n",
    "# new_df =  pd.DataFrame(\n",
    "#     {'paper_UT': lista_llaves,\n",
    "#      'subject': lista_values\n",
    "#     })\n",
    "\n",
    "# new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df1 =  pd.DataFrame(\n",
    "#     {'paper_UT': lista_UT,\n",
    "#      'subject': lista_subjects\n",
    "#     })\n",
    "\n",
    "# new_df1.subject.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# field_string =  'Physic'\n",
    "# new_df1[new_df1['subject'].str.contains(field_string)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista_peq_UTs=['000351275700019','000351275700002','000351275700021','000351275700005']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
