{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing data set from the perspective of Journals (Article type later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df_min = pickle.load(open('../ref_dataframe_min_full.pkl', 'rb'))\n",
    "citation_df = pickle.load(open('../citation_dataframe_full.pkl', 'rb'))\n",
    "plos_df = pickle.load(open('../plos_paper_dataframe_full.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df_min = pickle.load(open('/Users/Nathan/dataframes_2/dataframes_v3/ref_dataframe_min_full.pkl', 'rb'))\n",
    "plos_df = pickle.load(open('/Users/Nathan/dataframes_2/dataframes_v3/plos_paper_dataframe_full.pkl', 'rb'))\n",
    "cite_df = pickle.load(open('/Users/Nathan/dataframes_2/dataframes_v3/citation_dataframe_full.pkl', 'rb'))\n",
    "result = ref_df_min.join(cite_df, on='reference_UT')\n",
    "ref_df = result.join(plos_df, on='paper_UT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = ref_df_min.join(citation_df, on='reference_UT')\n",
    "ref_df = result.join(plos_df, on='paper_UT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing null reference columns (8907763 rows vs 10848620 rows)\n",
    "ref_df = ref_df.loc[(ref_df['reference_UT']!=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def section_regex_parser(ref_df):\n",
    "# Using regular expressions to sort sections. Reach a concensus on section identification by scanning both section_title and section_title_alt\n",
    "    import regex as re\n",
    "\n",
    "    sect_index_dict = {'intro': 0, 'methods': 1, 'results': 2, 'disc': 3, 'res_disc':4, 'concl':5, 'mixed':6, 'na':7}\n",
    "\n",
    "    intro_re = re.compile(r'(intro)')\n",
    "    method_re = re.compile(r'(method)')\n",
    "    results_re = re.compile(r'(results)')\n",
    "    disc_re = re.compile(r'(disc)')\n",
    "    concl_re = re.compile(r'(conclu)')\n",
    "    backgr_re = re.compile(r'(backgr)')\n",
    "    mater_re = re.compile(r'(mater)')\n",
    "    count = 0\n",
    "    judgement = []\n",
    "\n",
    "    for i in range(len(ref_df)):\n",
    "        count += 1\n",
    "        if count%100000 == 0:\n",
    "            print(count)\n",
    "        sect = ref_df.iloc[i]['section_title']\n",
    "        sect_alt = ref_df.iloc[i]['section_title_alt']\n",
    "        sect_tag = -1\n",
    "        sect_alt_tag = -1\n",
    "        sect_final = -1\n",
    "\n",
    "        if sect == None:\n",
    "            sect_tag = 'na'\n",
    "        else:\n",
    "            if re.search(intro_re, sect.lower()) or re.search(backgr_re, sect.lower()):\n",
    "                sect_tag = 'intro'\n",
    "\n",
    "            elif re.search(method_re,sect.lower()) or re.search(mater_re, sect.lower()):\n",
    "                if re.search(results_re, sect.lower()):\n",
    "                    sect_tag = 'mixed'\n",
    "\n",
    "                elif re.search(disc_re, sect.lower()):\n",
    "                    sect_tag = 'mixed'\n",
    "\n",
    "                else:\n",
    "                    sect_tag = 'methods'\n",
    "\n",
    "            elif re.search(results_re, sect.lower()):\n",
    "                if re.search(disc_re, sect.lower()):\n",
    "                    sect_tag = 'res_disc'\n",
    "                else:\n",
    "                    sect_tag = 'results'\n",
    "\n",
    "            elif re.search(disc_re, sect.lower()):\n",
    "                sect_tag = 'disc'\n",
    "\n",
    "            elif re.search(concl_re, sect.lower()):\n",
    "                sect_tag = 'concl'\n",
    "\n",
    "            else:\n",
    "                sect_tag = 'na'\n",
    "\n",
    "\n",
    "\n",
    "        if sect_alt == None:\n",
    "            sect_alt_tag = 'na'\n",
    "        else:\n",
    "            if re.search(intro_re, sect_alt.lower()) or re.search(backgr_re, sect_alt.lower()):\n",
    "                sect_alt_tag = 'intro'\n",
    "\n",
    "            elif re.search(method_re, sect_alt.lower()) or re.search(mater_re, sect_alt.lower()):\n",
    "                if re.search(results_re, sect_alt.lower()):\n",
    "                    sect_alt_tag = 'mixed'\n",
    "\n",
    "                elif re.search(disc_re, sect_alt.lower()):\n",
    "                    sect_alt_tag = 'mixed'\n",
    "\n",
    "                else:\n",
    "                    sect_alt_tag = 'methods'\n",
    "\n",
    "            elif re.search(results_re, sect_alt.lower()):\n",
    "                if re.search(disc_re, sect_alt.lower()):\n",
    "                    sect_alt_tag = 'res_disc'\n",
    "                else:\n",
    "                    sect_alt_tag = 'results'\n",
    "\n",
    "            elif re.search(disc_re, sect_alt.lower()):\n",
    "                sect_alt_tag = 'disc'\n",
    "\n",
    "            elif re.search(concl_re, sect_alt.lower()):\n",
    "                sect_alt_tag = 'concl'\n",
    "\n",
    "            else:\n",
    "                sect_alt_tag = 'na'\n",
    "\n",
    "\n",
    "\n",
    "        if sect_tag == sect_alt_tag: # Confident on label\n",
    "            sect_final = sect_tag\n",
    "            #ref_df.iloc[i]['regex_sect_index'] = sect_index_dict[sect_tag]\n",
    "\n",
    "        else:\n",
    "            if sect_tag == 'na':\n",
    "                sect_final = sect_alt_tag\n",
    "                #ref_df.iloc[i]['regex_sect_index'] = sect_index_dict[sect_alt_tag]\n",
    "            elif sect_alt_tag == 'na':\n",
    "                sect_final = sect_tag\n",
    "\n",
    "            elif sect_tag == 'mixed':\n",
    "                sect_final = sect_alt_tag\n",
    "\n",
    "            elif sect_alt_tag == 'mixed':\n",
    "                sect_final = sect_tag\n",
    "\n",
    "            elif sect_alt_tag == 'concl': # Conclusion is a plos-based standard that includes discussion/results and discussion\n",
    "                sect_final = sect_tag\n",
    "            else:\n",
    "                sect_final = sect_alt_tag\n",
    "\n",
    "        judgement.append(sect_final)\n",
    "    return judgement\n",
    "        #ref_df.iloc[i]['regex_sect_index'] = sect_index_dict[sect_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "judgement = section_regex_parser(ref_df)\n",
    "judgement_2 = []\n",
    "sect_index_dict = {'intro': 0, 'methods': 1, 'results': 2, 'disc': 3, 'res_disc':4, 'concl':5, 'mixed':6, 'na':7}\n",
    "for i in judgement:\n",
    "    judgement_2.append(sect_index_dict[i])\n",
    "ref_df['regex_sect_index'] = judgement_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "i=0\n",
    "#k = np.log2(i)\n",
    "bin_points = []\n",
    "while i<16:\n",
    "    k = 2**i\n",
    "    i+=1\n",
    "    bin_points.append(k)\n",
    "print(bin_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def err_bars_dist_plots(pd_column, folds, total_occs, n_cols):\n",
    "    err_list = []\n",
    "    for j in range(folds):\n",
    "        errs = np.random.choice(pd_column, size = total_occs, replace= True)\n",
    "        unique_vals, unique_counts = np.unique(errs, return_counts = True)\n",
    "        err_list_current = []\n",
    "                \n",
    "        if len(unique_vals) != n_cols:\n",
    "            err_list_current = np.array([])\n",
    "            for jj in range(n_cols):\n",
    "                if jj in unique_vals:\n",
    "                    \n",
    "                    err_list_current = np.append(err_list_current,unique_counts[np.where(unique_vals==jj)])\n",
    "                else:\n",
    "                    err_list_current = np.append(err_list_current,0)\n",
    "            err_list.append(err_list_current)\n",
    "        else:\n",
    "            err_list.append(unique_counts)\n",
    "\n",
    "    err_max = np.array([0]*n_cols)\n",
    "    err_min = np.array([0]*n_cols)\n",
    "            \n",
    "    for k in range(n_cols):\n",
    "        col = [item[k] for item in err_list]\n",
    "        err_upper = np.percentile(col, 97.5)\n",
    "        err_lower = np.percentile(col, 2.5)\n",
    "        err_max[k] = err_upper\n",
    "        err_min[k] = err_lower\n",
    "    #print(err_upper, err_lower)    \n",
    "    return err_max, err_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime_filler(df, date_column, year_column):\n",
    "    # Add datetime column to dataframe based on month/season and year columns\n",
    "    import regex as re\n",
    "    import datetime\n",
    "    import math\n",
    "    numeric = re.compile('[0-9]+')\n",
    "    alpha = re.compile('[A-Za-z]+')\n",
    "    month_dict = {'JAN':1, 'FEB': 2, 'MAR':3, 'APR':4, 'MAY':5, 'JUN': 6, \n",
    "                  'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT':10, 'NOV': 11, 'DEC': 12,\n",
    "                 'SPR': 3, 'SUM': 6, 'FAL': 9, 'WIN': 12}\n",
    "    date_dict = {}\n",
    "    set_dates = df[date_column].unique()\n",
    "\n",
    "    for date_u in set_dates:\n",
    "        date = str(date_u)\n",
    "\n",
    "        date_field = []\n",
    "        for i in re.findall(alpha, date):\n",
    "            date_field.append(i)\n",
    "\n",
    "        for i in re.findall(numeric, date):\n",
    "            date_field.append(i)\n",
    "\n",
    "        if len(date_field) > 2: # No weirdness here, only 2 values max for date field\n",
    "            print(date_field)\n",
    "\n",
    "        if len(date_field[0])==1: # There's a single date that is just '0'. Likely a database error/nan\n",
    "            print(date_field)\n",
    "            print(date)\n",
    "        #print(date_field)\n",
    "        formatted_date = [-1,-1]\n",
    "        if date_field != ['0']:\n",
    "            if date_field[0] in month_dict:\n",
    "                formatted_date[0] = month_dict[date_field[0]]\n",
    "                if len(date_field)==2:\n",
    "                    if len(date_field[1])<3:\n",
    "                        formatted_date[1] = int(date_field[1])\n",
    "\n",
    "        #print(formatted_date, date_field)\n",
    "        date_dict[date] = formatted_date\n",
    "    count = 0    \n",
    "    dt_col = []\n",
    "    #dt_dict = {}\n",
    "    for i,row in ref_df.iterrows():\n",
    "        date = row[date_column]\n",
    "        if date in date_dict:\n",
    "            date_field = date_dict[date]\n",
    "        else:\n",
    "            date_field = [-1,-1]\n",
    "\n",
    "        \n",
    "        year = row[year_column]\n",
    "        if not math.isnan(year):\n",
    "            year = int(year)\n",
    "        else:\n",
    "            year = -1\n",
    "\n",
    "        if year != -1 and date_field != [-1,-1]:\n",
    "            if date_field[1] != -1:\n",
    "                dt = datetime.datetime(year, date_field[0], date_field[1])\n",
    "            else:\n",
    "                dt = datetime.datetime(year, date_field[0], 15)\n",
    "        else:\n",
    "            dt = -1\n",
    "        #dt_dict[] = dt\n",
    "        \n",
    "        dt_col.append(dt)\n",
    "        count+= 1\n",
    "        if count%100000 == 0:\n",
    "            print(count)\n",
    "            \n",
    "    return dt_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = datetime_filler(ref_df, 'ref_pub_date', 'ref_pub_year')\n",
    "dt_plos = datetime_filler(ref_df, 'plos_pub_date', 'plos_pub_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df['ref_datetime'] = dt\n",
    "ref_df['plos_datetime'] = dt_plos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter to papers at least 10 years old\n",
    "import datetime\n",
    "ten_years= datetime.datetime(2006, 1,1)\n",
    "non_nan_dates = ref_df[ref_df['ref_datetime']!=-1]\n",
    "ten_year_df = non_nan_dates[non_nan_dates['ref_datetime']< ten_years]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_profile_plotter_error_bars(ref_df, date_title, directory, fig_num):\n",
    "    # Plot distributions given full dataframe\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.suptitle('Citation distribution across regex sections (normalized, log2 bins) (' + date_title + ')', size=30)\n",
    "    bin_num = 8\n",
    "    subplot_num = 1\n",
    "    hist_list_raw = []\n",
    "    #hist_list = np.array([])\n",
    "    hist_list = []\n",
    "    count_totals = []\n",
    "    major_ticks_y = np.arange(0, 1.1, 0.1)\n",
    "    minor_ticks_y = np.arange(0, 1.01, 0.05)\n",
    "    major_ticks_x = np.arange(0, 9, 1)\n",
    "\n",
    "    bin_indices = np.arange(0,9,1)\n",
    "    error_tracker = []\n",
    "    for i in range(len(bin_points)-1):\n",
    "        #print(subplot_num)\n",
    "        #print(i)\n",
    "        if i == len(bin_points)-2:\n",
    "            upper_bound = int(max(ref_df['cite_count'])+1)\n",
    "            print(upper_bound)\n",
    "        else:\n",
    "            upper_bound = bin_points[i+1]\n",
    "            #print('asdfasdf')\n",
    "\n",
    "\n",
    "        cite_count_log_list = ref_df.loc[ref_df['cite_count'].isin(range(bin_points[i], upper_bound))]\n",
    "        title_text = str(bin_points[i]) + '-' + str(upper_bound-1) + ' citations'\n",
    "        unique_papers = len(cite_count_log_list['reference_UT'].unique())\n",
    "        total_occs = len(cite_count_log_list)\n",
    "        ax = fig.add_subplot(4,4,subplot_num)\n",
    "\n",
    "       \n",
    "            \n",
    "        hist1, bins1 = np.histogram(cite_count_log_list['regex_sect_index'], bins= bin_num, range=[0,8])\n",
    "        hist_list_raw.append(hist1)\n",
    "        widths1 = np.diff(bins1)\n",
    "        hist1 = hist1/float(len(cite_count_log_list))\n",
    "        hist_list.append(hist1)\n",
    "        count_totals.append(total_occs)\n",
    "        #hist_list = np.append(hist_list, hist1, axis=1)\n",
    "        #ax.bar(bins1[:-1], hist1, widths1)\n",
    "        if len(cite_count_log_list)> 1:\n",
    "            print(len(cite_count_log_list['regex_sect_index']))\n",
    "            sns.distplot(cite_count_log_list['regex_sect_index'], bins1, kde=False, norm_hist=True, hist_kws = {'alpha':0.5},axlabel = False, ax=ax)\n",
    "            try:\n",
    "                err_max, err_min = err_bars_dist_plots(cite_count_log_list['regex_sect_index'], 100, total_occs, 8)\n",
    "            except:\n",
    "                err_max, err_min = np.array([0]*8), np.array([0]*8)\n",
    "            error_tracker.append([abs(err_min/total_occs-hist1), abs(err_max/total_occs-hist1)])\n",
    "            #print(abs(err_min/total_occs-hist_list[i]), abs(err_max/total_occs-hist_list[i]))\n",
    "            #print(err_min, err_max)\n",
    "            ax.errorbar([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5], hist1, yerr=[abs(err_min/total_occs-hist1), abs(err_max/total_occs-hist1)], fmt='o',capsize = 10, capthick=1)\n",
    "\n",
    "            ax.xaxis.set_ticklabels(['    i', '   m', '    r', '   d', '  rd', '    c','  mx', '   na'])\n",
    "            for tick in ax.xaxis.get_majorticklabels():\n",
    "                tick.set_horizontalalignment(\"left\")\n",
    "\n",
    "            ax.title.set_text(title_text)\n",
    "\n",
    "            subtext1 = 'unique refs:  ' + str(unique_papers)\n",
    "            subtext2 = 'total occs:   ' + str(total_occs)\n",
    "            ax.text(7.3, 0.83, subtext1 + '\\n' + subtext2, style='italic', horizontalalignment='right',\n",
    "                bbox={'facecolor':'white', 'alpha':0.6, 'pad':10})\n",
    "\n",
    "            ax.grid(which='both')\n",
    "            ax.set_ylim([0,1])\n",
    "            ax.set_xlim([0,7])\n",
    "\n",
    "            ax.set_yticks(minor_ticks_y, minor=True)\n",
    "            ax.set_yticks(major_ticks_y)\n",
    "            ax.set_xticks(major_ticks_x)\n",
    "            ax.set_xticks(major_ticks_x, minor=True) # not a typo, just getting around the seaborn grid overwriting\n",
    "        else:\n",
    "            error_tracker.append([abs(np.array([0]*8)/total_occs-hist1), abs(np.array([0]*8)/total_occs-hist1)])\n",
    "        subplot_num +=1\n",
    "\n",
    "    # Plotting the distribution for all papers    \n",
    "    cite_count_all = ref_df.loc[(ref_df['cite_count']>=0)]\n",
    "    title_text = 'All citations'\n",
    "    unique_papers = len(cite_count_all['reference_UT'].unique())\n",
    "    total_occs = len(cite_count_all)\n",
    "    ax = fig.add_subplot(4,4,subplot_num)\n",
    "    \n",
    "    hist1, bins1 = np.histogram(cite_count_all['regex_sect_index'], bins= bin_num, range=[0,8])\n",
    "    hist_all_raw = hist1\n",
    "    widths1 = np.diff(bins1)\n",
    "    hist1 = hist1/float(len(cite_count_all))\n",
    "    hist_all = hist1\n",
    "    #ax.bar(bins1[:-1], hist1, widths1)\n",
    "    if len(cite_count_all)>1:\n",
    "        sns.distplot(cite_count_all['regex_sect_index'], bins1, kde=False, norm_hist=True, hist_kws = {'alpha':0.5}, axlabel = False, ax=ax)\n",
    "        try:\n",
    "            err_max, err_min = err_bars_dist_plots(cite_count_all['regex_sect_index'], 100, total_occs, 8)\n",
    "        except:\n",
    "            err_max, err_min = np.array([0]*8), np.array([0]*8)\n",
    "        error_tracker.append([abs(err_min/total_occs-hist_all), abs(err_max/total_occs-hist_all)])\n",
    "        ax.errorbar([0.5,1.5,2.5,3.5,4.5,5.5,6.5,7.5], hist_all, yerr=[abs(err_min/total_occs-hist_all), abs(err_max/total_occs-hist_all)], fmt='o',capsize = 10, capthick=1)\n",
    "\n",
    "        ax.xaxis.set_ticklabels(['    i', '   m', '    r', '   d', '  rd', '    c','  mx', '   na'])\n",
    "        for tick in ax.xaxis.get_majorticklabels():\n",
    "            tick.set_horizontalalignment(\"left\")\n",
    "\n",
    "        ax.title.set_text(title_text)\n",
    "        subtext1 = 'unique refs:   ' + str(unique_papers)\n",
    "        subtext2 = 'total occs:   ' + str(total_occs)\n",
    "\n",
    "        ax.text(7.3, 0.83, subtext1 + '\\n' + subtext2, style='italic', horizontalalignment='right',\n",
    "            bbox={'facecolor':'white', 'alpha':0.6, 'pad':10})\n",
    "        ax.grid(which='both')\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_xlim([0,7])\n",
    "\n",
    "        ax.set_yticks(minor_ticks_y, minor=True)\n",
    "        ax.set_yticks(major_ticks_y)\n",
    "        ax.set_xticks(major_ticks_x)\n",
    "        ax.set_xticks(major_ticks_x, minor=True) # not a typo, just getting around the seaborn grid overwriting\n",
    "    else:\n",
    "        error_tracker.append([abs(np.array([0]*8)/total_occs-hist_all), abs(np.array([0]*8)/total_occs-hist_all)])\n",
    "        \n",
    "    fig.text(0.5, 0.08, 'Section Label', ha='center', size = 30)\n",
    "    fig.text(0.07, 0.55, 'Fraction of Total Occurences', va='center', rotation='vertical', size = 30)\n",
    "\n",
    "    hist_list_raw.append(hist_all_raw)\n",
    "    hist_list.append(hist_all)\n",
    "    count_totals.append(total_occs)\n",
    "\n",
    "    plt.savefig(directory+'section_hist_'+date_title+'_'+str(fig_num))\n",
    "    plt.close()\n",
    "    return hist_list, np.asarray(hist_list_raw), count_totals, error_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist_profile_plotter(ref_df, year_title):\n",
    "    # Plot distributions given full dataframe\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    fig.suptitle('Citation distribution across regex sections (normalized, log2 bins) (' + year_title + ')', size=30)\n",
    "    bin_num = 8\n",
    "    subplot_num = 1\n",
    "    hist_list_raw = []\n",
    "    #hist_list = np.array([])\n",
    "    hist_list = []\n",
    "    count_totals = []\n",
    "\n",
    "    for i in range(len(bin_points)-1):\n",
    "        print(subplot_num)\n",
    "        #print(i)\n",
    "        if i == len(bin_points)-2:\n",
    "            upper_bound = int(max(ref_df['cite_count'])+1)\n",
    "            print(upper_bound)\n",
    "        else:\n",
    "            upper_bound = bin_points[i+1]\n",
    "            #print('asdfasdf')\n",
    "\n",
    "\n",
    "        cite_count_log_list = ref_df.loc[ref_df['cite_count'].isin(range(bin_points[i], upper_bound))]\n",
    "        title_text = str(bin_points[i]) + '-' + str(upper_bound-1) + ' citations'\n",
    "        unique_papers = len(cite_count_log_list['reference_UT'].unique())\n",
    "        total_occs = len(cite_count_log_list)\n",
    "        ax = fig.add_subplot(4,4,subplot_num)\n",
    "\n",
    "\n",
    "        hist1, bins1 = np.histogram(cite_count_log_list['regex_sect_index'], bins= bin_num, range=[0,8])\n",
    "        hist_list_raw.append(hist1)\n",
    "        widths1 = np.diff(bins1)\n",
    "        hist1 = hist1/float(len(cite_count_log_list))\n",
    "        hist_list.append(hist1)\n",
    "        count_totals.append(total_occs)\n",
    "        #hist_list = np.append(hist_list, hist1, axis=1)\n",
    "        ax.bar(bins1[:-1], hist1, widths1)\n",
    "\n",
    "        ax.title.set_text(title_text)\n",
    "\n",
    "        subtext1 = 'unique refs:  ' + str(unique_papers)\n",
    "        subtext2 = 'total occs:   ' + str(total_occs)\n",
    "        ax.text(7.3, 0.83, subtext1 + '\\n' + subtext2, style='italic', horizontalalignment='right',\n",
    "            bbox={'facecolor':'white', 'alpha':0.6, 'pad':10})\n",
    "\n",
    "        ax.grid()\n",
    "        ax.set_ylim([0,1])\n",
    "        ax.set_xlim([0,7])\n",
    "        ax.set_xticks([0,1,2,3,4,5,6,7,8])\n",
    "        subplot_num +=1\n",
    "\n",
    "    # Plotting the distribution for all papers    \n",
    "    cite_count_all = ref_df.loc[(ref_df['cite_count']>=0)]\n",
    "    title_text = 'All citations'\n",
    "    unique_papers = len(cite_count_all['reference_UT'].unique())\n",
    "    total_occs = len(cite_count_all)\n",
    "    ax = fig.add_subplot(4,4,subplot_num)\n",
    "    hist1, bins1 = np.histogram(cite_count_all['regex_sect_index'], bins= bin_num, range=[0,8])\n",
    "    hist_all_raw = hist1\n",
    "    widths1 = np.diff(bins1)\n",
    "    hist1 = hist1/float(len(cite_count_all))\n",
    "    hist_all = hist1\n",
    "    ax.bar(bins1[:-1], hist1, widths1)\n",
    "    ax.title.set_text(title_text)\n",
    "    subtext1 = 'unique refs:   ' + str(unique_papers)\n",
    "    subtext2 = 'total occs:   ' + str(total_occs)\n",
    "\n",
    "    ax.text(7.3, 0.83, subtext1 + '\\n' + subtext2, style='italic', horizontalalignment='right',\n",
    "        bbox={'facecolor':'white', 'alpha':0.6, 'pad':10})\n",
    "    ax.grid()\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlim([0,7])\n",
    "    ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8],['i','m','r','d','rd','c','mx','na',''])\n",
    "\n",
    "\n",
    "    fig.text(0.5, 0.08, 'Section Label', ha='center', size = 30)\n",
    "    fig.text(0.07, 0.55, 'Fraction of Total Occurences', va='center', rotation='vertical', size = 30)\n",
    "    \n",
    "    hist_list_raw.append(hist_all_raw)\n",
    "    hist_list.append(hist_all)\n",
    "    count_totals.append(total_occs)\n",
    "    return hist_list, np.asarray(hist_list_raw), count_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log2_curves_plot(hist_list, error_tracker, date_title, directory, fig_num):\n",
    "    # Plot summarized curved of the binned distributions\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    section_curves = np.asarray(hist_list).transpose()\n",
    "    sect_labels = ['Intro', 'Methods', 'Results', 'Disc', 'Results/Disc', 'Conclu', 'Mixed', 'N/A']\n",
    "\n",
    "    err_mins = [item[0] for item in error_tracker]\n",
    "    err_maxs = [item[1] for item in error_tracker]\n",
    "    err_min_list = []\n",
    "    err_max_list = []\n",
    "    for k in range(len(err_mins[0])):\n",
    "        err_min_list.append([item[k] for item in err_mins])\n",
    "        err_max_list.append([item[k] for item in err_maxs])\n",
    "    colors = sns.color_palette(\"husl\", 8)\n",
    "    for i in range(len(section_curves)):\n",
    "        #ax.fill_between(x, 0, 1, where=y > theta, facecolor='green', alpha=0.5, transform=trans)\n",
    "        \n",
    "        plt.plot(section_curves[i], color = colors[i], marker = 'o')\n",
    "        plt.fill_between([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], -np.array(err_min_list[i])+section_curves[i], np.array(err_max_list[i])+section_curves[i], color = colors[i], alpha = 0.5)\n",
    "        #plt.plot(np.array(err_min_list[i])+section_curves[i], marker='+')\n",
    "        #plt.plot(np.array(err_max_list[i])+section_curves[i], marker='+')\n",
    "        #sns.tsplot(section_curves[i])\n",
    "        #plt.errorbar([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], section_curves[i], yerr=[err_min_list[i], err_max_list[i]], fmt='o',capsize = 10, capthick=1)\n",
    "        #print(section_curves)\n",
    "    plt.legend(sect_labels, loc='upper left')\n",
    "    \n",
    "    plt.ylim([0,1.0])\n",
    "    plt.xlim([0,15])\n",
    "    fig.suptitle('Section occurence fraction vs. citation bin (normalized, log2 bins) (' + date_title + ')' , size=16)\n",
    "    fig.text(0.5, 0.08, 'Bin number', ha='center', size = 15)\n",
    "    fig.text(0.07, 0.55, 'Fraction of Total Occurences', va='center', rotation='vertical', size = 15)\n",
    "    plt.savefig(directory+'section_line_graph_'+date_title+'_'+str(fig_num))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_filtered_hist_plotter(ref_df,year,month,day,column,older=True, first_dist_basis = True):\n",
    "    \n",
    "    dt = datetime.datetime(year,month,day)\n",
    "    non_nan_dates = ref_df[ref_df[column]!=-1]\n",
    "    if older:\n",
    "        filtered_df = non_nan_dates[non_nan_dates[column]< dt]\n",
    "    else:\n",
    "        filtered_df = non_nan_dates[non_nan_dates[column]> dt]\n",
    "\n",
    "    n_year_hist_list, hist_list_raw, count_totals, error_tracker = dist_profile_plotter_error_bars(filtered_df, str(year))\n",
    "    \n",
    "    log2_curves_plot(n_year_hist_list, error_tracker, str(year))\n",
    "    #chisqs, p_vals = chisquare_tester(hist_list_raw[:, 0:6]) # Only selecting the first five columns (to avoid the small population/un-useful section labels)\n",
    "    chisqs, p_vals = chisquare_tester(np.asarray(n_year_hist_list)[:, 0:4], count_totals, first_dist_basis = first_dist_basis)\n",
    "    #print(hist_list, hist_list_raw)\n",
    "    return chisqs, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import scipy.stats as sps\n",
    "\n",
    "try:\n",
    "    from itertools import izip as zip\n",
    "except ImportError: # will be 3.x series\n",
    "    pass\n",
    "\n",
    "def pairwise(iterable):\n",
    "    \"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\n",
    "    a, b = itertools.tee(iterable)\n",
    "    next(b, None)\n",
    "    return zip(a, b)\n",
    "\n",
    "def chisquare_tester(hist_list, count_totals, first_dist_basis = True):\n",
    "    \n",
    "    chisqs = []\n",
    "    p_vals = []\n",
    "    \n",
    "    if first_dist_basis == True:\n",
    "        i=0\n",
    "    else:\n",
    "        i=1\n",
    "        \n",
    "    for x, y in pairwise(hist_list):\n",
    "    \n",
    "        first_dist = x*count_totals[i]\n",
    "        second_dist = y*count_totals[i]\n",
    "        #print(first_dist,second_dist)\n",
    "        #score, p = sps.chisquare(second_dist,first_dist, ddof = 0)\n",
    "        score, p, dof, expctd = sps.chi2_contingency([list(first_dist),list(second_dist)])\n",
    "        print(dof)\n",
    "        chisqs.append(score)\n",
    "        p_vals.append(p)\n",
    "        i+=1\n",
    "    return chisqs, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#../In-Text-Citations-Project/plots\n",
    "def time_filtered_hist_plotter_save_figs(ref_df,year,month,day,column, directory, fig_num, older=True, first_dist_basis = True):\n",
    "    \n",
    "    dt = datetime.datetime(year,month,day)\n",
    "    non_nan_dates = ref_df[ref_df[column]!=-1]\n",
    "    if older:\n",
    "        filtered_df = non_nan_dates[non_nan_dates[column]< dt]\n",
    "    else:\n",
    "        filtered_df = non_nan_dates[non_nan_dates[column]> dt]\n",
    "\n",
    "    n_year_hist_list, hist_list_raw, count_totals, error_tracker = dist_profile_plotter_error_bars(filtered_df, str(year), directory, fig_num)\n",
    "    \n",
    "    log2_curves_plot(n_year_hist_list, error_tracker, str(year), directory, fig_num)\n",
    "    #chisqs, p_vals = chisquare_tester(hist_list_raw[:, 0:6]) # Only selecting the first five columns (to avoid the small population/un-useful section labels)\n",
    "    chisqs, p_vals = chisquare_tester(np.asarray(n_year_hist_list)[:, 0:4], count_totals, first_dist_basis = first_dist_basis)\n",
    "    #print(hist_list, hist_list_raw)\n",
    "    return chisqs, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chisq_matrix(hist_list, count_totals):\n",
    "    chisqs = []\n",
    "    p_vals = []\n",
    "    for i in range(len(hist_list)):\n",
    "        first_dist = hist_list[i]*count_totals[i]\n",
    "        for j in range(len(hist_list)):\n",
    "            second_dist = hist_list[j]*count_totals[j]\n",
    "            \n",
    "            zeros1 = [i for i, x in enumerate(first_dist) if x == 0]\n",
    "            zeros2 = [i for i, x in enumerate(second_dist) if x == 0]\n",
    "            overlapping_inds= list(set(zeros1).intersection(zeros2))\n",
    "            if set(overlapping_inds):\n",
    "                for k in overlapping_inds:\n",
    "                    second_dist[k] +=0.0001\n",
    "            \n",
    "            score, p, dof, expctd = sps.chi2_contingency([list(first_dist),list(second_dist)])\n",
    "            chisqs.append(round(score,2))\n",
    "            #chisqs.append(score)\n",
    "            p_vals.append(p)\n",
    "            \n",
    "    chisqs_np = np.asarray(chisqs)\n",
    "    chisqs_rs = chisqs_np.reshape(16,16)\n",
    "    p_vals_np = np.asarray(p_vals)\n",
    "    p_vals_rs = p_vals_np.reshape(16,16)\n",
    "    return chisqs_rs, p_vals_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_filtered_hist_plotter_matrix(ref_df,year, month, day, column, older=True, first_dist_basis = True):\n",
    "    \n",
    "    dt = datetime.datetime(year,month,day)\n",
    "    non_nan_dates = ref_df[ref_df['ref_datetime']!=-1]\n",
    "    \n",
    "    if older:\n",
    "        filtered_df = non_nan_dates[non_nan_dates[column]< dt]\n",
    "    else:\n",
    "        filtered_df = non_nan_dates[non_nan_dates[column]> dt]\n",
    "\n",
    "    n_year_hist_list, hist_list_raw, count_totals = dist_profile_plotter(filtered_df, str(year))\n",
    "    \n",
    "    log2_curves_plot(n_year_hist_list, str(year))\n",
    "    chisqs, p_vals = chisq_matrix(np.asarray(n_year_hist_list)[:, 0:4], count_totals)\n",
    "    #print(hist_list, hist_list_raw)\n",
    "    return chisqs, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_filtered_hist_plotter_matrix_dual_window(ref_df,plos_dt_max, plos_dt_min, ref_dt_max, ref_dt_min, directory, fig_num, title_prefix, first_dist_basis = True):\n",
    "    \n",
    "    #dt = datetime.datetime(year,month,day)\n",
    "    non_nan_dates = ref_df[(ref_df['ref_datetime']!=-1) & (ref_df['plos_datetime']!=-1)]\n",
    "    print(len(non_nan_dates))\n",
    "    filtered_df = non_nan_dates[(non_nan_dates['ref_datetime']> ref_dt_min) & (non_nan_dates['ref_datetime']<= ref_dt_max) & (non_nan_dates['plos_datetime']> plos_dt_min) & (non_nan_dates['plos_datetime']<= plos_dt_max)]\n",
    "    print(len(filtered_df))\n",
    "    title = title_prefix + '_plos_' + str(plos_dt_min.year) +'-'+ str(plos_dt_max.year) + '_ref_' + str(ref_dt_min.year) +'-'+ str(ref_dt_max.year)\n",
    "    \n",
    "    n_year_hist_list, hist_list_raw, count_totals, error_tracker = dist_profile_plotter_error_bars(filtered_df, title, directory, fig_num)\n",
    "    #n_year_hist_list, hist_list_raw, count_totals = dist_profile_plotter(filtered_df, title)\n",
    "    \n",
    "    log2_curves_plot(n_year_hist_list, error_tracker, title, directory, fig_num)\n",
    "    #log2_curves_plot(n_year_hist_list, title)\n",
    "    try:\n",
    "        chisqs, p_vals = chisq_matrix(np.asarray(n_year_hist_list)[:, 0:4], count_totals)\n",
    "\n",
    "\n",
    "    except:\n",
    "        chisqs, p_vals = [None, None]\n",
    "    try:\n",
    "        make_chi2_table(chisqs, p_vals, title, directory, fig_num)\n",
    "    except:\n",
    "        pass\n",
    "    #print(hist_list, hist_list_raw)\n",
    "\n",
    "    return chisqs, p_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_chi2_table(chisqs, p_vals, date_title, directory, fig_num):\n",
    "   \n",
    "    p_val_pd = pd.DataFrame(p_vals)\n",
    "    cmap1 = ListedColormap(['w'])\n",
    "    #p_val_pd[p_val_pd > (0.05/(16*15/2))] = 1\n",
    "    p_val_pd[p_val_pd < (0.05/(16*15/2))] = 0\n",
    "    plt.figure(figsize = (15,12))\n",
    "    plt.title('P-values ' + date_title)\n",
    "    sns.heatmap(p_val_pd, annot=True)\n",
    "    sns.heatmap(p_val_pd, mask =p_val_pd>(0.05/(16*15/2)), cmap=cmap1, cbar=False,annot=True)\n",
    "    \n",
    "    '''p_val_pd = pd.DataFrame(p_vals)\n",
    "    \n",
    "    #p_val_pd[p_val_pd > (0.05/(16*15/2))] = 1\n",
    "    #p_val_pd[p_val_pd < (0.05/(16*15/2))] = 0\n",
    "    plt.figure(figsize = (15,12))\n",
    "    plt.title('P-values ' + date_title)\n",
    "    sns.heatmap(p_val_pd, annot=True)'''\n",
    "    plt.savefig(directory+'chi_squared_'+date_title+'_'+str(fig_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_itc_hists(ref_df, directory):\n",
    "    min_plos_date = 2005\n",
    "    max_plos_date = 2016\n",
    "    min_ref_date = 1900\n",
    "    max_ref_date = 2016\n",
    "    plos_window = 11\n",
    "    ref_window = 116\n",
    "\n",
    "    fig_num = 1\n",
    "    count = 0\n",
    "    year_list = []\n",
    "    for i in range(max_plos_date-min_plos_date-plos_window+1):\n",
    "        for j in range(max_ref_date-min_ref_date-ref_window+1):\n",
    "            year_list.append([i + min_plos_date, i + min_plos_date+plos_window, j + min_ref_date, j + min_ref_date+ref_window])\n",
    "            count+=1\n",
    "    print(count)\n",
    "    for years in year_list:\n",
    "        plos_dt_min = datetime.datetime(years[0],1,1)\n",
    "        plos_dt_max = datetime.datetime(years[1],1,1)\n",
    "        ref_dt_min = datetime.datetime(years[2],1,1)\n",
    "        ref_dt_max = datetime.datetime(years[3],1,1)\n",
    "        if ref_dt_min >= plos_dt_max:\n",
    "            pass\n",
    "        else:\n",
    "            time_filtered_hist_plotter_matrix_dual_window(ref_df,plos_dt_max, plos_dt_min, ref_dt_max, ref_dt_min, directory, fig_num)\n",
    "            fig_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up datetime series for future analyses\n",
    "dt_window = [datetime.datetime(2017,1,1), datetime.datetime(2011,1,1), datetime.datetime(2018,1,1), datetime.datetime(1900,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example of this combined program\n",
    "fig_dir = '../plots/journal_conditioning/'\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'All_Journals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df_pone = ref_df[ref_df['plos_j1'] == 'PLOS ONE']\n",
    "ref_df_pgen = ref_df[ref_df['plos_j1'] == 'PLOS GENET']\n",
    "ref_df_pntd = ref_df[ref_df['plos_j1'] == 'PLO NE TR D']\n",
    "ref_df_ppat = ref_df[ref_df['plos_j1'] == 'PLOS PATHOG']\n",
    "ref_df_pcom = ref_df[ref_df['plos_j1'] == 'PLOS COMPUT']\n",
    "ref_df_pbio = ref_df[ref_df['plos_j1'] == 'PLOS BIOL']\n",
    "ref_df_pmed = ref_df[ref_df['plos_j1'] == 'PLOS MED']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dir = '../plots/journal_conditioning/plos_basis/'\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pone,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_ONE')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pgen,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_GEN')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pntd,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_NTD')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ppat,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_PATH')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pcom,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_COMP')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pbio,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_BIO')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pmed,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_MED')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df_rpnas = ref_df[ref_df['ref_j1'] == 'P NAS US']\n",
    "ref_df_rpone = ref_df[ref_df['ref_j1'] == 'PLOS ONE']\n",
    "ref_df_rnat = ref_df[ref_df['ref_j1'] == 'NATURE']\n",
    "ref_df_rjbc = ref_df[ref_df['ref_j1'] == 'J BIOL CHEM']\n",
    "ref_df_rsci = ref_df[ref_df['ref_j1'] == 'SCIENCE']\n",
    "ref_df_rcell = ref_df[ref_df['ref_j1'] == 'CELL']\n",
    "ref_df_rjn = ref_df[ref_df['ref_j1'] == 'J NEUROSC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dir = '../plots/journal_conditioning/ref_basis/'\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'All_Journals')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rpnas,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PNAS')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rpone,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'PLOS_ONE')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rnat,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'NATURE')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rjbc,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'J_BIOL_CHEM')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rsci,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'SCIENCE')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rjn,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'J_NEUROSC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_rcell,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'CELL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Articles, letters, notes, chronology\n",
    "'''array(['@ Article', -1, 'R Review', 'N Note', 'E Editorial Material',\n",
    "       '8 Hardware Review', 'L Letter', 'D Discussion',\n",
    "       '9 Software Review', 'M Meeting Abstract', '0 Database Review',\n",
    "       '5 News Item', '6 Reprint', 'Y Poetry', 'I Biographical-Item',\n",
    "       'C Correction', 'C Correction, Addition', 'A Art Exhibit Review',\n",
    "       'B Book Review', '7 Bibliography', 'K Chronology',\n",
    "       'I Item About an Individual', 'F Film Review',\n",
    "       'Z Dance Performance Review', '2 Abstract of Published Item',\n",
    "       'X Excerpt', 'O Fiction, Creative Prose'], dtype=object)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering on article types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df_plos_arts = ref_df[ref_df['plos_article_type']== '@ Article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# option to keep reviews or not - there's a big chunk of PLOS reviews we have\n",
    "ref_df_ref_with_reviews = ref_df_plos_arts[(ref_df_plos_arts['ref_article_type']=='@ Article') | (ref_df_plos_arts['ref_article_type']=='L Letter') | (ref_df_plos_arts['ref_article_type']=='N Note') | (ref_df_plos_arts['ref_article_type']=='R Review')]\n",
    "ref_df_ref_no_reviews = ref_df_ref_with_reviews[ref_df_ref_with_reviews['ref_article_type']!='R Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df_ref_articles = ref_df_plos_arts[ref_df_plos_arts['ref_article_type']=='@ Article']\n",
    "ref_df_ref_letters = ref_df_plos_arts[ref_df_plos_arts['ref_article_type']=='L Letter']\n",
    "ref_df_ref_notes = ref_df_plos_arts[ref_df_plos_arts['ref_article_type']=='N Note']\n",
    "ref_df_ref_reviews = ref_df_plos_arts[ref_df_plos_arts['ref_article_type']=='R Review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig_dir = '../plots/article_conditioning/'\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_plos_arts,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'All_Refs')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ref_with_reviews,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'Filtered')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ref_no_reviews,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'Filtered_sans_reviews')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ref_articles,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'Ref_Articles')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ref_letters,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'Ref_Letters')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ref_notes,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'Ref_Notes')\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_ref_reviews,dt_window[0],dt_window[1],dt_window[2], dt_window[3], fig_dir, 1, 'Ref_Reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def make_chi2_table_v2(chisqs, p_vals, date_title):\n",
    "    #print(p_vals)\n",
    "    p_val_pd = pd.DataFrame(p_vals)\n",
    "    cmap1 = ListedColormap(['w'])\n",
    "    #p_val_pd[p_val_pd > (0.05/(16*15/2))] = 1\n",
    "    p_val_pd[p_val_pd < (0.05/(16*15/2))] = 0\n",
    "    plt.figure(figsize = (15,12))\n",
    "    plt.title('P-values ' + date_title)\n",
    "    sns.heatmap(p_val_pd, annot=True)\n",
    "    sns.heatmap(p_val_pd, mask =p_val_pd>(0.05/(16*15/2)), cmap=cmap1, cbar=False,annot=True)\n",
    "    plt.show()\n",
    "    #plt.savefig(directory+'chi_squared_'+date_title+'_'+str(fig_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_window = [datetime.datetime(2017,1,1), datetime.datetime(2010,1,1), datetime.datetime(2018,1,1), datetime.datetime(2000,1,1)]\n",
    "a,b = time_filtered_hist_plotter_matrix_dual_window(ref_df_pcom,dt_window[0],dt_window[1],dt_window[2], dt_window[3], '/Users/Nathan/', 1, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "make_chi2_table_v2(a, b, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "years = ref_df['plos_pub_year'].value_counts(sort=False)\n",
    "plt.plot([math.log10(x) for x in list(years)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
