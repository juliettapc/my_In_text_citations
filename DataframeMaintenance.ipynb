{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a post-initial collection of data from Web of Science. That is to say, any additional data fields not collected in the original assembly can be integrated into the primary dataframe through this notebook.\n",
    "\n",
    "## In this case, fields such as publication date, journal name fields, and topic fields could be extracted individually or in groups.\n",
    "\n",
    "## Citation count can be updated with this notebook as well.\n",
    "\n",
    "## This notebook can also be used on the dataframe split to generate the three minimalist dataframes to avoid redundant data and excessively large pickle files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This code is the MongoConnection class from the Amaral lab LabTools folder.\n",
    "\n",
    "from __future__ import print_function, unicode_literals\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class MongoConnection(object):\n",
    "    def __init__(self, cxnSettings, **kwargs):\n",
    "        self.settings = cxnSettings\n",
    "        self.mongoURI = self._constructURI()\n",
    "        self.connect(**kwargs)\n",
    "        self.ensure_index()\n",
    "\n",
    "    def _constructURI(self):\n",
    "        '''\n",
    "        Construct the mongo URI\n",
    "        '''\n",
    "        mongoURI = 'mongodb://'\n",
    "        #User/password handling\n",
    "        if 'user'in self.settings and 'password' in self.settings:\n",
    "            mongoURI += self.settings['user'] + ':' + self.settings['password']\n",
    "            mongoURI += '@'\n",
    "        elif 'user' in self.settings:\n",
    "            print('Missing password for given user, proceeding without either')\n",
    "        elif 'password' in self.settings:\n",
    "            print('Missing user for given passord, proceeding without either')\n",
    "        #Host and port\n",
    "        try:\n",
    "            mongoURI += self.settings['host'] + ':'\n",
    "        except KeyError:\n",
    "            print('Missing the hostname. Cannot connect without host')\n",
    "            sys.exit()\n",
    "        try:\n",
    "            mongoURI += str(self.settings['port'])\n",
    "        except KeyError:\n",
    "            print('Missing the port. Substituting default port of 27017')\n",
    "            mongoURI += str('27017')\n",
    "        return mongoURI\n",
    "\n",
    "    def connect(self, **kwargs):\n",
    "        '''\n",
    "        Establish the connection, database, and collection\n",
    "        '''\n",
    "        self.connection = MongoClient(self.mongoURI, **kwargs)\n",
    "        #########\n",
    "        try:\n",
    "            self.db = self.connection[self.settings['db']]\n",
    "        except KeyError:\n",
    "            print(\"Must specify a database as a 'db' key in the settings file\")\n",
    "            sys.exit()\n",
    "        #########\n",
    "        try:\n",
    "            self.collection = self.db[self.settings['collection']]\n",
    "        except KeyError:\n",
    "            print('Should have a collection.', end='')\n",
    "            print('Starting a collection in database', end='')\n",
    "            print(' for current connection as test.')\n",
    "            self.collection = self.db['test']\n",
    "\n",
    "    def tearDown(self):\n",
    "        '''\n",
    "        Closes the connection\n",
    "        '''\n",
    "        self.connection.close()\n",
    "\n",
    "    def ensure_index(self):\n",
    "        '''\n",
    "        Ensures the connection has all given indexes.\n",
    "        indexes: list of (`key`, `direction`) pairs.\n",
    "            See docs.mongodb.org/manual/core/indexes/ for possible `direction`\n",
    "            values.\n",
    "        '''\n",
    "        if 'indexes' in self.settings:\n",
    "            for index in self.settings['indexes']:\n",
    "                self.collection.ensure_index(index[0], **index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import os,glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#sys.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up MongoConnection settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_papers_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"merged_papers\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "\n",
    "issues_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"issues\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "\n",
    "journal_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"journals\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "papers_con = MongoConnection(merged_papers_settings)\n",
    "issue_con = MongoConnection(issues_settings)\n",
    "journal_con = MongoConnection(journal_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load up current dataframes (if split dfs are available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ref_df = pickle.load(open('../ref_dataframe_min_full.pkl', 'rb'))\n",
    "plos_df = pickle.load(open('../plos_paper_dataframe_full.pkl', 'rb'))\n",
    "cite_df = pickle.load(open('../citation_dataframe_full.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join dataframes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = ref_df.join(cite_df, on='reference_UT')\n",
    "ref_df_full = result.join(plos_df, on='paper_UT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If there is no split dataframes yet, and only a combined dataframe is available, load that instead. The split dataframes will be created at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ref_df_full = pickle.load(open('..', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Because extraction takes a long time, storing the results between sessions is sometimes necessary. Any data here is stored in a pickled dictionary\n",
    "\n",
    "## If no data has been collected, then initialize suppl_dict as an empty dictionary. Be sure to comment out immediately to prevent overwriting when rerunning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#suppl_dict = {}\n",
    "#suppl_dict= pickle.load(open('../suppl_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect all UTs for both PLOS paper and reference paper for data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plos_uts = ref_df_full['paper_UT'].unique()\n",
    "ref_uts = ref_df_full['reference_UT'].unique()\n",
    "all_uts = set(np.append(plos_uts,ref_uts))\n",
    "print(len(all_uts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If suppl_dict is empty, populate it with empty subdicts\n",
    "\n",
    "#if suppl_dict == {}:\n",
    "#    for ut in all_uts:\n",
    "#        suppl_dict[ut] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we simply check for a specific type of entry in our dictionary and compare to the total number of UTs above\n",
    "c = 0\n",
    "for i in suppl_dict:\n",
    "    if 'J9' in suppl_dict[i]:\n",
    "        c+=1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This cell extracts supplementary information for each PLOS paper and reference paper in WoS. Includes publication date, year, and article type\n",
    "\n",
    "count = 0\n",
    "for ut in list(suppl_dict.keys()): # in all_uts\n",
    "    count+=1\n",
    "    if count%5000==0:\n",
    "        print(count)\n",
    "    \n",
    "    if 'SC' not in suppl_dict[ut]: # If target field not in a UT's field, then look it up on WoS (prevents rechecking entries already collected)\n",
    "        doc = papers_con.collection.find_one({\"UT\":ut})\n",
    "        if doc != None:\n",
    "\n",
    "            #if 'PY' in doc['issue']:\n",
    "            #    suppl_dict[ut]['year'] = doc['issue']['PY']\n",
    "\n",
    "            #if 'DT' in doc:\n",
    "            #    suppl_dict[ut]['paper_type'] = doc['DT']\n",
    "\n",
    "            if 'citations' in doc:\n",
    "                suppl_dict[ut]['cite_count'] = len(doc['citations'])\n",
    "\n",
    "            if 'J1' in doc['issue']:\n",
    "                suppl_dict[ut]['J1'] = doc['issue']['J1']\n",
    "            if 'J2' in doc['issue']:\n",
    "                suppl_dict[ut]['J2'] = doc['issue']['J2']\n",
    "            if 'J9' in doc['issue']:\n",
    "                suppl_dict[ut]['J9'] = doc['issue']['J9']\n",
    "            if 'JI' in doc['issue']:\n",
    "                suppl_dict[ut]['JI'] = doc['issue']['JI']\n",
    "\n",
    "            if 'PD' in doc['issue']:\n",
    "                suppl_dict[ut]['pub_date'] = doc['issue']['PD']\n",
    "\n",
    "            if 'SC' in doc['issue']:\n",
    "                suppl_dict[ut]['field'] = doc['issue']['SC']\n",
    "        else:\n",
    "            print('Error in ut: ' + str(ut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if stopped/paused, dump the dictionary in between sessions\n",
    "\n",
    "with open('../suppl_dict.pkl', 'wb') as handle:\n",
    "    pickle.dump(suppl_dict, handle, protocol = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have a populated suppl_dict, need to fill in the main dataframe. This process could probably be simplified by editing only the small dataframes, but it shouldn't be a big issue since it shouldn't need to be done often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set up empty column for new data\n",
    "\n",
    "ref_df_full['plos_j1'] = -1\n",
    "ref_df_full['plos_j2'] = -1\n",
    "ref_df_full['plos_j9'] = -1\n",
    "ref_df_full['plos_ji'] = -1\n",
    "ref_df_full['ref_j1'] = -1\n",
    "ref_df_full['ref_j2'] = -1\n",
    "ref_df_full['ref_j9'] = -1\n",
    "ref_df_full['ref_ji'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''ref_df_full['plos_pub_date'] = float('nan')\n",
    "ref_df_full['plos_pub_year'] = float('nan')\n",
    "ref_df_full['plos_article_type'] = -1\n",
    "ref_df_full['plos_field'] = -1\n",
    "ref_df_full['ref_pub_date'] = float('nan')\n",
    "ref_df_full['ref_pub_year'] = float('nan')\n",
    "ref_df_full['ref_article_type'] = -1\n",
    "ref_df_full['ref_field'] = -1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill in all the new supplementary data in the ref_df. First get array of results in the proper UT order...\n",
    "\n",
    "count = 0\n",
    "\n",
    "'''plos_pub_dates = [-1]*len(ref_df_full)\n",
    "plos_pub_years = [-1]*len(ref_df_full)\n",
    "plos_article_types = [-1]*len(ref_df_full)\n",
    "plos_fields = [-1]*len(ref_df_full)\n",
    "\n",
    "ref_pub_dates = [-1]*len(ref_df_full)\n",
    "ref_pub_years = [-1]*len(ref_df_full)\n",
    "ref_article_types = [-1]*len(ref_df_full)\n",
    "ref_fields = [-1]*len(ref_df_full)'''\n",
    "\n",
    "plos_j1s = [-1]*len(ref_df_full)\n",
    "plos_j2s = [-1]*len(ref_df_full)\n",
    "plos_j9s = [-1]*len(ref_df_full)\n",
    "plos_jis = [-1]*len(ref_df_full)\n",
    "plos_cite_counts = [-1]*len(ref_df_full)\n",
    "\n",
    "ref_j1s = [-1]*len(ref_df_full)\n",
    "ref_j2s = [-1]*len(ref_df_full)\n",
    "ref_j9s = [-1]*len(ref_df_full)\n",
    "ref_jis = [-1]*len(ref_df_full)\n",
    "ref_cite_counts = [-1]*len(ref_df_full)\n",
    "\n",
    "\n",
    "for i in range(len(ref_df_full)):\n",
    "    plos_ut = ref_df_full.iloc[i]['paper_UT']\n",
    "    ref_ut = ref_df_full.iloc[i]['reference_UT']\n",
    "    \n",
    "    try:\n",
    "        plos_j1 = suppl_dict[plos_ut]['J1']\n",
    "    except:\n",
    "        plos_j1 = -1\n",
    "    try:\n",
    "        plos_j2 = suppl_dict[plos_ut]['J2']\n",
    "    except:\n",
    "        plos_j2 = -1\n",
    "    try:\n",
    "        plos_j9 = suppl_dict[plos_ut]['J9']\n",
    "    except:\n",
    "        plos_j9 = -1\n",
    "    try:\n",
    "        plos_ji = suppl_dict[plos_ut]['JI']\n",
    "    except:\n",
    "        plos_ji = -1\n",
    "    try:\n",
    "        plos_cite_count = suppl_dict[plos_ut]['cite_count']\n",
    "    except:\n",
    "        plos_cite_count = -1\n",
    "        \n",
    "        \n",
    "    '''try:\n",
    "        plos_pub_date = suppl_dict[plos_ut]['pub_date']\n",
    "    except:\n",
    "        plos_pub_date = float('nan')\n",
    "    try:\n",
    "        plos_pub_year = suppl_dict[plos_ut]['year']\n",
    "    except:\n",
    "        plos_pub_year = float('nan')\n",
    "    try:\n",
    "        plos_article_type = suppl_dict[plos_ut]['paper_type']\n",
    "    except:\n",
    "        plos_article_type = -1\n",
    "    try:\n",
    "        plos_field = suppl_dict[plos_ut]['field']\n",
    "    except:\n",
    "        plos_field = -1        \n",
    "        \n",
    "        \n",
    "    try:\n",
    "        ref_pub_date = suppl_dict[ref_ut]['pub_date']\n",
    "    except:\n",
    "        ref_pub_date = float('nan')\n",
    "    try:\n",
    "        ref_pub_year = suppl_dict[ref_ut]['year']\n",
    "    except:\n",
    "        ref_pub_year = float('nan')\n",
    "    try:\n",
    "        ref_article_type = suppl_dict[ref_ut]['paper_type']\n",
    "    except:\n",
    "        ref_article_type = -1\n",
    "    try:\n",
    "        ref_field = suppl_dict[ref_ut]['field']\n",
    "    except:\n",
    "        ref_field = -1\n",
    "    '''\n",
    "    try:\n",
    "        ref_j1 = suppl_dict[ref_ut]['J1']\n",
    "    except:\n",
    "        ref_j1 = -1\n",
    "    try:\n",
    "        ref_j2 = suppl_dict[ref_ut]['J2']\n",
    "    except:\n",
    "        ref_j2 = -1\n",
    "    try:\n",
    "        ref_j9 = suppl_dict[ref_ut]['J9']\n",
    "    except:\n",
    "        ref_j9 = -1\n",
    "    try:\n",
    "        ref_ji = suppl_dict[ref_ut]['JI']\n",
    "    except:\n",
    "        ref_ji = -1\n",
    "    try:\n",
    "        ref_cite_count = suppl_dict[ref_ut]['cite_count']\n",
    "    except:\n",
    "        ref_cite_count = -1\n",
    "        \n",
    "    '''plos_pub_dates[i]=plos_pub_date\n",
    "    plos_pub_years[i]=plos_pub_year\n",
    "    plos_article_types[i]=plos_article_type\n",
    "    plos_fields[i] = plos_field\n",
    "    \n",
    "    ref_pub_dates[i]=ref_pub_date\n",
    "    ref_pub_years[i]=ref_pub_year\n",
    "    ref_article_types[i]=ref_article_type\n",
    "    ref_fields[i]=ref_field'''\n",
    "    \n",
    "    plos_j1s[i] = plos_j1\n",
    "    plos_j2s[i] = plos_j2\n",
    "    plos_j9s[i] = plos_j9\n",
    "    plos_jis[i] = plos_ji\n",
    "    plos_cite_counts[i] = plos_cite_count\n",
    "\n",
    "    ref_j1s[i] = ref_j1\n",
    "    ref_j2s[i] = ref_j2\n",
    "    ref_j9s[i] = ref_j9\n",
    "    ref_jis[i] = ref_ji\n",
    "    ref_cite_counts[i] = ref_cite_count\n",
    "    \n",
    "    \n",
    "    count+=1\n",
    "    if count%50000==0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Then assign the array as a new column in the dataframe\n",
    "ref_df_full['plos_j1'] = plos_j1s\n",
    "ref_df_full['plos_j2'] = plos_j2s\n",
    "ref_df_full['plos_j9'] = plos_j9s\n",
    "ref_df_full['plos_ji'] = plos_jis\n",
    "\n",
    "ref_df_full['ref_j1'] = ref_j1s\n",
    "ref_df_full['ref_j2'] = ref_j2s\n",
    "ref_df_full['ref_j9'] = ref_j9s\n",
    "ref_df_full['ref_ji'] = ref_jis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''ref_df_full['plos_pub_date'] = plos_pub_dates\n",
    "ref_df_full['plos_pub_year'] = plos_pub_years\n",
    "ref_df_full['plos_article_type'] = plos_article_types\n",
    "ref_df_full['plos_field'] = plos_fields\n",
    "\n",
    "ref_df_full['ref_pub_date'] = ref_pub_dates\n",
    "ref_df_full['ref_pub_year'] = ref_pub_years\n",
    "ref_df_full['ref_article_type'] = ref_article_types\n",
    "ref_df_full['ref_field'] = ref_fields'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now to make our two smaller dataframes with the new data, simply select the associated colummns.\n",
    "#(The ref_dataframe_min df doesn't include any additional info gathered this way, so it's just these two)\n",
    "\n",
    "plos_df = ref_df_full[['paper_UT','paper_cite_count','total_refs', 'paper_char_total', 'paper_word_total', 'plos_pub_date', 'plos_pub_year', 'plos_article_type', 'plos_field', 'plos_j1', 'plos_j2', 'plos_j9', 'plos_ji']]\n",
    "citation_df = ref_df_full[['reference_UT','cite_count','ref_pub_date','ref_pub_year', 'ref_article_type', 'ref_field', 'ref_j1', 'ref_j2', 'ref_j9', 'ref_ji']]\n",
    "\n",
    "# Drop all non-unique fields. Note that coincidentally overlapping rows with -1 for ut will be dropped as well\n",
    "plos_df_unique = plos_df.drop_duplicates('paper_UT')\n",
    "citation_df_unique = citation_df.drop_duplicates('reference_UT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reassign the index to be the respective UT column. May have to manually drop non-information rows with \n",
    "#(indicated by a -1 UT, as no information is possible to be matched in primary dataframe)\n",
    "\n",
    "plos_df_small = plos_df_unique.set_index('paper_UT')\n",
    "#plos_df_small = plos_df_small.drop(-1,0)\n",
    "citation_df_small = citation_df_unique.set_index('reference_UT')\n",
    "#citation_df_small = citation_df_small.drop(-1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lastly, pickle them\n",
    "with open('../plos_paper_dataframe_full_new.pkl', 'wb') as handle:\n",
    "    pickle.dump(plos_df_small, handle, protocol = 2)\n",
    "with open('../citation_dataframe_ful_newl.pkl', 'wb') as handle:\n",
    "    pickle.dump(citation_df_small, handle, protocol = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For completeness, or if this is the first time splitting files, then drop all non-UT fields that are already contained in the plos_df and citation_df dataframes\n",
    "ref_df_min = ref_df_full.drop(['paper_cite_count','total_refs', 'paper_char_total', 'paper_word_total', 'plos_pub_date', 'plos_pub_year', 'plos_article_type', 'cite_count','ref_pub_date','ref_pub_year', 'ref_article_type', 'plos_field','ref_field', 'plos_field', 'plos_j1', 'ref_j1', 'ref_j2',\n",
    "       'ref_j9', 'ref_ji', 'plos_j2', 'plos_j9', 'plos_ji'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# And pickle it\n",
    "with open('/Users/Nathan/dataframes_2/ref_dataframe_min_full.pkl', 'wb') as handle:\n",
    "    pickle.dump(ref_df_min, handle, protocol = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
