{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "#Due to leftoevererrors in Nathan's python installation, some cleaning up occurs here\n",
    "#sys.path.append(\"./code/\")\n",
    "#sys.path.remove('/usr/local/lib/python2.7/site-packages') \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import os,glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "import random\n",
    "from  scipy import stats\n",
    "\n",
    "\n",
    "\n",
    "from scipy.stats import norm\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "\n",
    "import itertools\n",
    "#sys.path\n",
    "\n",
    "\n",
    "import regex as re\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='juliettapc', api_key='nM6iUdx6dGaOiPXQTwpP')   # go to: https://plot.ly/settings/api#/   for a new key if needed\n",
    "\n",
    "########## to be able to plot offline (without sending the plots to the plotly server every time)\n",
    "import plotly.offline as offline\n",
    "from plotly.graph_objs import *\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "################\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######### I get the fraction of references that are self-citations (without repetion by section or anything!!)\n",
    "\n",
    "# %time df_merged_old = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl', 'rb'))  # this includes plos that are reviews or editorials or corrections!!!\n",
    "\n",
    "# df_references_old = df_merged_old.drop_duplicates(subset=['reference_UT'])\n",
    "# print (df_references_old.shape)\n",
    "\n",
    "# df_references_old.self_citation.value_counts()\n",
    "\n",
    "# # 0    2239458\n",
    "# # 1     367999\n",
    "\n",
    "\n",
    "# print (float(2239458)/float(2607457),   float(367999)/float(2607457))\n",
    "\n",
    "len(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.73 s, sys: 2.15 s, total: 6.88 s\n",
      "Wall time: 17.6 s\n",
      "done loading pickles (5787634, 34)\n",
      "(156558, 34)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>occurence</th>\n",
       "      <th>reference_UT</th>\n",
       "      <th>reference_rank</th>\n",
       "      <th>regex_sect_index</th>\n",
       "      <th>cite_count</th>\n",
       "      <th>ref_pub_year</th>\n",
       "      <th>paper_cite_count</th>\n",
       "      <th>plos_pub_year</th>\n",
       "      <th>sect_char_pos</th>\n",
       "      <th>sect_char_total</th>\n",
       "      <th>...</th>\n",
       "      <th>plos_article_type</th>\n",
       "      <th>num_cit_young_ref_by2009</th>\n",
       "      <th>num_cit_young_ref_by2010</th>\n",
       "      <th>num_cit_young_ref_by2011</th>\n",
       "      <th>num_cit_young_ref_by2012</th>\n",
       "      <th>num_cit_young_ref_by2013</th>\n",
       "      <th>num_cit_young_ref_by2009after8</th>\n",
       "      <th>num_cit_young_ref_by2008after8</th>\n",
       "      <th>num_cit_young_ref_by2010after8</th>\n",
       "      <th>num_cit_young_ref_by2007after8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A1995QY75100004</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>139</td>\n",
       "      <td>4029</td>\n",
       "      <td>...</td>\n",
       "      <td>@ Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A1995QY75100004</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>494</td>\n",
       "      <td>5398</td>\n",
       "      <td>...</td>\n",
       "      <td>@ Article</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>000263911400006</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>142</td>\n",
       "      <td>4029</td>\n",
       "      <td>...</td>\n",
       "      <td>@ Article</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>000289279600018</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>269</td>\n",
       "      <td>4029</td>\n",
       "      <td>...</td>\n",
       "      <td>@ Article</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>000289279600018</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>3844</td>\n",
       "      <td>5398</td>\n",
       "      <td>...</td>\n",
       "      <td>@ Article</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   occurence     reference_UT  reference_rank  regex_sect_index  cite_count  \\\n",
       "0          1  A1995QY75100004               1                 0        60.0   \n",
       "1          2  A1995QY75100004               1                 3        60.0   \n",
       "4          1  000263911400006               3                 0         5.0   \n",
       "5          1  000289279600018               4                 0        29.0   \n",
       "6          3  000289279600018               4                 3        29.0   \n",
       "\n",
       "   ref_pub_year  paper_cite_count  plos_pub_year  sect_char_pos  \\\n",
       "0        1995.0                 2         2013.0            139   \n",
       "1        1995.0                 2         2013.0            494   \n",
       "4        2009.0                 2         2013.0            142   \n",
       "5        2011.0                 2         2013.0            269   \n",
       "6        2011.0                 2         2013.0           3844   \n",
       "\n",
       "   sect_char_total              ...               plos_article_type  \\\n",
       "0             4029              ...                       @ Article   \n",
       "1             5398              ...                       @ Article   \n",
       "4             4029              ...                       @ Article   \n",
       "5             4029              ...                       @ Article   \n",
       "6             5398              ...                       @ Article   \n",
       "\n",
       "  num_cit_young_ref_by2009 num_cit_young_ref_by2010  num_cit_young_ref_by2011  \\\n",
       "0                      NaN                      NaN                       NaN   \n",
       "1                      NaN                      NaN                       NaN   \n",
       "4                      0.0                      0.0                       NaN   \n",
       "5                      0.0                      0.0                       0.0   \n",
       "6                      0.0                      0.0                       0.0   \n",
       "\n",
       "   num_cit_young_ref_by2012  num_cit_young_ref_by2013  \\\n",
       "0                       NaN                       NaN   \n",
       "1                       NaN                       NaN   \n",
       "4                       NaN                       NaN   \n",
       "5                       3.0                       NaN   \n",
       "6                       3.0                       NaN   \n",
       "\n",
       "   num_cit_young_ref_by2009after8 num_cit_young_ref_by2008after8  \\\n",
       "0                             NaN                            NaN   \n",
       "1                             NaN                            NaN   \n",
       "4                             5.0                            5.0   \n",
       "5                            39.0                           39.0   \n",
       "6                            39.0                           39.0   \n",
       "\n",
       "   num_cit_young_ref_by2010after8 num_cit_young_ref_by2007after8  \n",
       "0                             NaN                            NaN  \n",
       "1                             NaN                            NaN  \n",
       "4                             5.0                            5.0  \n",
       "5                            39.0                           39.0  \n",
       "6                            39.0                           39.0  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "                                     \n",
    "\n",
    "####  NEW FILE INCLUDING EARLY CITATIONS OF YOUNG REFERENCES:   ../data/df_reference_cite_plos_no_self-cit_one_ref_per_sect_ONLY_ARTICLES_early_cit.pkl\n",
    "                                            \n",
    "#%time df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns_no_self-cit_one_ref_per_sect_ONLY_ARTICLES.pkl', 'rb'))\n",
    "\n",
    "\n",
    "\n",
    "%time df_merged = pickle.load(open('../data/df_reference_cite_plos_no_self-cit_one_ref_per_sect_ONLY_ARTICLES_early_cit_and_after_accretion_time.pkl', 'rb'))\n",
    "print (\"done loading pickles\", df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_merged = df_merged[df_merged['cite_count'] != -1]   # i dont know why, but there are 7 occurrences with value -1\n",
    "\n",
    "\n",
    "\n",
    "plos_df = df_merged.drop_duplicates(subset=['paper_UT'])\n",
    "print (plos_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged.paper_UT.unique())  # 156558   only articles!!  (no reviews, commentaries,corrections,...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plos_df = df_merged.drop_duplicates(subset=['paper_UT'])\n",
    "print (plos_df.shape)\n",
    "\n",
    "\n",
    "df_ref = df_merged.drop_duplicates(subset=['reference_UT'])\n",
    "print (df_ref.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### get overall statistics \n",
    "##########################################\n",
    "\n",
    "\n",
    "list_quantiles_cell=[.25,.5,.75,.99]\n",
    "values_quantiles=list(plos_df['total_refs'].quantile(list_quantiles_cell))    \n",
    "print(\"for PLOS:         avg # references included per paper:\",plos_df.total_refs.mean(),  \"  STD:\", plos_df.total_refs.std(), \"   25-50-75:\",values_quantiles,len(plos_df),\"\\n\")\n",
    "\n",
    "values_quantiles_cit_plos=list(plos_df['paper_cite_count'].quantile(list_quantiles_cell))     \n",
    "print(\"                  avg # citations plos:\",plos_df.paper_cite_count.mean(),  \"  STD:\", plos_df.paper_cite_count.std(),  \"  25-50-75:\",values_quantiles_cit_plos,\"\\n\")\n",
    "\n",
    "\n",
    "values_quantiles_cit=list(df_ref['cite_count'].quantile(list_quantiles_cell))     \n",
    "print(\"for REFERENCES:   avg # citations ref:\",df_ref.cite_count.mean(),  \"  STD:\", df_ref.cite_count.std(),  \"  25-50-75:\",values_quantiles_cit,len(df_ref),\"\\n\")\n",
    "\n",
    "\n",
    "# values_quantiles=list(df_merged['cite_count'].quantile(list_quantiles_cell))   \n",
    "# print(\"for all df_merged records,     avg # citations plos:\",df_merged.cite_count.mean(),  \"  STD:\", df_merged.cite_count.std(),  \"   25-50-75:\",values_quantiles,len(df_merged),\"\\n\")\n",
    "\n",
    "\n",
    "values_quantiles=list(df_merged['cite_count'].quantile(list_quantiles_cell))   \n",
    "print(\"for all df_merged records,     avg # citations ref:\",df_merged.cite_count.mean(),  \"  STD:\", df_merged.cite_count.std(),  \"   25-50-75:\",values_quantiles,len(df_merged),\"\\n\")\n",
    "\n",
    "\n",
    "values_quantiles_age=list(df_ref['diff_year_plos_ref'].quantile(list_quantiles_cell)) \n",
    "print(\"                               avg age diff plos-ref:\",df_ref.diff_year_plos_ref.mean(),  \"  STD:\", df_ref.diff_year_plos_ref.std(),  \"  25-50-75:\",values_quantiles_age,\"\\n\")\n",
    "\n",
    "\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALL sections:  {'intro': 0, 'methods': 1, 'results': 2, 'disc': 3, 'res_disc':4, 'concl':5, 'mixed':6, 'na':7}\n",
    "\n",
    "# df_merged.regex_sect_index.value_counts()\n",
    "# 0    2188159 / 5787630. =  0.37807\n",
    "# 3    2105019 / 5787630. =  0.36371\n",
    "# 1     678770 / 5787630. =  0.11728\n",
    "# 2     563650 / 5787630. =  0.09739\n",
    "# 4     199399 / 5787630. =  0.03445\n",
    "# 7      37528 / 5787630. =  0.00648\n",
    "# 5      14813 / 5787630. =  0.00256\n",
    "# 6        292 / 5787630. =  0.00005\n",
    "\n",
    "#df_merged.shape\n",
    "#0.37807 + 0.36371 + 0.11728+ 0.09739+  0.03445+   0.00648+  0.00256+0.00005\n",
    "0.03445+   0.00648+  0.00256+0.00005"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plos_df.drop(columns=['num_old_ref_section0', 'num_old_ref_section1','num_old_ref_section2', 'num_old_ref_section3','num_old_ref_section4', 'num_old_ref_section5','num_old_ref_section6', 'num_old_ref_section7'], inplace=True)\n",
    "#plos_df.drop(columns=['num_young_ref_section0', 'num_young_ref_section1','num_young_ref_section2', 'num_young_ref_section3','num_young_ref_section4', 'num_young_ref_section5','num_young_ref_section6', 'num_young_ref_section7'], inplace=True)\n",
    "#plos_df.drop(columns=['num_ref_section0', 'num_ref_section1','num_ref_section2', 'num_ref_section3','num_ref_section4', 'num_ref_section5','num_ref_section6', 'num_ref_section7'], inplace=True)\n",
    "#plos_df.drop(columns=['cite_count', 'occurence','num_ref_section2', 'num_ref_section3','num_ref_section4', 'num_ref_section5','num_ref_section6', 'num_ref_section7'], inplace=True)\n",
    "\n",
    "sorted(plos_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from the entire df  (5787630, 34)   ; and plos one records: (5368040, 34) \n",
      "\n",
      "\n",
      "5087018 Biology and life sciences\n",
      "3807428 Medicine and health sciences\n",
      "3260169 Research and analysis methods\n",
      "1766356 Physical sciences\n",
      "1035007 Computer and information sciences\n",
      "647228 Social sciences\n",
      "579580 People and places\n",
      "526791 Ecology and environmental sciences\n",
      "444753 Earth sciences\n",
      "322656 Engineering and technology\n",
      "36492 Science policy\n"
     ]
    }
   ],
   "source": [
    "#### dictionary category-code\n",
    "#   ../data/dict_categ_code.pkl\n",
    "\n",
    "\n",
    "\n",
    "dict_categ_code = pickle.load(open('../data/dict_categ_code.pkl', 'rb'))\n",
    "#print (len(dict_categ_code))\n",
    "\n",
    "\n",
    "\n",
    "print (\"from the entire df \",df_merged.shape, \"  ; and plos one records:\",  df_merged[df_merged['plos_j1']== \"PLOS ONE\"].shape,\"\\n\\n\")\n",
    "# {'Biology and life sciences': 0,\n",
    "#  'Computer and information sciences': 1,\n",
    "#  'Earth sciences': 2,\n",
    "#  'Ecology and environmental sciences': 3,\n",
    "#  'Engineering and technology': 4,\n",
    "#  'Medicine and health sciences': 5,\n",
    "#  'People and places': 6,\n",
    "#  'Physical sciences': 7,\n",
    "#  'Research and analysis methods': 8,\n",
    "#  'Science policy': 9,\n",
    "#  'Social sciences': 10}\n",
    "\n",
    "dict_code_categ={}\n",
    "dict_size_categ={}\n",
    "for categ in dict_categ_code:\n",
    "    code = str(dict_categ_code[categ])\n",
    "    \n",
    "    df_selection_categ = df_merged[df_merged['categ_codes'].str.contains(code)]\n",
    "   # print (categ, code, df_selection_categ.shape)\n",
    "    size= len(df_selection_categ)\n",
    "    dict_size_categ[size] = categ\n",
    "    dict_code_categ[code] = categ\n",
    "\n",
    "for size in reversed(sorted(dict_size_categ)):\n",
    "    print (size, dict_size_categ[size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######  i create the new columns for fraction of references in each section (labelled 0 to 7)\n",
    "\n",
    "\n",
    "\n",
    "%time plos_df = pickle.load(open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect_young_old.pkl', 'rb'))\n",
    "print (\"done loading plos_df\", plos_df.shape)\n",
    "plos_df_simplified = plos_df[['paper_UT','num_ref_section0','num_ref_section1','num_ref_section2','num_ref_section3','num_ref_section4','num_ref_section5','num_ref_section6','num_ref_section7',\n",
    "    'fract_old_ref_section0', 'fract_old_ref_section1', 'fract_old_ref_section2', 'fract_old_ref_section3', 'fract_old_ref_section4', 'fract_old_ref_section5', 'fract_old_ref_section6', 'fract_old_ref_section7',\\\n",
    "    'fract_young_ref_section0', 'fract_young_ref_section1', 'fract_young_ref_section2', 'fract_young_ref_section3', 'fract_young_ref_section4', 'fract_young_ref_section5', 'fract_young_ref_section6', 'fract_young_ref_section7']]\n",
    "\n",
    "# print (\"plos_df_simplified\", plos_df_simplified.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#df_merged = pd.merge(df_merged, plos_df_simplified, on='paper_UT', how='left')\n",
    "df_merged = pd.merge(df_merged, plos_df_simplified, on='paper_UT', how='left')\n",
    "print (\"df_merged\", df_merged.shape)\n",
    "\n",
    "# df_merged[['paper_UT','reference_UT','regex_sect_index','eff_num_ref','num_ref_section0','num_ref_section1','num_ref_section2','num_ref_section3','num_ref_section4','num_ref_section5','num_ref_section6','num_ref_section7']].head(100)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_records_one_ref_per_plos = df_merged.drop_duplicates(subset=['paper_UT'])\n",
    "print (df_records_one_ref_per_plos.shape)\n",
    "\n",
    "\n",
    "\n",
    "# df_records_one_ref_per_plos.rename(columns={'paper_cite_count_x': 'paper_cite_count', 'plos_pub_year_x': 'plos_pub_year', 'ref_pub_year_x':'ref_pub_year','regex_sect_index_x':'regex_sect_index', 'total_refs_x':'total_refs'}, inplace=True)\n",
    "# df_merged.rename(columns={'paper_cite_count_x': 'paper_cite_count', 'plos_pub_year_x': 'plos_pub_year', 'ref_pub_year_x':'ref_pub_year','regex_sect_index_x':'regex_sect_index', 'total_refs_x':'total_refs','reference_UT_x':'reference_UT','cite_count_x':'cite_count'}, inplace=True)\n",
    "\n",
    "\n",
    "sorted(df_records_one_ref_per_plos.columns)\n",
    "\n",
    "df_records_one_ref_per_plos.head()\n",
    "\n",
    "\n",
    "\n",
    "###OJO!!!!! FALTA POR HACER BIEN EL MERGE PARA EVITAR REPETICION DE COLUMNAS!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.rename(columns={'paper_cite_count_x': 'paper_cite_count', 'plos_pub_year_x': 'plos_pub_year', 'ref_pub_year_x':'ref_pub_year','regex_sect_index_x':'regex_sect_index', 'total_refs_x':'total_refs','reference_UT_x':'reference_UT'}, inplace=True)\n",
    "#df_merged.rename(columns={'cite_count_x':'cite_count'}, inplace=True)\n",
    "\n",
    "sorted(df_merged.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_added_more_columns_no_self-cit_one_ref_per_sect_ONLY_ARTICLES.pkl', 'rb'))\n",
    "# print (\"done loading pickles\", df_merged.shape)\n",
    "\n",
    "\n",
    "# plos_df = df_merged.drop_duplicates(subset=['paper_UT'])\n",
    "# plos_df = plos_df.sort_values(by=['paper_UT'])\n",
    "# plos_df = plos_df[['paper_UT']]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_R9_UT = pickle.load(open('../data/dict_R9_UT.pkl', 'rb'))\n",
    "print (\"done loading pickle dict_R9_UT\", len(dict_R9_UT))\n",
    "\n",
    "\n",
    "dict_UT_R9 = pickle.load(open('../data/dict_UT_R9.pkl', 'rb'))\n",
    "print (\"done loading pickle dict_UT_R9\", len(dict_UT_R9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df_references.columns)\n",
    "# df_references.diff_year_plos_ref.value_counts()\n",
    "# df_references.shape\n",
    "\n",
    "# #1519/60/24\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_files =[ 'dict_all_authored_paper_UT_list_R9_references_partial1000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial2000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial3000000.pkl',\\\n",
    "#               'dict_all_authored_paper_UT_list_R9_references_partial4000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial5000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial6000000.pkl',\\\n",
    "#               'dict_all_authored_paper_UT_list_R9_references_partial7000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial8000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial9000000.pkl',\\\n",
    "#               'dict_all_authored_paper_UT_list_R9_references_partial10000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial11000000.pkl','dict_all_authored_paper_UT_list_R9_references_partial12000000.pkl',\\\n",
    "#               'dict_all_authored_paper_UT_list_R9_references_last_bit.pkl']\n",
    "\n",
    " \n",
    "path = '../data/dict_all_authored_paper_UT_list_R9_references*'   \n",
    "list_files=sorted(glob.glob(path))\n",
    "\n",
    "\n",
    "lista_all_authored_UTs=[]\n",
    "\n",
    "for filename in lista_files:\n",
    "    dict_all_authored_paper_UT_list_R9_references = pickle.load(open('../data/'+filename, 'rb'))\n",
    "    lista_all_authored_UTs += list(dict_all_authored_paper_UT_list_R9_references.keys())\n",
    "    print (\"done loading\", filename, len(dict_all_authored_paper_UT_list_R9_references), len(lista_all_authored_UTs))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_all_authored_paper_UT_list_UT_references\n",
    "\n",
    "path = '../data/dict_all_authored_paper_UT_list_UT_references*'\n",
    "lista_files=sorted(glob.glob(path))\n",
    "\n",
    "#print (sorted(lista_files))\n",
    "\n",
    "lista_all_authored_UTs=[]\n",
    "master_dict={}\n",
    "for filename in lista_files:\n",
    "    dict_all_authored_paper_UT_list_UT_references = pickle.load(open('../data/'+filename, 'rb')) \n",
    "    lista_all_authored_UTs += list(dict_all_authored_paper_UT_list_UT_references.keys()) \n",
    "    \n",
    "   \n",
    "#     for key in dict_all_authored_paper_UT_list_UT_references:\n",
    "#         if key in master_dict:\n",
    "#             print (key)\n",
    "#             print (dict_all_authored_paper_UT_list_UT_references[key])\n",
    "#             print (master_dict[key])\n",
    "            \n",
    "#             print \n",
    "#             print ()\n",
    "#             exit()\n",
    "\n",
    "\n",
    "    master_dict.update(dict_all_authored_paper_UT_list_UT_references)\n",
    "\n",
    "\n",
    "    print ( filename, len(dict_all_authored_paper_UT_list_UT_references), len(lista_all_authored_UTs), len(master_dict))\n",
    "    \n",
    "    with open('../data/master_dict_all_authored_paper_UT_list_UT_references_partial.pkl', 'wb') as handle:\n",
    "             pickle.dump(master_dict, handle, protocol = 2)\n",
    "    print (\"written:\",'../data/master_dict_all_authored_paper_UT_list_UT_references_partial.pkl',len(master_dict))   \n",
    "\n",
    "    \n",
    "print (\"done.\",len(lista_all_authored_UTs), len(set(lista_all_authored_UTs)), len(master_dict))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open('../data/master_dict_all_authored_paper_UT_list_UT_references.pkl', 'wb') as handle:\n",
    "             pickle.dump(master_dict, handle, protocol = 2)\n",
    "print (\"written:\",'../data/master_dict_all_authored_paper_UT_list_UT_references.pkl',len(master_dict))   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(master_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dest = dict(orig)  # or orig.copy()\n",
    "dest.update(extra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### how to combine two dictionaries (updating overwrites the common keys)\n",
    "\n",
    "# dict_a ={'a':1, 'b':2, 'c':3}\n",
    "# dict_b = {'d':4, 'e':5, 'a':10}\n",
    "\n",
    "\n",
    "# # dest = dict(dict_a)  # or orig.copy()\n",
    "# # dest.update(dict_b)\n",
    "# dest ={}\n",
    "# dest.update(dict_a)\n",
    "# dest.update(dict_b)\n",
    "\n",
    "# dest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all_authored_paper_UT_list_UT_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all_authored_paper_UT_list_R9_references['A1993LP96900052']   dict_all_authored_paper_UT_list_R9_references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict_all_authored_paper_UT_list_UT_references2 = pickle.load(open('../data/dict_all_authored_paper_UT_list_UT_references_partial1200000.pkl', 'rb'))\n",
    "print (\"done loading\", len(dict_all_authored_paper_UT_list_UT_references2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_all_authored_paper_UT_list_UT_references = pickle.load(open('../data/dict_all_authored_paper_UT_list_UT_references_partial12500000.pkl', 'rb'))\n",
    "print (\"done loading\", len(dict_all_authored_paper_UT_list_UT_references))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = dict_all_authored_paper_UT_list_UT_references2.copy()   # start with x's keys and values\n",
    "z.update(dict_all_authored_paper_UT_list_UT_references)  \n",
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(dict_all_authored_paper_UT_list_UT_references2.keys()) - set(dict_all_authored_paper_UT_list_UT_references.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### get average number of references included in the PLOS papers:\n",
    "\n",
    "print (plos_df.total_refs.mean(), plos_df.total_refs.median())\n",
    "list_q=[.25,.5,.75]\n",
    "\n",
    "quantiles=sorted(list(plos_df['total_refs'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles: \n",
    "# 0.10     0.0\n",
    "# 0.25     2.0\n",
    "# 0.50     5.0\n",
    "# 0.75    13.0 \n",
    "print (quantiles)\n",
    "\n",
    "\n",
    "# 47.9125499815 45.0\n",
    "# [(0.25, 34.0), (0.5, 45.0), (0.75, 58.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the entire df  (5787630, 25)   ; and plos one records: (5368040, 25) \n",
    "\n",
    "\n",
    "# 5087018 Biology and life sciences\n",
    "# 3807428 Medicine and health sciences\n",
    "# 3260169 Research and analysis methods\n",
    "# 1766356 Physical sciences\n",
    "# 1035007 Computer and information sciences\n",
    "# 647228 Social sciences\n",
    "# 579580 People and places\n",
    "# 526791 Ecology and environmental sciences\n",
    "# 444753 Earth sciences\n",
    "# 322656 Engineering and technology\n",
    "# 36492 Science policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial_list_keys_so_far = pickle.load(open('../data/partial_list_keys.pkl', 'rb'))      \n",
    "# print (\"done loading  ../data/partial_list_keys.pkl\", len(partial_list_keys_so_far))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged[df_merged['plos_j1']== \"PLOS ONE\"].paper_UT.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_code_categ = '5'\n",
    "select_df = plos_df[plos_df['categ_codes'].str.contains(string_code_categ)]     \n",
    "\n",
    "\n",
    "# {'Biology and life sciences': 0,\n",
    "#  'Computer and information sciences': 1,\n",
    "#  'Earth sciences': 2,\n",
    "#  'Ecology and environmental sciences': 3,\n",
    "#  'Engineering and technology': 4,\n",
    "#  'Medicine and health sciences': 5,\n",
    "#  'People and places': 6,\n",
    "#  'Physical sciences': 7,\n",
    "#  'Research and analysis methods': 8,\n",
    "#  'Science policy': 9,\n",
    "#  'Social sciences': 10}\n",
    "\n",
    "select_df2 = plos_df[plos_df['plos_j1'] == 'PLOS MED']     \n",
    "\n",
    "#['PLOS ONE', 'PLOS MED', 'PLO NE TR D', 'PLOS PATHOG', 'PLOS BIOL', 'PLOS GENET', 'PLOS COMPUT']\n",
    "\n",
    "\n",
    "\n",
    "print (\"Avg citations all papers\\t\\t\",plos_df.paper_cite_count.mean(), plos_df.paper_cite_count.median())\n",
    "print (\" PLOS ONE papers selected by category\\t\",select_df.paper_cite_count.mean() , select_df.paper_cite_count.median())\n",
    "print (\" papers selected by journal\\t\\t\", select_df2.paper_cite_count.mean() , select_df2.paper_cite_count.median()  )\n",
    "\n",
    "print (\"\\n\",df_references.cite_count.mean(), df_references.cite_count.median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#sorted(df_merged.columns)\n",
    "\n",
    "sorted(df_merged.ref_field.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.plos_j1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged[['paper_UT','reference_UT','regex_sect_index','sect_char_pos']].sort_values(['paper_UT','reference_UT','regex_sect_index','sect_char_pos'])\n",
    "#df.sort_values(['a', 'b'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.paper_UT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOS ONE       6367070\n",
    "# PLOS GENET      149923\n",
    "# PLO NE TR D     138289\n",
    "# PLOS PATHOG     109803\n",
    "# PLOS COMPUT      77924\n",
    "# PLOS BIOL        56754\n",
    "# PLOS MED         24506\n",
    "# Name: plos_j1, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing=df_merged.drop_duplicates(subset=['paper_UT'])\n",
    "# print (testing.shape)\n",
    "# testing.plos_j1.value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.plos_field.value_counts()  \n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       3779069\n",
    "# ['D CU BIOLOGY']                                                                                                                           880492\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        708479\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               119685\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                             95315\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                 83451\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          65072\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   44186\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       11776\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        105\n",
    "\n",
    "#df.groupby([\"Group\", \"Size\"]).size()\n",
    "#df_merged.groupby(['plos_field','plos_pub_year']).size()#.value_counts()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_merged.drop(['plos_field'], axis=1, inplace=True)\n",
    "# df_merged.plos_field.value_counts()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_merged=df_merged[['occurence', 'paper_UT', 'reference_UT', 'reference_rank',  'regex_sect_index', 'cite_count', 'ref_pub_year',  'paper_cite_count', 'total_refs','plos_pub_year']]\n",
    "# df_merged.to_pickle('../data/df_reference_cite_plos_merged_simplified_cols.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_merged.reference_UT.unique())\n",
    "df_info_ref_UTs=df_merged[['reference_UT','ref_pub_year','cite_count']].drop_duplicates()\n",
    "df_info_ref_UTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '../data/df_reference_cite_plos_merged_simplified_added_more_columns.pkl'\n",
    "# %time df_merged.to_pickle(path, compression='infer', protocol=2)\n",
    "\n",
    "# print (\"written:\",path)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df_merged.columns\n",
    "# 'occurence', 'paper_UT', 'paper_char_pos', 'paper_word_pos',\n",
    "#        'para_char_pos', 'para_char_total', 'para_index', 'para_word_pos',\n",
    "#        'para_word_total', 'reference_UT', 'reference_rank', 'sect_char_pos',\n",
    "#        'sect_char_total', 'sect_index', 'sect_word_pos', 'sect_word_total',\n",
    "#        'section_title', 'section_title_alt', 'regex_sect_index',\n",
    "#        'ref_datetime', 'plos_datetime', 'cite_count', 'ref_pub_date',\n",
    "#        'ref_pub_year', 'ref_article_type', 'ref_field', 'ref_j1', 'ref_j2',\n",
    "#        'ref_j9', 'ref_ji', 'paper_cite_count', 'total_refs',\n",
    "#        'paper_char_total', 'paper_word_total', 'plos_pub_date',\n",
    "#        'plos_pub_year', 'plos_article_type', 'plos_field', 'plos_j1',\n",
    "#        'plos_j2', 'plos_j9', 'plos_ji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE.##### Histogram number of plos papers published each year:\n",
    "\n",
    "\n",
    "v1_string = 'plos_pub_year'\n",
    "\n",
    "x1 = plos_df[v1_string]\n",
    "#x2 = plos_df[v1_string]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font=18\n",
    "font_ticks=50\n",
    "font_axes=60\n",
    "\n",
    "\n",
    "#1F77B4   blue\n",
    "#FF7F0E  orange\n",
    "#2CA02C  green\n",
    "#D62728  red\n",
    "#9575D2  purple\n",
    "#8C564B  brown\n",
    "#E377C0  pink\n",
    "#7F7F7F  grey\n",
    "#BCBD22  lime\n",
    "#17BECF turquesa\n",
    "\n",
    "trace1= Histogram(\n",
    "        x=x1, \n",
    "     marker=dict(\n",
    "        color='#17BECF',  #  'purple'     #EB89B5',\n",
    "    ),\n",
    "       # name='high cit',\n",
    "      # histnorm='probability',\n",
    "    #cumulative=dict(enabled=True)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# trace3= Histogram(\n",
    "#         x=x3, \n",
    "#         name=' top > 50',\n",
    "#         histnorm='probability'\n",
    "# )\n",
    "\n",
    "\n",
    "layout = Layout(    \n",
    "\n",
    "    bargap=0.2,\n",
    "\n",
    "\n",
    "\n",
    "    xaxis=dict(\n",
    "        \n",
    "        title= 'Year' , #v1_string.replace(\"_\",\" \"),   \n",
    "        titlefont=dict(           \n",
    "            size=font_axes,\n",
    "            color='black'\n",
    "        #    color='lightgrey'\n",
    "        ),  \n",
    "        tickfont=dict(               \n",
    "            size=font_ticks,\n",
    "            color='black'\n",
    "        ),\n",
    "        #type='log'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Number papers published',    \n",
    "        type='log',\n",
    "        titlefont=dict(            \n",
    "            size=font_axes,\n",
    "            color='black'\n",
    "        #    color='lightgrey'\n",
    "        ),  \n",
    "        tickfont=dict(               \n",
    "            size=font_ticks,\n",
    "            color='black'\n",
    "        ),\n",
    "        \n",
    "    ),                \n",
    "\n",
    "    margin=go.Margin(\n",
    "        l=250,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        #t=100,\n",
    "        #pad=80\n",
    "    ),\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     bargroupgap=0.1\n",
    ")       \n",
    "\n",
    "data = [trace1]#, trace2]#, trace3]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "fig_name='Number_PLOS_papers_published_per_year'\n",
    "path='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "#iplot(fig, filename=fig_name)\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_\"+v1_string+'.html' ,\n",
    "             output_type='file',image_width=2000, image_height=1400, filename=path+fig_name+\".html\", validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DONE ##### Histogram age of ALL references used.\n",
    "\n",
    "\n",
    "# v1_string = 'diff_year_plos_ref'\n",
    "\n",
    "# x1 = df_merged[v1_string]\n",
    "\n",
    "# #print (x1.value_counts())\n",
    "\n",
    "\n",
    "# font=20\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "# trace1= Histogram(\n",
    "#         x=x1, \n",
    "#      marker=dict(\n",
    "#         color='#4EEE94',\n",
    "#     ),\n",
    "#        # name='high cit',\n",
    "#        histnorm='probability',\n",
    "#     #cumulative=dict(enabled=True)\n",
    "# )\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     bargap=0.2,\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= 'Age of references' , #v1_string.replace(\"_\",\" \"),   \n",
    "#         #range=[-.49, 3.49],\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(              \n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         #type='log'\n",
    "#         #tickvals=[0,1,2,3],\n",
    "#         #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         #type='log',\n",
    "#         titlefont=dict(\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(               \n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "#     margin=go.Margin(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "\n",
    "\n",
    "# #     bargroupgap=0.1\n",
    "# )       \n",
    "\n",
    "# data = [trace1]#, trace2]#, trace3]\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Age_all_references.html')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_\"+v1_string+'.html' ,\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Age_all_references.html', validate=False)\n",
    "\n",
    "\n",
    "# print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DONE      ##### Histogram age of references used BY SECTION.\n",
    "\n",
    "\n",
    "# v1_string = 'diff_year_plos_ref'\n",
    "\n",
    "# x1 = df_merged[v1_string]\n",
    "\n",
    "# #print (x1.value_counts())\n",
    "\n",
    "\n",
    "# font=20\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# trace1= Histogram(\n",
    "#         x=x1, \n",
    "#      marker=dict(\n",
    "#         color='#848484',\n",
    "#     ),\n",
    "#        # name='high cit',\n",
    "#        histnorm='probability',\n",
    "#        name=\"All sections\"\n",
    "#     #cumulative=dict(enabled=True)\n",
    "# )\n",
    "\n",
    "# data.append(trace1)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_sect = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "# list_colors = [\"#7171C6\",\"#4EEE94\",\"#FF4040\",\"#FFA500\"]\n",
    "# for index in range(4):\n",
    "    \n",
    "#     df_selection=df_merged[ (df_merged['regex_sect_index'] == index)]   \n",
    "#     print (index, df_selection.shape)\n",
    "#     x1 = df_selection[v1_string]\n",
    "    \n",
    "    \n",
    "#     trace= Histogram(\n",
    "#             x=x1, \n",
    "#          marker=dict(\n",
    "#             color=list_colors[index],\n",
    "#         ),           \n",
    "#            histnorm='probability',\n",
    "#            name=list_sect[index]+\" median:\"+str(x1.median())\n",
    "#         #cumulative=dict(enabled=True)\n",
    "#         )\n",
    "\n",
    "\n",
    "#     data.append(trace)  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     bargap=0.2,\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= 'Age of references' , #v1_string.replace(\"_\",\" \"),   \n",
    "#         #range=[-.49, 3.49],\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(              \n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         #type='log'\n",
    "#         #tickvals=[0,1,2,3],\n",
    "#         #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         #type='log',\n",
    "#         titlefont=dict(\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(               \n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "#     margin=go.Margin(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "\n",
    "\n",
    "# #     bargroupgap=0.1\n",
    "# )       \n",
    "\n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Age_references_by_section.html')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"Age_references_by_section.html\",\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Age_references_by_section.html', validate=False)\n",
    "\n",
    "\n",
    "# print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['categ_codes',\n",
    " 'cite_count',\n",
    " 'diff_year_plos_ref',\n",
    " 'isolated_citation',\n",
    " 'log2_num_cit_paper',\n",
    " 'log2_num_cit_ref',\n",
    " 'number_authors',\n",
    " 'occurence',\n",
    " 'paper_UT',\n",
    " 'paper_cite_count',\n",
    " 'plos_field',\n",
    " 'plos_j1',\n",
    " 'plos_pub_year',\n",
    " 'ref_field',\n",
    " 'ref_j1',\n",
    " 'ref_pub_year',\n",
    " 'reference_UT',\n",
    " 'reference_rank',\n",
    " 'regex_sect_index',\n",
    " 'rel_loc_in_sect',\n",
    " 'sect_char_pos',\n",
    " 'sect_char_total',\n",
    " 'self_citation',\n",
    " 'total_refs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #done      ##### Histogram number of citations of references used BY SECTION.\n",
    "\n",
    "\n",
    "# v1_string = 'cite_count'\n",
    "\n",
    "# x1 = df_merged[v1_string]\n",
    "\n",
    "# #print (x1.value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font=20\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "\n",
    "\n",
    "# max_x=plos_df[v1_string].max()\n",
    "# min_x=plos_df[v1_string].min()\n",
    "# Nbins=1000\n",
    "# range_values=(min_x, max_x)  ## OJO! to be able to compare histograms with bins, i need the same number of bins and interval for all sets!!\n",
    "\n",
    "# count, boundary_bins = np.histogram(plos_df[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "# # cumulat=np.cumsum(count)\n",
    "\n",
    "# # counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "\n",
    "# #print (boundary_bins_cumulat)\n",
    "\n",
    "\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     x = boundary_bins,\n",
    "#     y = count,\n",
    "#     mode = 'markers',\n",
    "#     marker = dict(       \n",
    "#         color = 'rgba(0, 0, 0, .8)',\n",
    "#         symbol=\"square\"),\n",
    "#     name='All sections'\n",
    "#     )\n",
    "# data.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_sect = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "# list_colors = [\"#7171C6\",\"#4EEE94\",\"#FF4040\",\"#FFA500\"]\n",
    "# for index in range(4):\n",
    "    \n",
    "#     df_selection=df_merged[ (df_merged['regex_sect_index'] == index)]   \n",
    "#     print (index, df_selection.shape)\n",
    "#     x1 = df_selection[v1_string]\n",
    "    \n",
    "\n",
    "#     count, boundary_bins = np.histogram(df_selection[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "#     counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "    \n",
    "#     #print (sorted(boundary_bins))\n",
    "#     trace = go.Scatter(\n",
    "#         x = boundary_bins,\n",
    "#         y = count,\n",
    "#         mode = 'markers',   \n",
    "#         name=list_sect[index]+\" median:\"+str(x1.median())\n",
    "#     )\n",
    "\n",
    "   \n",
    "    \n",
    "#     data.append(trace)  \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= 'Number of citations of references' ,   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        range=[np.log10(50),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        type='log',\n",
    "      \n",
    "       \n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "#  margin=go.Margin(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "# )       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Number_citations_of_references_by_section.html')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"Number_citations_of_references_by_section.html\",\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Number_citations_of_references_by_section.html', validate=False)\n",
    "\n",
    "\n",
    "# print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(df_merged.columns)\n",
    "# df_merged.shape\n",
    "\n",
    "\n",
    "df_testing = df_testing[df_testing['regex_sect_index'] >3]   #  9089  records\n",
    "df_testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### OJOOOOOOOOOO!   THE BOX PLOT CANT HANDLE THE BIG DATA SET, SO I ONLY PLOT A SAMPLE OF ROWS (DOESNT CHANGE MUCH FROM HAEAD TO TAIL SELECTION...)\n",
    "# #   DONE.   BOXPLOT for age of references used, separating by section\n",
    "\n",
    "\n",
    "\n",
    "# font=18\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "# df_testing = df_merged.sample(4000000)   # tot number of records: 6,924,262\n",
    "# df_testing = df_testing[df_testing['regex_sect_index'] <=3]\n",
    "# df_testing = df_testing[df_testing['diff_year_plos_ref']>=0]\n",
    "\n",
    "# print (df_testing.diff_year_plos_ref.max())\n",
    "\n",
    "\n",
    "# list_colors=['#4F94CD','#4EEE94','#FF4040','#FF8C00']\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for col in df_testing['regex_sect_index'].unique():\n",
    "#     data.append(go.Box(y=df_testing[df_testing['regex_sect_index'] == col]['diff_year_plos_ref'], name=col, marker = dict( color = list_colors[col] ),showlegend=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     xaxis=dict(\n",
    "#         #title= 'Section' ,   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        #range=[np.log10(50),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        #type='log',\n",
    "#         tickvals=[0,1,2,3],\n",
    "#         ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]      \n",
    "       \n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='Age of references',\n",
    "#         type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickvals=[0,1,5,7,10,12,15,20,50,100,200],\n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "#  margin=go.Margin(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "# )       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Boxplot_age_references_by_section_N'+str(len(df_testing)))\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"Boxplot_age_references_by_section_N\"+str(len(df_testing)),\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Boxplot_age_references_by_section_N'+str(len(df_testing))+'.html', validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### OJOOOOOOOOOO!   THE BOX PLOT CANT HANDLE THE BIG DATA SET, SO I ONLY PLOT A SAMPLE OF ROWS (DOESNT CHANGE MUCH FROM HAEAD TO TAIL SELECTION...)\n",
    "# #   DONE    BOXPLOT for age of references used, separating by section\n",
    "\n",
    "\n",
    "\n",
    "# font=18\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "# df_testing = df_merged.sample(4000000)   # tot number of records: 6,924,262\n",
    "# df_testing = df_testing[df_testing['regex_sect_index'] <=3]\n",
    "# #df_testing = df_testing[df_testing['diff_year_plos_ref']>=0]\n",
    "\n",
    "\n",
    "# list_colors=['#4F94CD','#4EEE94','#FF4040','#FF8C00']\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for col in range(4):#df_testing['regex_sect_index'].unique():\n",
    "#     data.append(go.Box(y=df_testing[df_testing['regex_sect_index'] == col]['cite_count'], name=col, marker = dict( color = list_colors[col] ),showlegend=False))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     xaxis=dict(\n",
    "#         #title= 'Section' ,   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        #range=[np.log10(50),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        #type='log',\n",
    "#         tickvals=[0,1,2,3],\n",
    "#         ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "      \n",
    "       \n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='Number of citations of references',\n",
    "#         type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickvals=[1,10,100,1000,10000,100000],#5,7,10,12,15,20,50,100],\n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "#  margin=go.Margin(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "# )       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Boxplot_num_citations_references_by_section'+str(len(df_testing)))\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"Boxplot_num_citations_references_by_section\"+str(len(df_testing)),\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Boxplot_num_citations_references_by_section_N'+str(len(df_testing))+'.html', validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.plos_j1.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #   DONE.   barplot number of records and number of paper in each PLOS journal\n",
    "\n",
    "\n",
    "# list_colors=['#4F94CD','#4EEE94','#FF4040','#FF8C00']\n",
    "# list_journals = ['PLOS ONE', 'PLOS GENET', 'PLO NE TR D','PLOS PATHOG', 'PLOS COMPUT', 'PLOS BIOL',  'PLOS MED']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font=18\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# list_num_papers = []\n",
    "# list_num_records = []\n",
    "# for journal in list_journals:\n",
    "#     df_selection = df_merged[df_merged['plos_j1'] == journal]\n",
    "    \n",
    "# #     list_num_records.append(len(df_selection))\n",
    "# #     list_num_papers.append(len(df_selection.paper_UT.unique()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     trace = go.Bar(\n",
    "#         x=['number of records'],    #x=['number of records', 'number of papers'],\n",
    "#         y=[len(df_selection)],\n",
    "#         name=journal.replace('PLO NE TR D','PLOS Neglected Tropical Diseases').replace('PLOS BIOL','PLOS Biology').replace('PLOS COMPUT','PLOS Computational Biology').replace('PLOS GENET','PLOS Genetics').replace('PLOS MED','PLOS Medicine').replace('PLOS PATHOG','PLOS Pathology')\n",
    "#     )\n",
    "#     data.append(trace)\n",
    "\n",
    "# layout = Layout(    \n",
    "#     barmode='stack',\n",
    "#     legend=dict(font=dict( size=45 ),                              \n",
    "#                ),     #legend=dict(x=-.1, y=1.2)\n",
    "#     xaxis=dict(\n",
    "#         #title= 'Section' ,   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        #range=[np.log10(50),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        #type='log',\n",
    "#       #  tickvals=[0,1,2,3],\n",
    "#        # ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "      \n",
    "       \n",
    "#      ),\n",
    "#     yaxis=dict(\n",
    "#         title='Count',\n",
    "#        # type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#        # tickvals=[1,10,100,1000,10000,100000],#5,7,10,12,15,20,50,100],\n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),   \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "  \n",
    "\n",
    "#  margin=go.Margin(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "# )       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Barplot_num_records_by_journal')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"Barplot_num_records_by_journal\",\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Barplot_num_records_by_journal.html', validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# #   DONE   barplot number of records and number of paper in each PLOS journal\n",
    "\n",
    "\n",
    "# list_colors=['#4F94CD','#4EEE94','#FF4040','#FF8C00']\n",
    "# list_journals = ['PLOS ONE', 'PLOS GENET', 'PLO NE TR D','PLOS PATHOG', 'PLOS COMPUT', 'PLOS BIOL',  'PLOS MED']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font=18\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# list_num_papers = []\n",
    "# list_num_records = []\n",
    "# for journal in list_journals:\n",
    "#     df_selection = df_merged[df_merged['plos_j1'] == journal]\n",
    "    \n",
    "# #     list_num_records.append(len(df_selection))\n",
    "# #     list_num_papers.append(len(df_selection.paper_UT.unique()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     trace = go.Bar(\n",
    "#         x=['number of papers'],\n",
    "#         y=[ len(df_selection.paper_UT.unique())],\n",
    "#         name=journal.replace('PLO NE TR D','PLOS Neglected Tropical Diseases').replace('PLOS BIOL','PLOS Biology').replace('PLOS COMPUT','PLOS Computational Biology').replace('PLOS GENET','PLOS Genetics').replace('PLOS MED','PLOS Medicine').replace('PLOS PATHOG','PLOS Pathology')\n",
    "#     )\n",
    "#     data.append(trace)\n",
    "\n",
    "# layout = Layout(    \n",
    "#     barmode='stack',\n",
    "#     showlegend=False,\n",
    "# #     legend=dict(font=dict( size=18 ),                              \n",
    "# #                ),     #legend=dict(x=-.1, y=1.2)\n",
    "#     xaxis=dict(\n",
    "#         #title= 'Section' ,   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        #range=[np.log10(50),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        #type='log',\n",
    "#       #  tickvals=[0,1,2,3],\n",
    "#        # ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "      \n",
    "       \n",
    "#      ),\n",
    "#     yaxis=dict(\n",
    "#         #title='Count',\n",
    "#        # type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#        # tickvals=[1,10,100,1000,10000,100000],#5,7,10,12,15,20,50,100],\n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#          side='right',\n",
    "#     ),   \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "  \n",
    "\n",
    "#  margin=go.Margin(\n",
    "#         r=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "# )       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Barplot_num_papers_by_journal')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"Barplot_num_papers_num_records_by_journal\",\n",
    "#              output_type='file', image_width=1400, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Barplot_num_papers_by_journal.html', validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: (5787630, 34)\n",
      "[2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017]\n",
      "size of preselection1 (by plos years): (5787630, 34)\n",
      "size of preselection1 (by isolated/group ref): (5787630, 34) \n",
      " size of preselection2 (by plos journal): (5787630, 34) \n",
      " size of preselection2 (by plos field): (5787630, 34) \n",
      "  No preselection by age of references: (5787630, 34)\n",
      "\n",
      "Tot # records included: 5787630    # number of plos papers: 156558    # unique ref: 2320774 \n",
      "\n",
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/staff/julia/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/plotly/graph_objs/_deprecations.py:396: DeprecationWarning:\n",
      "\n",
      "plotly.graph_objs.Margin is deprecated.\n",
      "Please replace it with one of the following more specific types\n",
      "  - plotly.graph_objs.layout.Margin\n",
      "\n",
      "\n",
      "/home/staff/julia/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/plotly/offline/offline.py:463: UserWarning:\n",
      "\n",
      "Your filename `/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/fig1a` didn't end with .html. Adding .html to the end of your file.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/fig1a.html'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " ### DONE.  MULTIPLOT FOR BARPLOTS OF NUMBER OF RECORDS AND NUMBER OF PAPERS PER JOURNAL\n",
    "\n",
    "    \n",
    "### FIGURE 1A\n",
    "    \n",
    "    ### NOTE: THIS FIGURE REQUIRES HEAVY TOUCH-UPS ON INKSCAPE (FOR THE BRONEN Y AXIS)\n",
    "    \n",
    "    \n",
    "v1_string =  'cite_count'#  log2_num_cit_ref'  ######  log2_num_cit_ref' # #     #'ref_pub_year'     cite_count    diff_year_plos_ref \n",
    "       \n",
    "\n",
    "years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "string_references_age = \"\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "string_self_ref =0    #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "#  '0': 'Biology and life sciences'             6,032,537\n",
    "#  '1': 'Computer and information sciences'     1,207,799\n",
    "#  '10': 'Social sciences'                      755,899\n",
    "#  '2': 'Earth sciences'                        533,155\n",
    "#  '3': 'Ecology and environmental sciences'    624,142\n",
    "#  '4': 'Engineering and technology'            382,247 \n",
    "#  '5': 'Medicine and health sciences'          4,535,926   \n",
    "#  '6': 'People and places'                     691,523\n",
    "#  '7': 'Physical sciences'                     2,100,827\n",
    "#  '8': 'Research and analysis methods'         3,871,470\n",
    "#  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    \n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "string_references_age\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    " \n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "list_colors = [\"#FF4040\",\"#4EEE94\",\"#87CEFA\",\"#FFA500\",\"#EE82EE\",\"#8B8B83\", \"#0000FF\"]\n",
    "list_journals = ['PLOS GENET', 'PLO NE TR D','PLOS PATHOG', 'PLOS COMPUT', 'PLOS BIOL',  'PLOS MED', 'PLOS ONE']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.001, horizontal_spacing=0.001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = []\n",
    "\n",
    "list_num_papers = []\n",
    "list_num_records = []\n",
    "cont = 0\n",
    "for journal in list_journals:\n",
    "    df_selection = preselection_df3[preselection_df3['plos_j1'] == journal]        \n",
    "    \n",
    "    trace1 = go.Bar(\n",
    "        x=['# papers'],\n",
    "        y=[ len(df_selection.paper_UT.unique())],\n",
    "        marker=dict( color=list_colors[cont]),\n",
    "        #name=journal.replace('PLO NE TR D','PLOS Neglected<br>  Tropical Diseases').replace('PLOS BIOL','PLOS Biology').replace('PLOS COMPUT','PLOS Computational<br>  Biology').replace('PLOS GENET','PLOS Genetics').replace('PLOS MED','PLOS Medicine').replace('PLOS PATHOG','PLOS Pathology'),        \n",
    "        name=journal.replace('PLO NE TR D','Neglected<br>  Tropical Diseases').replace('PLOS BIOL','Biology').replace('PLOS COMPUT','Computational<br>  Biology').replace('PLOS GENET','Genetics').replace('PLOS MED','Medicine').replace('PLOS PATHOG','Pathology'),\n",
    "        showlegend=True\n",
    "    )\n",
    "   \n",
    "    fig.append_trace(trace1, 1, 1)\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "cont = 0    \n",
    "for journal in list_journals:\n",
    "    df_selection = preselection_df3[preselection_df3['plos_j1'] == journal]\n",
    "    \n",
    "    \n",
    "    trace2 = go.Bar(\n",
    "        x=['# records'],   \n",
    "        y=[len(df_selection)],\n",
    "        marker=dict( color=list_colors[cont]),\n",
    "        #name=journal.replace('PLO NE TR D','PLOS Neglected\\n Tropical Diseases').replace('PLOS BIOL','PLOS Biology').replace('PLOS COMPUT','PLOS Computational Biology').replace('PLOS GENET','PLOS Genetics').replace('PLOS MED','PLOS Medicine').replace('PLOS PATHOG','PLOS Pathology'),\n",
    "        showlegend=False,\n",
    "        xaxis='x2',\n",
    "        yaxis='y2'\n",
    "    )\n",
    "    fig.append_trace(trace2, 1, 2)\n",
    "    cont  +=1\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "#fig['layout'].update(height=400, width=1200, legend=dict(orientation=\"h\"), margin=go.Margin( l=200))#, b=150),)# r=50, b=150,  #t=100, # pad=4 ), )\n",
    "fig['layout'].update( legend=dict(x=1.2, y=1.0), margin=go.Margin( l=250))#, b=150),)# r=50, b=150,  #t=100, # pad=4 ), )\n",
    "\n",
    "\n",
    "\n",
    "fig['layout'].update(barmode='stack')\n",
    "\n",
    "\n",
    "fig['layout']['yaxis2'].update(side='right')\n",
    "fig['layout']['yaxis'].update(title='Count')\n",
    "\n",
    "# fig['layout']['yaxis'].update(range=[0, 15000])  \n",
    "# fig['layout']['yaxis2'].update(range=[0, 800000])  \n",
    "\n",
    "\n",
    "#fig['layout']['yaxis'].update(type='log')\n",
    "#fig['layout']['yaxis2'].update(type='log')\n",
    "# fig['layout']['yaxis'].update(range=[0, 4])  ### ojo!! rango logaritmico if axis in log type!  0, 5.3\n",
    "# fig['layout']['yaxis'].update(range=[0, 6.84])  ### ojo!! rango logaritmico!   0, 6.84\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55 # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering axis\n",
    "fig['layout']['xaxis']['tickfont']['size']  = font_gral + 10\n",
    "fig['layout']['xaxis2']['tickfont']['size'] = font_gral + 10\n",
    "fig['layout']['yaxis']['tickfont']['size']  = font_gral + 10\n",
    "fig['layout']['yaxis2']['tickfont']['size'] = font_gral + 10\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['xaxis'].update(domain=[0, 0.45]) # this is to force the relative location of the two panels with respect to each other\n",
    "fig['layout']['yaxis'].update(domain=[0, 1])\n",
    "\n",
    "fig['layout']['xaxis2'].update( domain=[0.55, 1])\n",
    "fig['layout']['yaxis2'].update(domain=[0, 1],  anchor='x2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=\"fig1a\" ,image_width=2000, image_height=1000, \n",
    "              filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/fig1a', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### DONE.  MULTIPLOT FOR BARPLOTS OF NUMBER OF RECORDS AND NUMBER OF PAPERS PER JOURNAL\n",
    "# #### same but horizontal bars\n",
    "# list_colors = [\"#0000FF\",\"#FF4040\",\"#4EEE94\",\"#87CEFA\",\"#FFA500\",\"#EE82EE\",\"#8B8B83\"]\n",
    "\n",
    "# fig = tools.make_subplots(rows=2, cols=1, vertical_spacing=0.001, horizontal_spacing=0.001)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = []\n",
    "\n",
    "# list_num_papers = []\n",
    "# list_num_records = []\n",
    "# cont = 0\n",
    "# for journal in list_journals:\n",
    "#     df_selection = df_merged[df_merged['plos_j1'] == journal]        \n",
    "    \n",
    "#     trace1 = go.Bar(\n",
    "#         y=['Number of papers'],\n",
    "#         x=[ len(df_selection.paper_UT.unique())],\n",
    "#         orientation = 'h',\n",
    "#         marker=dict( color=list_colors[cont]),\n",
    "#         name=journal.replace('PLO NE TR D','PLOS Neglected<br>  Tropical Diseases').replace('PLOS BIOL','PLOS Biology').replace('PLOS COMPUT','PLOS Computational<br>  Biology').replace('PLOS GENET','PLOS Genetics').replace('PLOS MED','PLOS Medicine').replace('PLOS PATHOG','PLOS Pathology'),\n",
    "#         showlegend=True\n",
    "#     )\n",
    "   \n",
    "#     fig.append_trace(trace1, 1, 1)\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# cont = 0    \n",
    "# for journal in list_journals:\n",
    "#     df_selection = df_merged[df_merged['plos_j1'] == journal]\n",
    "    \n",
    "    \n",
    "#     trace2 = go.Bar(\n",
    "#         y=['Number of records'],   \n",
    "#         x=[len(df_selection)],\n",
    "#         orientation = 'h',\n",
    "#         marker=dict( color=list_colors[cont]),\n",
    "#         #name=journal.replace('PLO NE TR D','PLOS Neglected\\n Tropical Diseases').replace('PLOS BIOL','PLOS Biology').replace('PLOS COMPUT','PLOS Computational Biology').replace('PLOS GENET','PLOS Genetics').replace('PLOS MED','PLOS Medicine').replace('PLOS PATHOG','PLOS Pathology'),\n",
    "#         showlegend=False,\n",
    "#         xaxis='x2',\n",
    "#         yaxis='y2'\n",
    "#     )\n",
    "#     fig.append_trace(trace2, 2, 1)\n",
    "#     cont  +=1\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# #fig['layout'].update(height=400, width=1200, legend=dict(orientation=\"h\"), margin=go.Margin( l=200))#, b=150),)# r=50, b=150,  #t=100, # pad=4 ), )\n",
    "# #fig['layout'].update( legend=dict(x=1.1, y=1.0), margin=go.Margin( l=200))#, b=150),)# r=50, b=150,  #t=100, # pad=4 ), )\n",
    "# fig['layout'].update( legend=dict(orientation=\"h\"), margin=go.Margin( l=200))#, b=150),)# r=50, b=150,  #t=100, # pad=4 ), )\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(barmode='stack')\n",
    "# #fig['layout']['yaxis'].update(type='log')\n",
    "# #fig['layout']['yaxis'].update(range=[0, 5.3])  ### ojo!! rango logaritmico!\n",
    "\n",
    "# #fig['layout']['yaxis2'].update(type='log')\n",
    "# #fig['layout']['yaxis'].update(range=[0, 6.84])  ### ojo!! rango logaritmico!\n",
    "# fig['layout']['yaxis2'].update(side='right')\n",
    "\n",
    "# fig['layout']['yaxis'].update(title='Count')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=50 # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering axis\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral  + 5\n",
    "# fig['layout']['xaxis2']['tickfont']['size'] = font_gral  +5\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral  - 5\n",
    "# fig['layout']['yaxis2']['tickfont']['size'] = font_gral + 5\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['xaxis'].update(domain=[0, 1]) # this is to force the relative location of the two panels with respect to each other\n",
    "# fig['layout']['yaxis'].update(domain=[0, 0.45])\n",
    "\n",
    "# fig['layout']['xaxis2'].update( domain=[0, 1])\n",
    "# fig['layout']['yaxis2'].update(domain=[0.55, 1])#,  anchor='x2')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"testing_multiplot\" ,image_width=2000, image_height=1000, \n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/testing_multiplot.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DONE.   # HISTOGRAM for number of cites received by plos papers, separating plos papers by publication year\n",
    "\n",
    "\n",
    "# data=[]\n",
    "\n",
    "# v1_string = 'paper_cite_count'\n",
    "\n",
    "\n",
    "# max_x=plos_df[v1_string].max()\n",
    "# min_x=plos_df[v1_string].min()\n",
    "# Nbins=1000\n",
    "# range_values=(min_x, max_x)  ## OJO! to be able to compare histograms with bins, i need the same number of bins and interval for all sets!!\n",
    "\n",
    "# count, boundary_bins = np.histogram(plos_df[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "# #cumulat=np.cumsum(count)\n",
    "\n",
    "# #counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "\n",
    "# #print (boundary_bins_cumulat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     x = boundary_bins,\n",
    "#     y = count,\n",
    "#     mode = 'markers',\n",
    "#     marker = dict(       \n",
    "#         color = 'rgba(0, 0, 0, .8)',\n",
    "#         symbol=\"square\"),\n",
    "#     name='All PLOS papers'\n",
    "#     )\n",
    "# data.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "# ini_year=2005.\n",
    "# fin_year=2016.\n",
    "# year= ini_year\n",
    "# while year <= fin_year:\n",
    "    \n",
    "#     df_selection=plos_df[ (plos_df['plos_pub_year'] == year)]   \n",
    "#     print (year, df_selection.shape)\n",
    "#     x1 = df_selection[v1_string]\n",
    "#     count, boundary_bins = np.histogram(df_selection[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "#     counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "    \n",
    "#     #print (sorted(boundary_bins))\n",
    "#     trace = go.Scatter(\n",
    "#     x = boundary_bins,\n",
    "#     y = count,\n",
    "#     mode = 'markers',   \n",
    "#     name=\"published in \"+str(int(year))\n",
    "#     )\n",
    "\n",
    "#     data.append(trace)  \n",
    "#     year += 1.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= \"Number of citations received\",   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        type='log',\n",
    "      \n",
    "       \n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "\n",
    "\n",
    "# )       \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "# # Plot and embed in ipython notebook!\n",
    "# #py.iplot(data, filename='basic-scatter')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_number_citations_for_PLOS_papers\" ,image_width=2000, image_height=1400, \n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/histogram_number_citations_for_PLOS_papers.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info_ref_UTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DONE.# HISTOGRAM for number of cites received by reference papers, separating papers by publication year\n",
    "\n",
    "\n",
    "# data=[]\n",
    "\n",
    "# v1_string = 'cite_count'\n",
    "\n",
    "\n",
    "# max_x=df_info_ref_UTs[v1_string].max()\n",
    "# min_x=df_info_ref_UTs[v1_string].min()\n",
    "# Nbins=100000\n",
    "# range_values=(min_x, max_x)  ## OJO! to be able to compare histograms with bins, i need the same number of bins and interval for all sets!!\n",
    "\n",
    "# count, boundary_bins = np.histogram(df_info_ref_UTs[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "# #cumulat=np.cumsum(count)\n",
    "\n",
    "# #counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "\n",
    "# #print (boundary_bins_cumulat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     x = boundary_bins,\n",
    "#     y = count,\n",
    "#     mode = 'markers',\n",
    "#     marker = dict(       \n",
    "#         color = 'rgba(0, 0, 0, .8)',\n",
    "#         symbol=\"square\"),\n",
    "#     name='All reference papers'\n",
    "#     )\n",
    "# data.append(trace)\n",
    "\n",
    "\n",
    "# lista_bins_years=[[1900,1995],[1995,2000],[2000,2003],[2003,2005],[2005,2007],[2007,2008],[2008,2009],[2009,2010],[2010,2012],[2012,2016]] # this comes from deciles of number of papers per year\n",
    "\n",
    "\n",
    "# # ini_year=1900.\n",
    "# # fin_year=2016.\n",
    "# # delta=10.\n",
    "# # year= ini_year\n",
    "# for item in lista_bins_years:\n",
    "    \n",
    "#     min_year=item[0]\n",
    "#     max_year= item[1]\n",
    "    \n",
    "#     df_selection=df_info_ref_UTs[ (df_info_ref_UTs['ref_pub_year'] >= min_year) & (df_info_ref_UTs['ref_pub_year'] < max_year)]   \n",
    "#     print (item, df_selection.shape)\n",
    "#     x1 = df_selection[v1_string]\n",
    "#     count, boundary_bins = np.histogram(df_selection[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "#     counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "    \n",
    "#     #print (sorted(boundary_bins))\n",
    "#     trace = go.Scatter(\n",
    "#     x = boundary_bins,\n",
    "#     y = count,\n",
    "#     mode = 'markers',   \n",
    "#     name=\" published in [\"+str(int(min_year))+\"-\"+str(int(max_year))+\"), N:\"+str(len(df_selection))\n",
    "#     )\n",
    "\n",
    "#     data.append(trace)  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= \"Number of citations received\",   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#        type='log',\n",
    "      \n",
    "       \n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "\n",
    "\n",
    "# )       \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "# # Plot and embed in ipython notebook!\n",
    "# #py.iplot(data, filename='basic-scatter')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_number_citations_for_ref_papers\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/histogram_number_citations_for_ref_papers.html', validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()\n",
    "# 'occurence', 'paper_UT', 'paper_char_pos', 'paper_word_pos',\n",
    "#        'para_char_pos', 'para_char_total', 'para_index', 'para_word_pos',\n",
    "#        'para_word_total', 'reference_UT', 'reference_rank', 'sect_char_pos',\n",
    "#        'sect_char_total', 'sect_index', 'sect_word_pos', 'sect_word_total',\n",
    "#        'section_title', 'section_title_alt', 'regex_sect_index',\n",
    "#        'ref_datetime', 'plos_datetime', 'cite_count', 'ref_pub_date',\n",
    "#        'ref_pub_year', 'ref_article_type', 'ref_field', 'ref_j1', 'ref_j2',\n",
    "#        'ref_j9', 'ref_ji', 'paper_cite_count', 'total_refs',\n",
    "#        'paper_char_total', 'paper_word_total', 'plos_pub_date',\n",
    "#        'plos_pub_year', 'plos_article_type', 'plos_field', 'plos_j1',\n",
    "#        'plos_j2', 'plos_j9', 'plos_ji']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)\n",
    "#df_merged[['paper_UT','reference_UT','sect_index','regex_sect_index']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #df_merged_fewer_col = df_merged[['occurence','paper_UT','paper_cite_count','reference_UT','regex_sect_index','plos_pub_year','ref_pub_year','plos_j1','cite_count']]#.drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_plotting = df_merged[df_merged['occurence'] == 1]\n",
    "# print (  \"shape merged df:\",df_merged.shape, \"    number of records for first occur:\",df_plotting.shape, len(df_plotting.reference_UT.unique()),\"    number of unique ref_UTs:\",len(df_merged.reference_UT.unique()),  \"    number PLOS papers:\",len(df_plotting.paper_UT.unique()))\n",
    "# df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "37+35+14+10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DONE.      ##### Histogram number of occurrences in each section (all papers)\n",
    "#### FIGURE 1B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "v1_string = 'regex_sect_index'\n",
    "       \n",
    "\n",
    "years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "string_references_age = \"\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "string_self_ref =0    #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "#  '0': 'Biology and life sciences'             6,032,537\n",
    "#  '1': 'Computer and information sciences'     1,207,799\n",
    "#  '10': 'Social sciences'                      755,899\n",
    "#  '2': 'Earth sciences'                        533,155\n",
    "#  '3': 'Ecology and environmental sciences'    624,142\n",
    "#  '4': 'Engineering and technology'            382,247 \n",
    "#  '5': 'Medicine and health sciences'          4,535,926   \n",
    "#  '6': 'People and places'                     691,523\n",
    "#  '7': 'Physical sciences'                     2,100,827\n",
    "#  '8': 'Research and analysis methods'         3,871,470\n",
    "#  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "    \n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    " \n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1 = preselection_df3[v1_string]\n",
    "\n",
    "print (x1.value_counts())\n",
    "\n",
    "font_gral=60\n",
    "\n",
    "font=font_gral#20\n",
    "font_ticks=font_gral#70\n",
    "font_axes=font_gral#80\n",
    "\n",
    "\n",
    "trace1= Histogram(\n",
    "        x=x1, \n",
    "     marker=dict(\n",
    "        color='#9900ff',\n",
    "    ),\n",
    "       # name='high cit',\n",
    "       histnorm='probability',\n",
    "    #cumulative=dict(enabled=True)\n",
    ")\n",
    "\n",
    "\n",
    "layout = Layout(    \n",
    "\n",
    "    bargap=0.2,\n",
    "\n",
    "    xaxis=dict(\n",
    "        #title= 'Section' , #v1_string.replace(\"_\",\" \"),   \n",
    "        range=[-.49, 3.49],\n",
    "        titlefont=dict(            \n",
    "            size=font_axes,\n",
    "            color='black'\n",
    "        #    color='lightgrey'\n",
    "        ),  \n",
    "        tickfont=dict(              \n",
    "            size=font_ticks+30,\n",
    "            color='black'\n",
    "        ),\n",
    "        #type='log'\n",
    "       # tickvals=[0,1,2,3],\n",
    "        ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "        #ticktext=[\"I\",\"M\",\"R\",\"D\"]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Fraction of references',\n",
    "        #type='log',\n",
    "        titlefont=dict(\n",
    "            size=font_axes+30,\n",
    "            color='black'\n",
    "        #    color='lightgrey'\n",
    "        ),  \n",
    "        tickfont=dict(               \n",
    "            size=font_ticks+30,\n",
    "            color='black'\n",
    "        ),\n",
    "        \n",
    "    ),                \n",
    "\n",
    "    margin=go.Margin(\n",
    "        l=350,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        #t=100,\n",
    "       # pad=4\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    ")       \n",
    "\n",
    "data = [trace1]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='Fraction_references_each_section_overall.html')\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=\"fig1b\" ,\n",
    "             output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/fig1b.html', validate=False)\n",
    "\n",
    "\n",
    "print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     bargap=0.2,\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         #title= 'Section' , #v1_string.replace(\"_\",\" \"),   \n",
    "#         range=[-.49, 3.49],\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(              \n",
    "#             size=font_ticks+30,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         #type='log'\n",
    "#        # tickvals=[0,1,2,3],\n",
    "#         #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "#         #ticktext=[\"I\",\"M\",\"R\",\"D\"]\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='Fraction of references',\n",
    "#         #type='log',\n",
    "#         titlefont=dict(\n",
    "#             size=font_axes+30,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(               \n",
    "#             size=font_ticks+30,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         # tickvals=[0,0.1,0.2,0.3],\n",
    "#         #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "        \n",
    "#     ),                \n",
    "\n",
    "#     margin=go.Margin(\n",
    "#         l=300,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "\n",
    "\n",
    "#    annotations=[\n",
    "#         dict(\n",
    "#             x=.0,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text='(c)',\n",
    "#             showarrow=True,\n",
    "#             font=dict(               \n",
    "#                 size=50,               \n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "   \n",
    "         \n",
    "         \n",
    "\n",
    "    \n",
    "    \n",
    "# #     bargroupgap=0.1\n",
    "# )       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = [trace1]#, trace2]#, trace3]\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Fraction_references_each_section_overall.html')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_\"+v1_string+'.html' ,\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Fraction_references_each_section_overall.html', validate=False)\n",
    "\n",
    "\n",
    "# print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   ##### Histogram number of occurrences in each section (selection by year, or journal, or subject)\n",
    "####  for supplementary info\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "v1_string = 'regex_sect_index'\n",
    "       \n",
    "\n",
    "years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016] \n",
    "\n",
    "#years=[2017] \n",
    "\n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "string_references_age = \"\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "string_self_ref =0    #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "#  '0': 'Biology and life sciences'             6,032,537\n",
    "#  '1': 'Computer and information sciences'     1,207,799\n",
    "#  '10': 'Social sciences'                      755,899\n",
    "#  '2': 'Earth sciences'                        533,155\n",
    "#  '3': 'Ecology and environmental sciences'    624,142\n",
    "#  '4': 'Engineering and technology'            382,247 \n",
    "#  '5': 'Medicine and health sciences'          4,535,926   \n",
    "#  '6': 'People and places'                     691,523\n",
    "#  '7': 'Physical sciences'                     2,100,827\n",
    "#  '8': 'Research and analysis methods'         3,871,470\n",
    "#  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"PLOS COMPUT\"\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "    \n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    " \n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1 = preselection_df3[v1_string]\n",
    "\n",
    "print (x1.value_counts())\n",
    "\n",
    "font_gral=60\n",
    "\n",
    "font=font_gral#20\n",
    "font_ticks=font_gral#70\n",
    "font_axes=font_gral#80\n",
    "\n",
    "\n",
    "trace1= Histogram(\n",
    "        x=x1, \n",
    "     marker=dict(\n",
    "        color='#9900ff',\n",
    "    ),\n",
    "       # name='high cit',\n",
    "       histnorm='probability',\n",
    "    #cumulative=dict(enabled=True)\n",
    ")\n",
    "\n",
    "\n",
    "layout = Layout(    \n",
    "\n",
    "    bargap=0.2,\n",
    "\n",
    "    xaxis=dict(\n",
    "        #title= 'Section' , #v1_string.replace(\"_\",\" \"),   \n",
    "        range=[-.49, 3.49],\n",
    "        titlefont=dict(            \n",
    "            size=font_axes,\n",
    "            color='black'\n",
    "        #    color='lightgrey'\n",
    "        ),  \n",
    "        tickfont=dict(              \n",
    "            size=font_ticks+30,\n",
    "            color='black'\n",
    "        ),\n",
    "        #type='log'\n",
    "        tickvals=[0,1,2,3],\n",
    "        #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "        ticktext=[\"I\",\"M\",\"R\",\"D\"]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Fraction of references',\n",
    "        #type='log',\n",
    "        titlefont=dict(\n",
    "            size=font_axes+30,\n",
    "            color='black'\n",
    "        #    color='lightgrey'\n",
    "        ),  \n",
    "        tickfont=dict(               \n",
    "            size=font_ticks+30,\n",
    "            color='black'\n",
    "        ),\n",
    "         tickvals=[0,0.1,0.2,0.3],\n",
    "        #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "        \n",
    "    ),                \n",
    "\n",
    "    margin=go.Margin(\n",
    "        l=300,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        #t=100,\n",
    "       # pad=4\n",
    "    ),\n",
    "\n",
    "\n",
    "\n",
    "#     bargroupgap=0.1\n",
    ")       \n",
    "\n",
    "data = [trace1]#, trace2]#, trace3]\n",
    "\n",
    "fig = Figure(data=data, layout=layout)\n",
    "\n",
    "iplot(fig, filename='Fraction_references_each_section_overall.html')\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_\"+v1_string+'.html' ,\n",
    "             output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Fraction_references_each_section_overall.html', validate=False)\n",
    "\n",
    "\n",
    "print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))\n",
    "\n",
    "\n",
    "\n",
    "###  PLO NE TR D,   Tot # records included: 95315    # number of plos papers: 3218    # unique ref: 54854 \n",
    "### PLOS GENET, Tot # records included: 119685    # number of plos papers: 2212    # unique ref: 70334 \n",
    "### PLOS ONE,  Tot # records included: 5368040    # number of plos papers: 146772    # unique ref: 2240771 \n",
    "###  PLOS PATHOG, Tot # records included: 83451    # number of plos papers: 1648    # unique ref: 53370 \n",
    "###  PLOS COMPUT,  Tot # records included: 65177    # number of plos papers: 1360    # unique ref: 40323\n",
    "### PLOS BIOL,  Tot # records included: 44186    # number of plos papers: 896    # unique ref: 32171 \n",
    "###  PLOS MED,  Tot # records included: 11776    # number of plos papers: 452    # unique ref: 9388 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NEW      ##### Histogram number of occurrences in each section (all papers)  but using each paper's fraction of ref. in each section\n",
    "# #### FIGURE 1B  (OJO!! THIS PLOT WORKS, BUT THE ERROR BARS ARE WAY TOO BIG)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "       \n",
    "\n",
    "# years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "# string_references_age = \"\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "# string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "# string_self_ref =0    #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "    \n",
    "# print (\"original size:\",df_records_one_ref_per_plos.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ################## OJO!!! THIS PLOT CONCERNS THE FRACTION OF REFERENCES USED IN EACH SECTION BY EACH PAPER (TOTAL SIZE OF DF: 158k)\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_records_one_ref_per_plos[df_records_one_ref_per_plos['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "   \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    " \n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "# lista_x_names =    ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "# lista_y_values = [df_records_one_ref_per_plos.fract_ref_section0.mean(), df_records_one_ref_per_plos.fract_ref_section1.mean(), df_records_one_ref_per_plos.fract_ref_section2.mean(), df_records_one_ref_per_plos.fract_ref_section3.mean()] \n",
    "# lista_errors_y = [2.*df_records_one_ref_per_plos.fract_ref_section0.std(), 2.*df_records_one_ref_per_plos.fract_ref_section1.std(), 2.*df_records_one_ref_per_plos.fract_ref_section2.std(), 2.*df_records_one_ref_per_plos.fract_ref_section3.std()] \n",
    "\n",
    "\n",
    "# # lista_y_values = [df_records_one_ref_per_plos.num_ref_section0.mean(), df_records_one_ref_per_plos.num_ref_section1.mean(), df_records_one_ref_per_plos.num_ref_section2.mean(), df_records_one_ref_per_plos.num_ref_section3.mean()] \n",
    "# # lista_errors_y = [2.*df_records_one_ref_per_plos.num_ref_section0.std(), 2.*df_records_one_ref_per_plos.num_ref_section1.std(), 2.*df_records_one_ref_per_plos.num_ref_section2.std(), 2.*df_records_one_ref_per_plos.num_ref_section3.std()] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x1 = preselection_df3[v1_string]\n",
    "\n",
    "# print (x1.value_counts())\n",
    "\n",
    "# font_gral=60\n",
    "\n",
    "# font=font_gral#20\n",
    "# font_ticks=font_gral#70\n",
    "# font_axes=font_gral#80\n",
    "\n",
    "\n",
    "# # trace1= Histogram(\n",
    "# #         x=x1, \n",
    "# #      marker=dict(\n",
    "# #         color='#9900ff',\n",
    "# #     ),\n",
    "# #        # name='high cit',\n",
    "# #        histnorm='probability',\n",
    "# #     #cumulative=dict(enabled=True)\n",
    "# # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trace1 = go.Bar(\n",
    "#      x=lista_x_names,\n",
    "#      y=lista_y_values,\n",
    "#    # name='   expected value',# top '+str(int(100-100*list_q_plos[-2]))+'% PLOS',# ('+str(Niter)+' iter)',\n",
    "#   #  text= lista_text_old,\n",
    "#      error_y=dict(\n",
    "#        # type='data',\n",
    "#         array=lista_errors_y,\n",
    "#         thickness=5,\n",
    "#         visible=True\n",
    "#         ),\n",
    "#      marker=dict(\n",
    "#          color='#9900ff',     #color='#8c96c6',          #'#e6fff2',#'rgb(158,202,225)',\n",
    "# #         line=dict(\n",
    "# #             color='rgb(8,48,107)',\n",
    "# #             width=1.5,\n",
    "# #         )\n",
    "#          ),\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "#     bargap=0.2,\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         #title= 'Section' , #v1_string.replace(\"_\",\" \"),   \n",
    "#         range=[-.49, 3.49],\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(              \n",
    "#             size=font_ticks+30,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         #type='log'\n",
    "#         tickvals=[0,1,2,3],\n",
    "#         #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "#         ticktext=[\"I\",\"M\",\"R\",\"D\"]\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='Fraction of references',\n",
    "#         #type='log',\n",
    "#         titlefont=dict(\n",
    "#             size=font_axes+30,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(               \n",
    "#             size=font_ticks+30,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         tickvals=[0,0.1,0.2,0.3],\n",
    "#         #ticktext=[\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "        \n",
    "#     ),                \n",
    "\n",
    "#     margin=go.Margin(\n",
    "#         l=300,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     ),\n",
    "\n",
    "\n",
    "\n",
    "# #     bargroupgap=0.1\n",
    "# )       \n",
    "\n",
    "# data = [trace1]#, trace2]#, trace3]\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# iplot(fig, filename='Fraction_references_each_section_overall.html')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_\"+v1_string+'.html' ,\n",
    "#              output_type='file', image_width=2000, image_height=1400, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/Fraction_references_each_section_overall.html', validate=False)\n",
    "\n",
    "\n",
    "# print (\"occurrences:\", df_merged.shape, \"unique ref_UTs:\",len(df_merged.reference_UT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "\n",
    "#list_q=[0.9,.99,.999,1]\n",
    "list_q=[0.25,.5,.75,1]\n",
    "\n",
    "\n",
    "quantiles=sorted(list(df_merged['cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles: \n",
    "# 0.10     0.0\n",
    "# 0.25     2.0\n",
    "# 0.50     5.0\n",
    "# 0.75    13.0 \n",
    "print (df_merged['cite_count'].mean(), quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_section(value):\n",
    "#     sect=\"\"\n",
    "#     if value ==0:\n",
    "#         sect = \"i\"\n",
    "#     elif value==1:\n",
    "#         sect = \"m\"\n",
    "#     elif value==2:\n",
    "#         sect = \"r\"\n",
    "#     elif value==3:\n",
    "#         sect = \"d\"\n",
    "#     elif value==4:\n",
    "#         sect = \"r-d\"\n",
    "#     elif value==5:\n",
    "#         sect = \"c\"\n",
    "#     elif value==6:\n",
    "#         sect=\"mx\"\n",
    "#     elif value==7:\n",
    "#         sect=\"NA\"\n",
    "    \n",
    "\n",
    "# df_merged[\"section\"] = df_merged.regex_sect_index.apply(get_section)\n",
    "\n",
    "# # df_disamb_wos_test['full_name'] = df_disamb_wos_test.full_name.apply(convert_unicode_to_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v1_string = 'regex_sect_index'\n",
    "# string_filtering = 'cite_count'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print  (df_merged[string_filtering].max(),df_merged[string_filtering].min() )\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "\n",
    "# list_q=[0.9,.99,.999,1]\n",
    "\n",
    "# quantiles=sorted(list(df_merged[string_filtering].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (quantiles)   # extreme bins for all plos papers: [(0.9, 28.0), (0.99, 86.0), (0.999, 221.0), (1.0, 1994.0)]  # extreme bins for 2009 plos: [(0.9, 68.0), (0.99, 184.0), (0.999, 452.0), (1.0, 1994.0)]\n",
    "\n",
    "# lista_bins=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins)\n",
    "\n",
    "# #lista_bins_plos_citations=[[0,68],[68,184],[184,452],[452,1994]]   # extreme bins (90%, 99%, 99.9%) for 2009\n",
    "# #lista_bins_plos_citations=[[1,6],[6,11],[11,19],[19,972]]  # quantiles  for 2012\n",
    "# #lista_bins_plos_citations=[[1, 3.0], [3, 5],[5,7],[7,9],[9,11],[11,14],[14,17],[17,22],[22,31],[31,972]] # deciles for 2012\n",
    "                           \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONE  Replicate Nathan's multiplot figure (location of references, bin references' citations)\n",
    "###############################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #lista_bins=[[1,2],[2,4],[4,8],[8,16],[16,32],[32,64],[64,128],[128,256],[256,512],[512,1024],[1024,2048],[2048,4096],[4096,8192],[8192,16384],[16384,326393]]\n",
    "\n",
    "# #lista_bins=[[1,26],[26,63],[63,174],[174,326393]]  # quantiles\n",
    "\n",
    "\n",
    "\n",
    "# #lista_bins=[[1,32],[32,512],[512,4096],[4096,326393]]  # 4 bins to see the diff. in the very highly cited\n",
    "\n",
    "\n",
    "# #lista_bins=[[0,502],[502,4985],[4985,34610],[34610,326393]]    # extreme bins (90%, 99%, 99.9%)\n",
    "# #lista_bins_plos_citations=[[0,68],[68,184],[184,452],[452,1994]]   # extreme bins (90%, 99%, 99.9%) for 2009\n",
    "# #lista_bins_plos_citations=[[1,6],[6,11],[11,19],[19,972]]  # quantiles  for 2012\n",
    "# #lista_bins_plos_citations=[[1, 3.0], [3, 5],[5,7],[7,9],[9,11],[11,14],[14,17],[17,22],[22,31],[31,972]] # deciles for 2012\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "# string_filtering = 'cite_count'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print  (df_merged[string_filtering].max(),df_merged[string_filtering].min() )\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "\n",
    "# list_q=[0.9,.99,.999,1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# quantiles=sorted(list(df_merged[string_filtering].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (quantiles)   # extreme bins for all plos papers: [(0.9, 28.0), (0.99, 86.0), (0.999, 221.0), (1.0, 1994.0)]  # extreme bins for 2009 plos: [(0.9, 68.0), (0.99, 184.0), (0.999, 452.0), (1.0, 1994.0)]\n",
    "\n",
    "# lista_bins=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "\n",
    "                           \n",
    "     \n",
    "\n",
    "\n",
    "    \n",
    "# print (lista_bins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_titles=str(lista_bins).replace(\"[[\",\"[\").replace(\"]]\",\"]\").replace(\"]\",\") \").split(' ,') \n",
    "# lista_titles = [\"ref's cit:\"+i for i in lista_titles]+ ['All papers, N:'+str(len(df_merged))]\n",
    "# #print (lista_titles)\n",
    "# print (len(lista_titles))\n",
    "\n",
    "\n",
    "# cont=0\n",
    "# for item in lista_bins:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "     \n",
    "#     df_select = df_merged[(df_merged[string_filtering] >= minimo)  &  (df_merged[string_filtering] < maximo)]\n",
    "#     print (minimo, maximo, df_select.shape)\n",
    "\n",
    "#     title=lista_titles[cont]+\"<br>N:\"+str(len(df_select))\n",
    "#     lista_titles[cont] = title\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "\n",
    "# print (lista_titles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=0\n",
    "# Nrows=0\n",
    "# diff=0\n",
    "\n",
    "# Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# if diff == 0.:\n",
    "#     pass\n",
    "# else:\n",
    "#     while Ncols*Nrows < len(lista_titles):\n",
    "#         Ncols +=1\n",
    "        \n",
    "  \n",
    "# print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font=40\n",
    "# font_ticks=20\n",
    "# font_axes=20\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=(lista_titles))\n",
    "                                                                \n",
    "# print (\"subplots created\")\n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "\n",
    "# for item in lista_bins:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "     \n",
    "#     df_select = df_merged[(df_merged[string_filtering] >= minimo)  &  (df_merged[string_filtering] < maximo)]\n",
    "#     print (minimo, maximo, df_select.shape)\n",
    "\n",
    "\n",
    "#     x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#     if cont_rows ==1 and cont_cols ==1:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "#     else:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "\n",
    "#         cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#     fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#     if cont_cols < Ncols:\n",
    "#         cont_cols += 1\n",
    "#     else:\n",
    "#         cont_cols=1\n",
    "#         cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = df_merged[v1_string]\n",
    "# print (df_merged.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "            \n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=1200, width=1200, title='Location for references, bins by their total number of citations')\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # layout = Layout(    \n",
    "\n",
    "# #     bargap=0.2,\n",
    "\n",
    "\n",
    "\n",
    "# #     xaxis=dict(\n",
    "# #         title= 'Year' , #v1_string.replace(\"_\",\" \"),   \n",
    "# #         titlefont=dict(\n",
    "# #             family=font,#family='Arial, sans-serif',\n",
    "# #             size=font_axes,\n",
    "# #             color='black'\n",
    "# #         #    color='lightgrey'\n",
    "# #         ),  \n",
    "# #         tickfont=dict(   \n",
    "# #             family=font,\n",
    "# #             size=font_ticks,\n",
    "# #             color='black'\n",
    "# #         ),\n",
    "# #         #type='log'\n",
    "# #     ),\n",
    "# #     yaxis=dict(\n",
    "# #         title='Number of PLOS papers published',\n",
    "# #         type='log',\n",
    "# #         titlefont=dict(\n",
    "# #             family=font,#family='Arial, sans-serif',\n",
    "# #             size=font_axes,\n",
    "# #             color='black'\n",
    "# #         #    color='lightgrey'\n",
    "# #         ),  \n",
    "# #         tickfont=dict(   \n",
    "# #             family=font,\n",
    "# #             size=font_ticks,\n",
    "# #             color='black'\n",
    "# #         ),\n",
    "# #     ),                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # #     bargroupgap=0.1\n",
    "# # )       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=False)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.paper_cite_count.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONE. Multiplot location of references, bins by citations of plos papers  (controlling for plos publ. year)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# years=[2010]\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "\n",
    "\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "# string_filtering = 'paper_cite_count'   # by plos' citations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print  (preselection_df[string_filtering].max(),preselection_df[string_filtering].min() )\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "\n",
    "# list_q=[0.9,.99,.999,1]\n",
    "\n",
    "# quantiles=sorted(list(preselection_df[string_filtering].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (quantiles)   # extreme bins for all plos papers: [(0.9, 28.0), (0.99, 86.0), (0.999, 221.0), (1.0, 1994.0)]  # extreme bins for 2009 plos: [(0.9, 68.0), (0.99, 184.0), (0.999, 452.0), (1.0, 1994.0)]\n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "# #lista_bins_plos_citations=[[0,68],[68,184],[184,452],[452,1994]]   # extreme bins (90%, 99%, 99.9%) for 2009\n",
    "# #lista_bins_plos_citations=[[1,6],[6,11],[11,19],[19,972]]  # quantiles  for 2012\n",
    "# #lista_bins_plos_citations=[[1, 3.0], [3, 5],[5,7],[7,9],[9,11],[11,14],[14,17],[17,22],[22,31],[31,972]] # deciles for 2012\n",
    "                           \n",
    "     \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_titles=str(lista_bins_plos_citations).replace(\"[[\",\"[\").replace(\"]]\",\"]\").replace(\"]\",\") \").split(' ,') \n",
    "# lista_titles = [\"plos' cit:\"+i for i in lista_titles]+ ['All plos '+str(years)+', N:'+str(len(preselection_df))]\n",
    "\n",
    "\n",
    "# cont=0\n",
    "# for item in lista_bins_plos_citations:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "     \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "#     print (minimo, maximo, df_select.shape)\n",
    "\n",
    "#     title=lista_titles[cont]+\", N:\"+str(len(df_select))\n",
    "#     lista_titles[cont] = title\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "\n",
    "\n",
    "# Ncols=0\n",
    "# Nrows=0\n",
    "# diff=0\n",
    "\n",
    "# Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# if diff == 0.:\n",
    "#     pass\n",
    "# else:\n",
    "#     while Ncols*Nrows < len(lista_titles):\n",
    "#         Ncols +=1\n",
    "        \n",
    "  \n",
    "# print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=(lista_titles))\n",
    "                          \n",
    "                                                                \n",
    "# print (\"subplots created\")\n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "# # minimo =0\n",
    "# #for i in range(len(quantiles)):  # i add the sub plots for the bins\n",
    "      \n",
    "# #   try:    \n",
    "# #         minimo = quantiles[i][1]\n",
    "# #         maximo = quantiles[i+1][1]      \n",
    "# for item in lista_bins_plos_citations:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "#     #df_select = df_merged[(df_merged[string_filtering] >= minimo)  &  (df_merged[string_filtering] < maximo)]\n",
    "   \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "#     print (minimo, maximo, df_select.shape)\n",
    "\n",
    "\n",
    "#     x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#     if cont_rows ==1 and cont_cols ==1:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "#     else:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "\n",
    "#         cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#     fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#     if cont_cols < Ncols:\n",
    "#         cont_cols += 1\n",
    "#     else:\n",
    "#         cont_cols=1\n",
    "#         cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = preselection_df[v1_string]\n",
    "# print (preselection_df.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "        \n",
    "    \n",
    "# # if cont_cols+1 < Ncols:\n",
    "# #     cont_cols += 1\n",
    "# # else:\n",
    "# #     cont_cols=1\n",
    "# #     cont_rows +=1\n",
    "\n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=1200, width=1200, title='Location for references by total number of citations of plos papers from '+str(years))\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MODIFICATION: ADD SEPARATED ZERO BIN. Multiplot location of references, bins by citations of plos papers  (controlling for plos publ. year).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# years=[2010]\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "\n",
    "\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "# string_filtering = 'paper_cite_count'   # by plos' citations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print  (preselection_df[string_filtering].max(),preselection_df[string_filtering].min() )\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "\n",
    "# list_q=[0.9,.99,.999,1]\n",
    "\n",
    "# quantiles=sorted(list(preselection_df[string_filtering].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# #print (quantiles)   # extreme bins for all plos papers: [(0.9, 28.0), (0.99, 86.0), (0.999, 221.0), (1.0, 1994.0)]  # extreme bins for 2009 plos: [(0.9, 68.0), (0.99, 184.0), (0.999, 452.0), (1.0, 1994.0)]\n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "      \n",
    "# print (lista_bins_plos_citations)\n",
    "        \n",
    "# ### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2\n",
    "       \n",
    "\n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations\n",
    "    \n",
    "# # df_select0 = preselection_df[preselection_df[string_filtering] ==0]   \n",
    "# # print (df_select0.shape )\n",
    "    \n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# lista_titles=str(lista_bins_plos_citations).replace(\"[[\",\"[\").replace(\"]]\",\"]\").replace(\"]\",\") \").split(' ,') \n",
    "# lista_titles = [\"plos' cit:\"+i for i in lista_titles]+ ['All plos '+str(years)+', N:'+str(len(preselection_df))]\n",
    "\n",
    "# # lista_titles = ['plos with 0 cit.'+', N:'+str(len(df_select0))] + lista_titles\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"lista titles:\",lista_titles)\n",
    "\n",
    "# cont=0\n",
    "# for item in lista_bins_plos_citations:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "     \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "#     #print (minimo, maximo, df_select.shape)\n",
    "\n",
    "#     title=lista_titles[cont]+\", N:\"+str(len(df_select))\n",
    "#     lista_titles[cont] = title\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "\n",
    "# print (lista_titles)                   \n",
    "\n",
    "\n",
    "\n",
    "# #### i calculate the number of columns and rows\n",
    "# Ncols=0\n",
    "# Nrows=0\n",
    "# diff=0\n",
    "\n",
    "# Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# # print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# # print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "# print (lista_titles)\n",
    "# if diff == 0.:\n",
    "#     pass\n",
    "# else:\n",
    "#     while Ncols*Nrows < len(lista_titles):\n",
    "#         Ncols +=1        \n",
    "  \n",
    "# print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=(lista_titles))\n",
    "   \n",
    "\n",
    "    \n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "# cont_plots=2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ### i add the zero-bin\n",
    "# # x1 = df_select0[v1_string]\n",
    "    \n",
    "# # trace= Histogram(\n",
    "# #        x=x1, \n",
    "# #        marker=dict(\n",
    "# #        color='#EB89B5'),\n",
    "# #        # name='high cit',\n",
    "# #        histnorm='probability',\n",
    "# #        showlegend=False,\n",
    "# #        xaxis='x'+str(cont_plots),\n",
    "# #        yaxis='y'+str(cont_plots)\n",
    "# #         #cumulative=dict(enabled=True)\n",
    "# #        )\n",
    "\n",
    "# # cont_plots +=1\n",
    "\n",
    "# # print (\" row:\", cont_rows, \"col:\", cont_cols, df_select0.shape)\n",
    "# # fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "# # if cont_cols < Ncols:\n",
    "# #     cont_cols += 1\n",
    "# # else:\n",
    "# #     cont_cols=1\n",
    "# #     cont_rows +=1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# #### i add the rest of the bins\n",
    "# for item in lista_bins_plos_citations:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "#     #df_select = df_merged[(df_merged[string_filtering] >= minimo)  &  (df_merged[string_filtering] < maximo)]\n",
    "   \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "    \n",
    "\n",
    "#     x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#     if cont_rows ==1 and cont_cols ==1:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "#     else:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "\n",
    "#         cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "#     print (\" row:\", cont_rows, \"col:\", cont_cols, df_select.shape)\n",
    "#     fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "#     if cont_cols < Ncols:\n",
    "#         cont_cols += 1\n",
    "#     else:\n",
    "#         cont_cols=1\n",
    "#         cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = preselection_df[v1_string]\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "        \n",
    "    \n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols,preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=1200, width=1200, title='Location for references by total number of citations of plos papers from '+str(years))\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #DONE. Multiplot location of references, bins by AGE of plos papers\n",
    "\n",
    "\n",
    "\n",
    "# lista_years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016]\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "\n",
    "# string_filtering = 'plos_pub_year'\n",
    "\n",
    "\n",
    "# lista_titles = [\"plos from \"+str(i) for i in lista_years]+ ['All papers']\n",
    "# #print (lista_titles)\n",
    "# print (len(lista_titles))\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=0\n",
    "# Nrows=0\n",
    "# diff=0\n",
    "\n",
    "# Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# if diff == 0.:\n",
    "#     pass\n",
    "# else:\n",
    "#     while Ncols*Nrows < len(lista_titles):\n",
    "#         Ncols +=1\n",
    "        \n",
    "  \n",
    "# print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=(lista_titles))\n",
    "                          \n",
    "                                                                \n",
    "# print (\"subplots created\")\n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "# # minimo =0\n",
    "# #for i in range(len(quantiles)):  # i add the sub plots for the bins\n",
    "      \n",
    "# #   try:    \n",
    "# #         minimo = quantiles[i][1]\n",
    "# #         maximo = quantiles[i+1][1]      \n",
    "# for year in lista_years:\n",
    "    \n",
    "       \n",
    "     \n",
    "#     df_select = df_merged[ df_merged[string_filtering] == year  ]\n",
    "#     print (year, df_select.shape)\n",
    "\n",
    "\n",
    "#     x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#     if cont_rows ==1 and cont_cols ==1:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "#     else:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "\n",
    "#         cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#     fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#     if cont_cols < Ncols:\n",
    "#         cont_cols += 1\n",
    "#     else:\n",
    "#         cont_cols=1\n",
    "#         cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = df_merged[v1_string]\n",
    "# print (df_merged.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "        \n",
    "    \n",
    "# # if cont_cols+1 < Ncols:\n",
    "# #     cont_cols += 1\n",
    "# # else:\n",
    "# #     cont_cols=1\n",
    "# #     cont_rows +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=1200, width=1200, title='Location for references by publication year of plos papers')\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.ref_pub_year.value_counts()   # 1990\n",
    "\n",
    "\n",
    "# year=2009\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'] == year]\n",
    "\n",
    "\n",
    "\n",
    "print  (df_merged['ref_pub_year'].max(),df_merged['ref_pub_year'].min() )\n",
    "list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "\n",
    "#list_q=[.33,.66,1]\n",
    "#list_q=[.25,.5,.75,1]\n",
    "\n",
    "quantiles=sorted(list(df_merged['ref_pub_year'].quantile(list_q).to_dict().items())) # [(0.25, 2000.0), (0.5, 2004.0), (0.75, 2006.0), (1.0, 2010.0)]\n",
    "# In [91]:\n",
    "\n",
    "# quantiles: \n",
    "# 0.10     0.0\n",
    "# 0.25     2.0\n",
    "# 0.50     5.0\n",
    "# 0.75    13.0 \n",
    "print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### DONE. Multiplot location of references, bins by AGE of reference papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# years=[2010]\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "\n",
    "# string_filtering = 'ref_pub_year'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########## i create the bins and number of rows and columns\n",
    "\n",
    "# #lista_bins_years=[[1900,2002],[2002,2007],[2007,2010],[2010,2016]]\n",
    "# lista_bins_years=[[1900,1995],[1995,2000],[2000,2003],[2003,2005],[2005,2007],[2007,2008],[2008,2009],[2009,2010],[2010,2012],[2012,2016]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_titles=str(lista_bins_years).replace(\"[[\",\"[\").replace(\"]]\",\"]\").replace(\"]\",\") \").split(' ,') \n",
    "# lista_titles = [\"ref from:\"+i for i in lista_titles]+ ['All plos papers, years'+str(years)+'<br>N:'+str(len(preselection_df))]\n",
    "# print (lista_titles)\n",
    "\n",
    "# cont=0\n",
    "# for item in lista_bins_years:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "     \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "#     print (minimo, maximo, df_select.shape)\n",
    "\n",
    "#     title=lista_titles[cont]+\"<br>N:\"+str(len(df_select))\n",
    "#     lista_titles[cont] = title\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "# print (lista_titles)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=0\n",
    "# Nrows=0\n",
    "# diff=0\n",
    "\n",
    "# Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# # print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# # print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# if diff == 0.:\n",
    "#     pass\n",
    "# else:\n",
    "#     while Ncols*Nrows < len(lista_titles):\n",
    "#         Ncols +=1\n",
    "          \n",
    "# print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=(lista_titles))\n",
    "                          \n",
    "              \n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "\n",
    "\n",
    "# for item in lista_bins_years:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "    \n",
    "   \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "#     print (minimo, maximo, preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # for year in lista_years:\n",
    "    \n",
    "       \n",
    "     \n",
    "# #     df_select = df_merged[ df_merged[string_filtering] == year  ]\n",
    "# #     print (year, df_select.shape)\n",
    "\n",
    "\n",
    "#     x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#     if cont_rows ==1 and cont_cols ==1:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "#     else:\n",
    "#         trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "\n",
    "#         cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#     fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#     if cont_cols < Ncols:\n",
    "#         cont_cols += 1\n",
    "#     else:\n",
    "#         cont_cols=1\n",
    "#         cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = preselection_df[v1_string]\n",
    "# print (preselection_df.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "        \n",
    "    \n",
    "# # if cont_cols+1 < Ncols:\n",
    "# #     cont_cols += 1\n",
    "# # else:\n",
    "# #     cont_cols=1\n",
    "# #     cont_rows +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=1200, width=1200, title='Location for references, bins by AGE of reference papers')\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONE.  Multiplot location of references, bins by citations of plos papers AND citations of ref_UT\n",
    "\n",
    "\n",
    "\n",
    "# dict_bins_list_papers_for_sampling={}\n",
    "\n",
    "\n",
    "# # lista_bins_ref_UT_citations=[[1,599],[599,4670],[4670,33414],[33414,326393]]  # 4 bins to see the diff. in the very highly cited\n",
    "# # lista_bins_plos_citations=[[1,54],[54,165],[165,332],[332,1173]]  # 4 bins to see the diff. in the very highly cited\n",
    "\n",
    "\n",
    "# string_filtering1 = 'cite_count'   # for reference UT papers\n",
    "# string_filtering2 = 'paper_cite_count'   # for plos papers\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# years=[2009,2010]\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "# print  (preselection_df[string_filtering2].max(), preselection_df[string_filtering2].min() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "# list_q=[0.9,.99,1]     # string_filtering2 = 'paper_cite_count'   # for plos papers\n",
    "\n",
    "# quantiles=sorted(list(preselection_df[string_filtering2].quantile(list_q).to_dict().items())) # [(0.25, 2000.0), (0.5, 2004.0), (0.75, 2006.0), (1.0, 2010.0)]\n",
    "\n",
    "# print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2\n",
    "     \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.9,.99,1]     #string_filtering1 = 'cite_count'   # for reference UT papers\n",
    "# quantiles=sorted(list(preselection_df[string_filtering1].quantile(list_q).to_dict().items())) \n",
    "# print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "# lista_bins_ref_UT_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_ref_UT_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_ref_UT_citations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_titles=[]\n",
    "# for pair2 in lista_bins_plos_citations:\n",
    "#     minimo_cit_plos = pair2[0]\n",
    "#     maximo_cit_plos = pair2[1]\n",
    "        \n",
    "        \n",
    "#     for pair1 in lista_bins_ref_UT_citations:\n",
    "#         minimo_cit_ref_UT = pair1[0]\n",
    "#         maximo_cit_ref_UT = pair1[1]\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "#         df_select = preselection_df[(preselection_df['cite_count'] >= minimo_cit_ref_UT)       &  (preselection_df['cite_count'] < maximo_cit_ref_UT)   &  \\\n",
    "#                                     (preselection_df['paper_cite_count'] >= minimo_cit_plos)  &  (preselection_df['paper_cite_count'] < maximo_cit_plos)  ]\n",
    "        \n",
    "#         print (\"for ref_UT:\",minimo_cit_ref_UT, maximo_cit_ref_UT,\"   for plos:\", minimo_cit_plos , maximo_cit_plos , df_select.shape)\n",
    "\n",
    "  \n",
    "#         tupla=\"plos_cit:\"+str(pair2)+\"-ref_cit\"+str(pair1)\n",
    "#         dict_bins_list_papers_for_sampling[tupla]=list(df_select.paper_UT.unique())\n",
    "        \n",
    "        \n",
    "        \n",
    "#         name=\"plos' cit:\"+str(pair2).replace(\"]\",\")\")+\",ref_UT's cit:\"+str(pair1).replace(\"]\",\")\")+\"<br>N:\"+str(len(df_select))\n",
    "#         lista_titles.append(name)\n",
    "        \n",
    "    \n",
    "      \n",
    "# lista_titles +=[\"All papers in \"+str(years)+\"<br>N:\"+str(len(preselection_df))]\n",
    "\n",
    "# print (lista_titles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Ncols=0\n",
    "# # Nrows=0\n",
    "# # diff=0\n",
    "\n",
    "# # Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# # Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# # diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# # print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# # print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# # if diff == 0.:\n",
    "# #     pass\n",
    "# # else:\n",
    "# #     while Ncols*Nrows < len(lista_titles):\n",
    "# #         Ncols +=1\n",
    "        \n",
    "  \n",
    "# # print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=3\n",
    "# Nrows=5\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=lista_titles)\n",
    "                          \n",
    "                                                                \n",
    "# print (\"subplots created\")\n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "\n",
    "\n",
    "# # year=2010\n",
    "# # preselection_df = df_merged[df_merged['plos_pub_year'] == year]\n",
    "\n",
    "\n",
    "\n",
    "# string_filtering1 = 'cite_count'\n",
    "# string_filtering2 = 'paper_cite_count'\n",
    "\n",
    "\n",
    "# for pair1 in lista_bins_ref_UT_citations:\n",
    "    \n",
    "#     minimo_cit_ref_UT = pair1[0]\n",
    "#     maximo_cit_ref_UT = pair1[1]\n",
    "    \n",
    "    \n",
    "#     for pair2 in reversed(lista_bins_plos_citations):\n",
    "    \n",
    "#         minimo_cit_plos = pair2[0]\n",
    "#         maximo_cit_plos = pair2[1]\n",
    "\n",
    "#         #df_select = preselection_df[(preselection_df['cite_count'] >= minimo_cit_ref_UT)       &  (preselection_df['cite_count'] < maximo_cit_ref_UT)   &  \\\n",
    "#          #                           (preselection_df['paper_cite_count'] >= minimo_cit_plos)  &  (preselection_df['paper_cite_count'] < maximo_cit_plos)  ]\n",
    "       \n",
    "    \n",
    "    \n",
    "#         df_select = preselection_df[(preselection_df['cite_count'] >= minimo_cit_ref_UT)       &  (preselection_df['cite_count'] < maximo_cit_ref_UT)   &  \\\n",
    "#                                     (preselection_df['paper_cite_count'] >= minimo_cit_plos)  &  (preselection_df['paper_cite_count'] < maximo_cit_plos)  ]\n",
    "        \n",
    "#         print (\"for ref_UT:\",minimo_cit_ref_UT, maximo_cit_ref_UT,\"   for plos:\", minimo_cit_plos , maximo_cit_plos , df_select.shape)\n",
    "\n",
    "\n",
    "#         x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#         if cont_rows ==1 and cont_cols ==1:\n",
    "#             trace= Histogram(\n",
    "#                    x=x1, \n",
    "#                    marker=dict(\n",
    "#                    color='#EB89B5'),\n",
    "#                    # name='high cit',\n",
    "#                    histnorm='probability',\n",
    "#                    showlegend=False\n",
    "#                     #cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "#         else:\n",
    "#             trace= Histogram(\n",
    "#                    x=x1, \n",
    "#                    marker=dict(\n",
    "#                    color='#EB89B5'),\n",
    "#                    # name='high cit',\n",
    "#                    histnorm='probability',\n",
    "#                    showlegend=False,\n",
    "#                    xaxis='x'+str(cont_plots),\n",
    "#                    yaxis='y'+str(cont_plots)\n",
    "#                     #cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "\n",
    "#             cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#         fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#         if cont_cols < Ncols:\n",
    "#             cont_cols += 1\n",
    "#         else:\n",
    "#             cont_cols=1\n",
    "#             cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "#  # if i want to modify the axis of one subplot   \n",
    "# #fig['layout']['xaxis1'].update(title='xaxis 1 title')    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = preselection_df[v1_string]\n",
    "# print (preselection_df.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "        \n",
    "    \n",
    "# # if cont_cols+1 < Ncols:\n",
    "# #     cont_cols += 1\n",
    "# # else:\n",
    "# #     cont_cols=1\n",
    "# #     cont_rows +=1\n",
    "\n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=2000, width=2000)#, title='Location for references by total number of citations of plos papers<br>')\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_select = df_merged[(df_merged['ref_pub_year'] - df_merged['plos_pub_year']  ) >1 ]\n",
    "# df_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONE. Multiplot location of references, bins by citations of plos papers AND age of ref_UT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # lista_bins_ref_UT_citations=[[1,599],[599,4670],[4670,33414],[33414,326393]]  # 4 bins to see the diff. in the very highly cited\n",
    "# # lista_bins_plos_citations=[[1,54],[54,165],[165,332],[332,1173]]  # 4 bins to see the diff. in the very highly cited\n",
    "\n",
    "\n",
    "\n",
    "# string_filtering1 = 'ref_pub_year'   # for reference UT papers\n",
    "# string_filtering2 = 'paper_cite_count'   # for plos papers\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# years=[2010]\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "# print  (\"preselection df for years\",years,\"  size:\", preselection_df.shape, \" max\",string_filtering2,preselection_df[string_filtering2].max(), \"min\",preselection_df[string_filtering2].min() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_q=[.25,.5,.75,1]\n",
    "# #list_q=[0.9,.99,1]\n",
    "# quantiles=sorted(list(preselection_df[string_filtering1].quantile(list_q).to_dict().items())) # [(0.25, 2000.0), (0.5, 2004.0), (0.75, 2006.0), (1.0, 2010.0)]\n",
    "\n",
    "# #print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "# lista_bins_ref_UT=[]\n",
    "# old_value=1900\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_ref_UT.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_ref_UT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "# list_q=[0.9,.99,1]\n",
    "\n",
    "# quantiles=sorted(list(preselection_df[string_filtering2].quantile(list_q).to_dict().items())) # [(0.25, 2000.0), (0.5, 2004.0), (0.75, 2006.0), (1.0, 2010.0)]\n",
    "\n",
    "# #print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2\n",
    "     \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########### this loop is only to get the size of each bin and add it as title\n",
    "# lista_titles=[]\n",
    "# for pair2 in lista_bins_plos_citations:\n",
    "#     minimo_cit_plos = pair2[0]\n",
    "#     maximo_cit_plos = pair2[1]\n",
    "        \n",
    "        \n",
    "#     for pair1 in lista_bins_ref_UT:\n",
    "#         minimo_cit_ref_UT = pair1[0]\n",
    "#         maximo_cit_ref_UT = pair1[1]                \n",
    "        \n",
    "#         df_select = preselection_df[(preselection_df[string_filtering1] >= minimo_cit_ref_UT)       &  (preselection_df[string_filtering1] < maximo_cit_ref_UT)   &  \\\n",
    "#                                     (preselection_df[string_filtering2] >= minimo_cit_plos)  &  (preselection_df[string_filtering2] < maximo_cit_plos)  ]\n",
    "        \n",
    "#         print (\"ref_UT:\",minimo_cit_ref_UT, maximo_cit_ref_UT,\"   for plos:\", minimo_cit_plos , maximo_cit_plos , df_select.shape)\n",
    "        \n",
    "#         name=\"plos' cit:\"+str(pair2).replace(\"]\",\")\")+\",ref_UT:\"+str(pair1).replace(\"]\",\")\")+\"<br>N:\"+str(len(df_select))\n",
    "#         lista_titles.append(name)\n",
    "        \n",
    " \n",
    "\n",
    "      \n",
    "# lista_titles +=[\"All papers in \"+str(years)+\"<br>N:\"+str(len(preselection_df))]\n",
    "\n",
    "# print (lista_titles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=0\n",
    "# Nrows=0\n",
    "# diff=0\n",
    "\n",
    "# Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# if diff == 0.:\n",
    "#     pass\n",
    "# else:\n",
    "#     while Ncols*Nrows < len(lista_titles):\n",
    "#         Ncols +=1\n",
    "        \n",
    "  \n",
    "# print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=4\n",
    "# Nrows=5\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=lista_titles)\n",
    "                          \n",
    "                                                                \n",
    "# print (\"subplots created\")\n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "\n",
    "\n",
    "# for pair1 in lista_bins_ref_UT:\n",
    "    \n",
    "#     minimo_year_ref_UT = pair1[0]\n",
    "#     maximo_year_ref_UT = pair1[1]\n",
    "    \n",
    "    \n",
    "#     for pair2 in lista_bins_plos_citations:\n",
    "    \n",
    "#         minimo_cit_plos = pair2[0]\n",
    "#         maximo_cit_plos = pair2[1]\n",
    "\n",
    "    \n",
    "\n",
    "#         df_select = preselection_df[(preselection_df[string_filtering1] >= minimo_year_ref_UT)       &  (preselection_df[string_filtering1] < maximo_year_ref_UT)   &  \\\n",
    "#                                     (preselection_df[string_filtering2] >= minimo_cit_plos)  &  (preselection_df[string_filtering2] < maximo_cit_plos)  ]\n",
    "        \n",
    "#         print (\"for ref_UT:\",minimo_cit_ref_UT, maximo_cit_ref_UT,\"   for plos:\", minimo_cit_plos , maximo_cit_plos , df_select.shape)\n",
    "\n",
    "\n",
    "#         x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#         if cont_rows ==1 and cont_cols ==1:\n",
    "#             trace= Histogram(\n",
    "#                    x=x1, \n",
    "#                    marker=dict(\n",
    "#                    color='#EB89B5'),\n",
    "#                    # name='high cit',\n",
    "#                    histnorm='probability',\n",
    "#                    showlegend=False\n",
    "#                     #cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "#         else:\n",
    "#             trace= Histogram(\n",
    "#                    x=x1, \n",
    "#                    marker=dict(\n",
    "#                    color='#EB89B5'),\n",
    "#                    # name='high cit',\n",
    "#                    histnorm='probability',\n",
    "#                    showlegend=False,\n",
    "#                    xaxis='x'+str(cont_plots),\n",
    "#                    yaxis='y'+str(cont_plots)\n",
    "#                     #cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "\n",
    "#             cont_plots +=1\n",
    "\n",
    "\n",
    "#         print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#         fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#         if cont_cols < Ncols:\n",
    "#             cont_cols += 1\n",
    "#         else:\n",
    "#             cont_cols=1\n",
    "#             cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = preselection_df[v1_string]\n",
    "# print (preselection_df.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "\n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #fig['layout'].update(height=2000, width=2000, title='Location for references by total number of citations of plos papers<br>')\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FALTA. what new papers do you cite? do they end up being well cited? (vs your own cites) \n",
    "# #Multiplot location of references, bins by citations of plos papers AND citations of ref_UT, only for young references!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # lista_bins_ref_UT_citations=[[1,599],[599,4670],[4670,33414],[33414,326393]]  # 4 bins to see the diff. in the very highly cited\n",
    "# # lista_bins_plos_citations=[[1,54],[54,165],[165,332],[332,1173]]  # 4 bins to see the diff. in the very highly cited\n",
    "\n",
    "\n",
    "# string_filtering1 = 'cite_count'   # for reference UT papers\n",
    "# string_filtering2 = 'paper_cite_count'   # for plos papers\n",
    "\n",
    "# v1_string = 'regex_sect_index'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### fist selection: control for plos publication year\n",
    "# years=[2009,2010]\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "# print  (\"1st selection:\",preselection_df.shape )\n",
    "\n",
    "\n",
    "\n",
    "# # second selection: select only young references:\n",
    "# min_year=min(years)\n",
    "# young_years=[ min_year-i  for i in range(5)]\n",
    "\n",
    "# print (years, young_years)\n",
    "\n",
    "\n",
    "# preselection_df = preselection_df[preselection_df['ref_pub_year'].isin(young_years)]\n",
    "\n",
    "\n",
    "# print  (\"2nd selection:\",preselection_df.shape )\n",
    "\n",
    "\n",
    "# #list_q=[0.1,.2,.3,.4,.5,.6,.7,.8,.9,1]\n",
    "# #list_q=[.33,.66,1]\n",
    "# #list_q=[.25,.5,.75,1]\n",
    "# list_q=[0.9,.99,1]     # string_filtering2 = 'paper_cite_count'   # for plos papers\n",
    "\n",
    "# quantiles=sorted(list(preselection_df[string_filtering2].quantile(list_q).to_dict().items())) # [(0.25, 2000.0), (0.5, 2004.0), (0.75, 2006.0), (1.0, 2010.0)]\n",
    "\n",
    "# print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2\n",
    "     \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.9,.99,1]     #string_filtering1 = 'cite_count'   # for reference UT papers\n",
    "# quantiles=sorted(list(preselection_df[string_filtering1].quantile(list_q).to_dict().items())) \n",
    "# print (quantiles )   # 1/3s:  [(0.33, 35.0), (0.66, 116.0), (1.0, 326393.0)]\n",
    "\n",
    "# lista_bins_ref_UT_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_ref_UT_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_ref_UT_citations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_titles=[]\n",
    "# for pair2 in lista_bins_plos_citations:\n",
    "#     minimo_cit_plos = pair2[0]\n",
    "#     maximo_cit_plos = pair2[1]\n",
    "        \n",
    "        \n",
    "#     for pair1 in lista_bins_ref_UT_citations:\n",
    "#         minimo_cit_ref_UT = pair1[0]\n",
    "#         maximo_cit_ref_UT = pair1[1]\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "#         df_select = preselection_df[(preselection_df['cite_count'] >= minimo_cit_ref_UT)       &  (preselection_df['cite_count'] < maximo_cit_ref_UT)   &  \\\n",
    "#                                     (preselection_df['paper_cite_count'] >= minimo_cit_plos)  &  (preselection_df['paper_cite_count'] < maximo_cit_plos)  ]\n",
    "        \n",
    "#         print (\"for ref_UT:\",minimo_cit_ref_UT, maximo_cit_ref_UT,\"   for plos:\", minimo_cit_plos , maximo_cit_plos , df_select.shape)\n",
    "\n",
    "  \n",
    "        \n",
    "#         name=\"plos' cit:\"+str(pair2).replace(\"]\",\")\")+\",ref_UT's cit:\"+str(pair1).replace(\"]\",\")\")+\"<br>N:\"+str(len(df_select))\n",
    "#         lista_titles.append(name)\n",
    "        \n",
    "    \n",
    "      \n",
    "# lista_titles +=[\"All papers in \"+str(years)+\"<br>N:\"+str(len(preselection_df))]\n",
    "\n",
    "# print (lista_titles)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Ncols=0\n",
    "# # Nrows=0\n",
    "# # diff=0\n",
    "\n",
    "# # Ncols=int(np.sqrt(len(lista_titles)))\n",
    "# # Nrows=int(np.sqrt(len(lista_titles)))\n",
    "\n",
    "# # diff=np.sqrt(len(lista_titles)) - int(np.sqrt(len(lista_titles))) \n",
    "# # print (\"sqrt:\",np.sqrt(len(lista_titles)) , int(np.sqrt(len(lista_titles))), \"   diff:\", diff)\n",
    "# # print (\"Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "# # if diff == 0.:\n",
    "# #     pass\n",
    "# # else:\n",
    "# #     while Ncols*Nrows < len(lista_titles):\n",
    "# #         Ncols +=1\n",
    "        \n",
    "  \n",
    "# # print (\"correction:    Nrows and Ncols:\", Nrows, Ncols)\n",
    "\n",
    "\n",
    "\n",
    "# Ncols=3\n",
    "# Nrows=5\n",
    "\n",
    "\n",
    "# fig = tools.make_subplots(rows=Nrows, cols=Ncols, subplot_titles=lista_titles)\n",
    "                          \n",
    "                                                                \n",
    "# print (\"subplots created\")\n",
    "\n",
    "# cont_rows=1\n",
    "# cont_cols=1\n",
    "\n",
    "# cont_plots=2\n",
    "\n",
    "\n",
    "# # year=2010\n",
    "# # preselection_df = df_merged[df_merged['plos_pub_year'] == year]\n",
    "\n",
    "\n",
    "\n",
    "# string_filtering1 = 'cite_count'\n",
    "# string_filtering2 = 'paper_cite_count'\n",
    "\n",
    "\n",
    "# for pair1 in lista_bins_ref_UT_citations:\n",
    "    \n",
    "#     minimo_cit_ref_UT = pair1[0]\n",
    "#     maximo_cit_ref_UT = pair1[1]\n",
    "    \n",
    "    \n",
    "#     for pair2 in reversed(lista_bins_plos_citations):\n",
    "    \n",
    "#         minimo_cit_plos = pair2[0]\n",
    "#         maximo_cit_plos = pair2[1]\n",
    "\n",
    "#         #df_select = preselection_df[(preselection_df['cite_count'] >= minimo_cit_ref_UT)       &  (preselection_df['cite_count'] < maximo_cit_ref_UT)   &  \\\n",
    "#          #                           (preselection_df['paper_cite_count'] >= minimo_cit_plos)  &  (preselection_df['paper_cite_count'] < maximo_cit_plos)  ]\n",
    "       \n",
    "    \n",
    "    \n",
    "#         df_select = preselection_df[(preselection_df['cite_count'] >= minimo_cit_ref_UT)       &  (preselection_df['cite_count'] < maximo_cit_ref_UT)   &  \\\n",
    "#                                     (preselection_df['paper_cite_count'] >= minimo_cit_plos)  &  (preselection_df['paper_cite_count'] < maximo_cit_plos)  ]\n",
    "        \n",
    "#         print (\"for ref_UT:\",minimo_cit_ref_UT, maximo_cit_ref_UT,\"   for plos:\", minimo_cit_plos , maximo_cit_plos , df_select.shape)\n",
    "\n",
    "\n",
    "#         x1 = df_select[v1_string]\n",
    "\n",
    "\n",
    "#         if cont_rows ==1 and cont_cols ==1:\n",
    "#             trace= Histogram(\n",
    "#                    x=x1, \n",
    "#                    marker=dict(\n",
    "#                    color='#EB89B5'),\n",
    "#                    # name='high cit',\n",
    "#                    histnorm='probability',\n",
    "#                    showlegend=False\n",
    "#                     #cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "#         else:\n",
    "#             trace= Histogram(\n",
    "#                    x=x1, \n",
    "#                    marker=dict(\n",
    "#                    color='#EB89B5'),\n",
    "#                    # name='high cit',\n",
    "#                    histnorm='probability',\n",
    "#                    showlegend=False,\n",
    "#                    xaxis='x'+str(cont_plots),\n",
    "#                    yaxis='y'+str(cont_plots)\n",
    "#                     #cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "\n",
    "#             cont_plots +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "#         fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "\n",
    "\n",
    "#         if cont_cols < Ncols:\n",
    "#             cont_cols += 1\n",
    "#         else:\n",
    "#             cont_cols=1\n",
    "#             cont_rows +=1\n",
    "\n",
    "   \n",
    "        \n",
    "   \n",
    "\n",
    "#  # if i want to modify the axis of one subplot   \n",
    "# #fig['layout']['xaxis1'].update(title='xaxis 1 title')    \n",
    "\n",
    "# ### i add the final subplot for the entire dataset\n",
    "# x1 = preselection_df[v1_string]\n",
    "# print (preselection_df.shape)\n",
    "# trace= Histogram(\n",
    "#                x=x1, \n",
    "#                marker=dict(\n",
    "#                color='#EB89B5'),\n",
    "#                # name='high cit',\n",
    "#                histnorm='probability',\n",
    "#                showlegend=False,\n",
    "#                xaxis='x'+str(cont_plots),\n",
    "#                yaxis='y'+str(cont_plots)\n",
    "#                 #cumulative=dict(enabled=True)\n",
    "#                )\n",
    "        \n",
    "    \n",
    "# # if cont_cols+1 < Ncols:\n",
    "# #     cont_cols += 1\n",
    "# # else:\n",
    "# #     cont_cols=1\n",
    "# #     cont_rows +=1\n",
    "\n",
    "\n",
    "# fig.append_trace(trace, cont_rows, cont_cols)   \n",
    "# print (\" row:\", cont_rows, \"col:\", cont_cols)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout'].update(height=2000, width=2000)#, title='Location for references by total number of citations of plos papers<br>')\n",
    "# #iplot(fig, filename='simple-subplot')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "#               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### 2D histograms\n",
    "\n",
    "# ### fist selection: control for plos publication year\n",
    "# years=[2008]\n",
    "\n",
    "\n",
    "\n",
    "# x1_var='paper_cite_count'\n",
    "# x2_var='cite_count'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "# print  (\"1st selection:\",preselection_df.shape )\n",
    "\n",
    "\n",
    "# young_window=5\n",
    "\n",
    "# # second selection: select only young references:\n",
    "# min_year=min(years)\n",
    "# young_years=[ min_year-i  for i in range(young_window)]\n",
    "\n",
    "# print (years, young_years)\n",
    "\n",
    "\n",
    "# preselection_df = preselection_df[preselection_df['ref_pub_year'].isin(young_years)]\n",
    "\n",
    "\n",
    "# print  (\"2nd selection:\",preselection_df.shape )\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove the zeros to be able to plot log 2d histogram\n",
    "# preselection_df = preselection_df[preselection_df[x1_var]>0]\n",
    "# preselection_df = preselection_df[preselection_df[x2_var]>0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x = preselection_df[x1_var]  # plos paper's citations\n",
    "# y = preselection_df[x2_var]        # reference paper's citations  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trace = Histogram2d(x=x, y=y, histnorm='probability',\n",
    "#         autobinx=False,\n",
    "#         xbins=dict(start=0, end=100, size=1),\n",
    "#         autobiny=False,\n",
    "#         ybins=dict(start=1, end=600, size=1),\n",
    "#         colorscale=[[0, 'rgb(12,51,131)'], [0.25, 'rgb(10,136,186)'], [0.5, 'rgb(242,211,56)'], [0.75, 'rgb(242,143,56)'], [1, 'rgb(217,30,30)']]\n",
    "#     )\n",
    "\n",
    "\n",
    "# data = [ trace ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(\n",
    "#     xaxis=dict(\n",
    "#        # type='log',\n",
    "#      title= x1_var ,\n",
    "#         #autorange=True\n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         #type='log',\n",
    "#      title=x2_var ,\n",
    "#         #autorange=True\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "# iplot(fig, filename='Number_PLOS_papers_published_per_year.html')\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"histogram_2d.html\" ,\n",
    "#              output_type='file', image_width=1600, image_height=1200, filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/histogram_2d.html', validate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.max()  # y: 1   - 49346;  x: 1-1994\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # library & dataset\n",
    "# import seaborn as sns\n",
    "# df = sns.load_dataset('iris')\n",
    " \n",
    "# # Basic 2D density plot\n",
    "# sns.set_style(\"white\")\n",
    "# sns.kdeplot(df.sepal_width, df.sepal_length)\n",
    "# #sns.plt.show()\n",
    " \n",
    "# # Custom it with the same argument as 1D density plot\n",
    "# sns.kdeplot(df.sepal_width, df.sepal_length, cmap=\"Reds\", shade=True, bw=.15)\n",
    " \n",
    "# # Some features are characteristic of 2D: color palette and wether or not color the lowest range\n",
    "# sns.kdeplot(df.sepal_width, df.sepal_length, cmap=\"Blues\", shade=True, shade_lowest=True, )\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ### fist selection: control for plos publication year\n",
    "# years=[2009,2010]\n",
    "\n",
    "\n",
    "\n",
    "# x1_var='paper_cite_count'\n",
    "# x2_var='cite_count'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "# print  (\"1st selection:\",preselection_df.shape )\n",
    "\n",
    "\n",
    "# young_window=2\n",
    "\n",
    "# # second selection: select only young references:\n",
    "# min_year=min(years)\n",
    "# young_years=[ min_year-i  for i in range(young_window)]\n",
    "\n",
    "# print (years, young_years)\n",
    "\n",
    "\n",
    "# preselection_df = preselection_df[preselection_df['ref_pub_year'].isin(young_years)]\n",
    "\n",
    "\n",
    "# print  (\"2nd selection:\",preselection_df.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x = preselection_df[x1_var]  # plos paper's citations\n",
    "# y = preselection_df[x2_var]        # reference paper's citations  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # library & dataset\n",
    "# import seaborn as sns\n",
    "# df = sns.load_dataset('iris')\n",
    " \n",
    "# # Basic 2D density plot\n",
    "# sns.set_style(\"white\")\n",
    "# sns.kdeplot(x, y)\n",
    "# #sns.plt.show()\n",
    " \n",
    "# # Custom it with the same argument as 1D density plot\n",
    "# sns.kdeplot(x, y, cmap=\"Reds\", shade=True, bw=.15)\n",
    " \n",
    "# # Some features are characteristic of 2D: color palette and wether or not color the lowest range\n",
    "# sns.kdeplot(x, y, cmap=\"Blues\", shade=True, shade_lowest=True, )\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.plos_pub_year.max()#  2005-2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONE. THE BEHAVIOUR IN THE DISCUSSION or INTRO SECTION etc: distribution of publication year of references, bins by citations of plos papers  (controlling for plos publ. year)\n",
    "\n",
    "\n",
    "\n",
    "# string_section=\"discussion\"\n",
    "\n",
    "# if  string_section == \"intro\":\n",
    "#     section=0\n",
    "# elif  string_section == \"methods\":\n",
    "#     section=1\n",
    "# elif  string_section == \"results\":\n",
    "#     section=2\n",
    "# elif  string_section == \"discussion\":\n",
    "#     section=3\n",
    "\n",
    "\n",
    "\n",
    "# ### preselection by plos year\n",
    "# years=[2012]\n",
    "# print (string_section, years)\n",
    "\n",
    "\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]\n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "# # preselection to include only occurences in the DISCUSSION section of the papers\n",
    "# preselection_df = preselection_df[preselection_df['regex_sect_index']== section]   \n",
    "# print (\"size of preselection2 (by section):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ### preselection only young/old references:\n",
    "# time_window = 2\n",
    "# preselection_df = preselection_df[preselection_df['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "# print (\"size of preselection3 (only young references):\",preselection_df.shape, \" time_window >=\", (min(years)-time_window))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# v1_string = 'cite_count' #varible:   ref_pub_year'  #'cite_count'\n",
    "\n",
    "\n",
    "# string_filtering = 'paper_cite_count'   # bins by plos' citations\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i get the bins number of citation of the plos papers\n",
    "# list_q=[0.5,0.9,.99,1]\n",
    "\n",
    "# quantiles=sorted(list(preselection_df[string_filtering].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "    \n",
    "        \n",
    "# ### i modify the bins to separete the zero-one\n",
    "# # lista_bins_plos_citations[0][0]=2       \n",
    "# # lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data=[]\n",
    "\n",
    "# # max_x=preselection_df[v1_string].max()\n",
    "# # min_x=preselection_df[v1_string].min()\n",
    "# # Nbins=1000\n",
    "# # range_values=(min_x, max_x)  ## OJO! to be able to compare histograms with bins, i need the same number of bins and interval for all sets!!\n",
    "\n",
    "# # print (range_values)\n",
    "\n",
    "\n",
    "\n",
    "# ### first i add the histogram for all data selection\n",
    "# #count, boundary_bins = np.histogram(preselection_df[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "# #cumulat=np.cumsum(count)\n",
    "\n",
    "# #counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "# #print (boundary_bins)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# x1_All = list(preselection_df[v1_string])\n",
    "# max_x=max(x1_All)\n",
    "# min_x=min(x1_All)\n",
    "\n",
    "# size=1\n",
    "\n",
    "# trace= Histogram(\n",
    "#             x=x1_All, \n",
    "# #             marker = dict(       \n",
    "# #                      color = 'rgba(0, 0, 0, .8)',\n",
    "# #                      symbol=\"square\"),\n",
    "#              name='All PLOS from '+str(years)+\"<br>N:\"+str(len(preselection_df)),\n",
    "#              histnorm='probability',\n",
    "#              xbins=dict(\n",
    "#                        start=min_x,\n",
    "#                        end=max_x,\n",
    "#                        size=1\n",
    "#                        ),\n",
    "# #             showlegend=False\n",
    "#             cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "\n",
    "\n",
    "# data.append(trace)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# i=1\n",
    "# for item in lista_bins_plos_citations:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "          \n",
    "#     df_select = preselection_df[(preselection_df[string_filtering] >= minimo)  &  (preselection_df[string_filtering] < maximo)]\n",
    "                \n",
    "   \n",
    "#     x1 = list(df_select[v1_string])\n",
    "#     #count, boundary_bins = np.histogram(df_select[v1_string], bins=Nbins, normed=True, range=range_values)\n",
    "#     #counts_cumulat = np.cumsum(count)#, bins=Nbins, normed=True, range=range_values)\n",
    "    \n",
    "   \n",
    "#     trace= Histogram(\n",
    "#                    x=x1, \n",
    "# #                    marker=dict(\n",
    "# #                           color='#EB89B5'),\n",
    "#                    name=\"plos' cit: \"+str(item)+\"<br>N:\"+str(len(df_select)),\n",
    "#                    histnorm='probability',\n",
    "#                    xbins=dict(\n",
    "#                        start=min_x,\n",
    "#                        end=max_x,\n",
    "#                        size=1\n",
    "#                        ),\n",
    "#                   # showlegend=False\n",
    "#                    cumulative=dict(enabled=True)\n",
    "#                    )\n",
    "\n",
    "#     data.append(trace)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "#     x1_rand = random.sample(x1_All, len(x1))\n",
    "\n",
    "#     trace_rand= Histogram(\n",
    "#                        x=x1_rand, \n",
    "#     #                    marker=dict(\n",
    "#     #                           color='#EB89B5'),\n",
    "#                        name=\"rand\"+str(i)+\" N:\"+str(len(x1)),\n",
    "#                        histnorm='probability',\n",
    "#                        xbins=dict(\n",
    "#                        start=min_x,\n",
    "#                        end=max_x,\n",
    "#                        size=1\n",
    "#                        ),\n",
    "#                       # showlegend=False\n",
    "#                       cumulative=dict(enabled=True)\n",
    "#                        )\n",
    "\n",
    "#     data.append(trace_rand)  \n",
    "    \n",
    "    \n",
    "    \n",
    "#     i+=1\n",
    "    \n",
    "    \n",
    "#     print (\"\\n\",item, df_select.shape, \"median All:\",np.median(x1_All),\"std:\",np.nanstd(x1_All), \"   median selection:\",np.median(x1),\"std:\",np.nanstd(x1), \"   median rand:\",np.median(x1_rand),\"std:\",np.nanstd(x1_rand))#, count, boundary_bins)\n",
    "      \n",
    "#     #print (\"\\n\",item, df_select.shape, \"avg All:\",np.nanmean(x1_All),\"std:\",np.nanstd(x1_All), \"   avg selection:\",np.nanmean(x1),\"std:\",np.nanstd(x1), \"   avg rand:\",np.nanmean(x1_rand),\"std:\",np.nanstd(x1_rand))#, count, boundary_bins)\n",
    "#     print (\"comparison All to selection set:\",item, stats.ks_2samp(x1_All, x1))  \n",
    "#     # This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution.\n",
    "#     #If the K-S statistic is small or the p-value is high, then we cannot reject the hypothesis that the distributions of the two samples are the same\n",
    "    \n",
    "#     print (\"comparison selection set to same size random set:\", stats.ks_2samp(x1, x1_rand))      \n",
    "#     print (\"comparison same size random set to All:\", stats.ks_2samp(x1_All, x1_rand))  \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# #### i add a couple random samples \n",
    "\n",
    "# # for i in range(2):\n",
    "# #     x1= list(preselection_df[v1_string]    )\n",
    "# #     x1 = random.sample(x1, 100000)\n",
    "\n",
    "# #     trace= Histogram(\n",
    "# #                        x=x1, \n",
    "# #     #                    marker=dict(\n",
    "# #     #                           color='#EB89B5'),\n",
    "# #                        name=\"random\"+str(i)+\" N:\"+str(len(x1)),\n",
    "# #                        histnorm='probability',\n",
    "# #                       # showlegend=False\n",
    "# #                       cumulative=dict(enabled=True)\n",
    "# #                        )\n",
    "\n",
    "# #     data.append(trace)  \n",
    "\n",
    "# #     print (\"\\ncomparison All to random set:\", stats.ks_2samp(x1_All, x1))      \n",
    "\n",
    "\n",
    "\n",
    "# # for i in range(2):\n",
    "# #     x1= list(preselection_df[v1_string]    )\n",
    "# #     x1 = random.sample(x1, 1000)\n",
    "\n",
    "# #     trace= Histogram(\n",
    "# #                        x=x1, \n",
    "# #     #                    marker=dict(\n",
    "# #     #                           color='#EB89B5'),\n",
    "# #                        name=\"random\"+str(1+i)+\" N:\"+str(len(x1)),\n",
    "# #                        histnorm='probability',\n",
    "# #                       # showlegend=False\n",
    "# #                       cumulative=dict(enabled=True)\n",
    "# #                        )\n",
    "\n",
    "# #     data.append(trace)  \n",
    "\n",
    "# #     print (\"\\ncomparison All to random set:\", stats.ks_2samp(x1_All, x1))      \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "#     bargap=0.01,\n",
    "\n",
    "#     title=\"Age of references in the \"+string_section+\" for plos from\"+str(years),\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= v1_string,   \n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#        #range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#       #type='log',\n",
    "      \n",
    "       \n",
    "#     ),\n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         type='log',\n",
    "#         titlefont=dict(\n",
    "#             family=font,#family='Arial, sans-serif',\n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "# )       \n",
    "\n",
    "\n",
    "# # fig = Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # offline.plot(fig, auto_open=True, image = 'png', image_filename=\"simple-subplot\" ,\n",
    "# #               filename='/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/simple-subplot.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.self_citation.value_counts()\n",
    "\n",
    "# 0    5,856,677\n",
    "# 1    1,067,592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.categ_codes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## OLD ARRANGEMENT COLUMNS-ROWS FOR HEATMAP\n",
    "# # DONE! annotated heatmap plot for median publication year (OR CITATIONS) of the references used in the different sections, and separating by citation category of the plos\n",
    "\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_median_value={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string =  'diff_year_plos_ref'#  log2_num_cit_ref'  ######  log2_num_cit_ref' # #     #'ref_pub_year'     cite_count    diff_year_plos_ref \n",
    "       \n",
    "\n",
    "# years=[2013]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "# string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "\n",
    "    \n",
    "\n",
    "# string_references_age=\"young\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "    \n",
    "# string_self_ref=0#\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. records: 6.9M\n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    \n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "    \n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_colorscale = \"Reds\"\n",
    "#     fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "  \n",
    "#     if  v1_string ==  'log_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     elif  v1_string ==  'log2_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# elif v1_string =='ref_pub_year':\n",
    "#     fig_colorscale = \"Viridis\"\n",
    "#     fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "# elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "#     fig_colorscale = [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "#     fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "           \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "# #list_q=[0.5,0.9,.99,1]\n",
    "# #list_q=[0.25,0.5,0.9,.99,1]\n",
    "# list_q=[0.3,0.6,.9,.99,1]\n",
    "\n",
    "# #quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "\n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# # lista_bins_plos_citations[0][0]=2       \n",
    "# # lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "# ################################################3\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_sections = [\"intro\",\"methods\",\"results\",\"discussion\"]\n",
    "# for string_section in lista_sections:\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection to include only occurences in the DISCUSSION section of the papers\n",
    "#     if  string_section == \"intro\":\n",
    "#         section=0\n",
    "#     elif  string_section == \"methods\":\n",
    "#         section=1\n",
    "#     elif  string_section == \"results\":\n",
    "#         section=2\n",
    "#     elif  string_section == \"discussion\":\n",
    "#         section=3\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#     preselection_df4 = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "#     #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "#     x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "#     cont=0\n",
    "#     for item in lista_bins_plos_citations:\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]\n",
    "        \n",
    "            \n",
    "                \n",
    "        \n",
    "#         df_select = preselection_df4[(preselection_df4[string_filtering_x] >= minimo)  &  (preselection_df4[string_filtering_x] < maximo)]\n",
    "#         #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "        \n",
    "#         x1 = list(df_select[v1_string])\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "# #         if cont ==0:            \n",
    "# #             group=string_section+\" Bottom 50%\"          \n",
    "# #         elif cont==1:\n",
    "# #              group=string_section+\" 51% to 90%\"            \n",
    "# #         elif cont==2: \n",
    "# #             group=string_section+\" 91% to 99%\"            \n",
    "# #         elif cont==3:\n",
    "# #              group=string_section+\" Top 1%\"     \n",
    "        \n",
    "        \n",
    "\n",
    "# #         if cont ==0:            \n",
    "# #             group=string_section+\" Bottom 25%\" \n",
    "# #         elif cont ==1:            \n",
    "# #             group=string_section+\" 26% to 50%\"            \n",
    "# #         elif cont==2:\n",
    "# #              group=string_section+\" 51% to 90%\"            \n",
    "# #         elif cont==3: \n",
    "# #             group=string_section+\" 91% to 99%\"            \n",
    "# #         elif cont==4:\n",
    "# #              group=string_section+\" Top 1%\"    \n",
    "\n",
    "\n",
    "#         if cont ==0:            \n",
    "#             group=string_section+\" Bottom 30%\" \n",
    "#         elif cont ==1:            \n",
    "#             group=string_section+\" 31% to 60%\"            \n",
    "#         elif cont==2:\n",
    "#              group=string_section+\" 61% to 90%\"            \n",
    "#         elif cont==3: \n",
    "#             group=string_section+\" 91% to 99%\"            \n",
    "#         elif cont==4:\n",
    "#              group=string_section+\" Top 1%\"    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "#         tupla=[np.nanmedian(x1), len(x1)]\n",
    "#         dict_group_median_value[group]=tupla\n",
    "        \n",
    "#         dict_group_subset_data[group]=x1\n",
    "# #         print (\"group\",group)\n",
    "        \n",
    "#         cont +=1\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#     ### i also add the median values for the section across all data in the preselection\n",
    "#     tupla=[np.nanmedian(x1_All), len(x1_All)]\n",
    "#     dict_group_median_value[string_section+\" All\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" All\"]=x1_All    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "# lista_y=lista_sections\n",
    "# lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" All\"]\n",
    "# #lista_x=[\" Bottom 50%\",\" 51% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" All\"]\n",
    "\n",
    "\n",
    "# lista_z=[]\n",
    "# lista_z_sizes=[]\n",
    "\n",
    "# for y_value in lista_y:\n",
    "#     aux_lista=[]\n",
    "#     aux_lista_sizes=[]\n",
    "    \n",
    "#     for x_value in lista_x:    \n",
    "# #         for llave in dict_group_median_value:\n",
    "# #             if (x_value in llave) and (y_value in llave):\n",
    "\n",
    "#         llave=y_value+x_value\n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_median_value[llave][0])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_median_value[llave][0]\n",
    "#         aux_lista.append(value)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         value_size=dict_group_median_value[llave][1]\n",
    "#         aux_lista_sizes.append(value_size)\n",
    "        \n",
    "        \n",
    "#         #print (y_value,\" \",x_value, value, value_size)\n",
    "#     lista_z.append(aux_lista)\n",
    "#     lista_z_sizes.append(aux_lista_sizes)\n",
    "    \n",
    "   \n",
    "\n",
    "# print (lista_z)\n",
    "# print (lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_text_z=[]\n",
    "# for i in range(len(lista_z)):\n",
    "#     aux=[]\n",
    "#     for j in range(len(lista_z[0])):\n",
    "#         value=\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "#         aux.append(value)\n",
    "#     lista_text_z.append(aux)\n",
    "# # print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### using a different library\n",
    "# path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=lista_z, x=lista_x, y=lista_y, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "# fig.layout.title =fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# fig.layout.xaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "# fig.layout.yaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=10   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral# -5\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral #-5\n",
    "\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "\n",
    "\n",
    "# # Altering main font\n",
    "# #fig['layout']['font'] [\"family\"] = \"Gill Sans MT\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1400,\n",
    "#               filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for pair in itertools.combinations(dict_group_subset_data.keys(), 2):    \n",
    "#     set1=dict_group_subset_data[pair[0]]\n",
    "#     set2=dict_group_subset_data[pair[1]]\n",
    "#     print (\"comparison\",pair, \"\\t\\t\",stats.ks_2samp(set1, set2),\"\\n\"  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged[df_merged['plos_j1']=='PLOS ONE']\n",
    "df_merged.plos_j1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###  MORE TEXT IN EACH CELL AND FLIPPING COLUMNS-ROWS IN THE HEATMAP ...........\n",
    "# # DONE! annotated heatmap plot for median publication year (OR CITATIONS) of the references used in the different sections, and separating by citation category of the plos\n",
    "# ###  OLD\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string =  'cite_count'#  log2_num_cit_ref'  ######  log2_num_cit_ref' # #     #'ref_pub_year'     cite_count    diff_year_plos_ref \n",
    "       \n",
    "\n",
    "# years=[2011] \n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "# string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "# string_references_age = \"young\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "# string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "# string_self_ref =0      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    \n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "   \n",
    "    \n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "# fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    \n",
    "    \n",
    "#     fig_colorscale = \"Reds\"\n",
    "#     fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "  \n",
    "#     if  v1_string ==  'log_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     elif  v1_string ==  'log2_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# elif v1_string =='ref_pub_year':\n",
    "#     fig_colorscale = \"Viridis\"\n",
    "#     fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "# elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "#     fig_colorscale = [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "#     fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "           \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "# #quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     try:\n",
    "#         pair=[old_value, int(item[1])]    \n",
    "#     except:  # if it is a nan:\n",
    "#         pair=[old_value, item[1]]\n",
    "    \n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "#     try:\n",
    "#         old_value = int(item[1])\n",
    "#     except:\n",
    "#         old_value = item[1]\n",
    "\n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# # lista_bins_plos_citations[0][0]=2       \n",
    "# # lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "# ################################################\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_sections = [\"intro\",\"methods\",\"results\",\"discussion\"]\n",
    "\n",
    "# cont=0\n",
    "# for item in lista_bins_plos_citations:\n",
    "\n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "\n",
    "\n",
    "#     preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "#     #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "#     x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "#     for string_section in lista_sections:\n",
    "\n",
    "\n",
    "#         ##### preselection to include only occurences in a section of the paper\n",
    "#         if  string_section == \"intro\":\n",
    "#             section=0\n",
    "#         elif  string_section == \"methods\":\n",
    "#             section=1\n",
    "#         elif  string_section == \"results\":\n",
    "#             section=2\n",
    "#         elif  string_section == \"discussion\":\n",
    "#             section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "#         #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "#         x1 = list(df_select[v1_string])       \n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "#         if cont ==0:            \n",
    "#             group=string_section+\" Bottom \"+str(int(100.*list_q[0]))+\"%\"             \n",
    "#         elif cont ==1:            \n",
    "#             group=string_section+\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\"            \n",
    "#         elif cont==2:\n",
    "#              group=string_section+\" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\"              \n",
    "#         elif cont==3: \n",
    "#             group=string_section+\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\"          \n",
    "#         elif cont==4:\n",
    "#              group=string_section+\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"    \n",
    "\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "#         ######### i get also quantiles for each cell:    \n",
    "#         list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "#         values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "#         tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "#         dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "#         dict_group_subset_data[group]=x1\n",
    "               \n",
    "\n",
    "\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ################ i also add the median values for the section across all data in the preselection\n",
    "# for string_section in lista_sections:\n",
    "\n",
    "        \n",
    "#     if  string_section == \"intro\":\n",
    "#         section=0\n",
    "#     elif  string_section == \"methods\":\n",
    "#         section=1\n",
    "#     elif  string_section == \"results\":\n",
    "#         section=2\n",
    "#     elif  string_section == \"discussion\":\n",
    "#         section=3\n",
    "\n",
    "\n",
    "#     df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "        \n",
    "#     list_quantiles_cell=[.25,.5,.75]\n",
    "#     values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "#     tupla=values_quantiles + [len(df_select)]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# ########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "# lista_y=lista_sections\n",
    "# #lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "# lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "#                  \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "# #lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "# lista_x=lista_bin_names\n",
    "\n",
    "# lista_z25=[]\n",
    "# lista_z50=[]\n",
    "# lista_z75=[]\n",
    "# lista_z_sizes=[]\n",
    "\n",
    "# for x_value in lista_x:    \n",
    "#     aux_lista25=[]\n",
    "#     aux_lista50=[]\n",
    "#     aux_lista75=[]\n",
    "#     aux_lista_sizes=[]\n",
    "    \n",
    "#     for y_value in lista_y:       \n",
    "\n",
    "#         llave=y_value+x_value\n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][0])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][0]\n",
    "#         aux_lista25.append(value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][1])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][1]\n",
    "#         aux_lista50.append(value)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][2])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][2]\n",
    "#         aux_lista75.append(value)\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "#         value_size=dict_group_quantiles_size[llave][3]\n",
    "#         aux_lista_sizes.append(value_size)\n",
    "        \n",
    "        \n",
    "#         #print (y_value,\" \",x_value, value, value_size)\n",
    "#     lista_z25.append(aux_lista25)\n",
    "#     lista_z50.append(aux_lista50)\n",
    "#     lista_z75.append(aux_lista75)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     lista_z_sizes.append(aux_lista_sizes)\n",
    "    \n",
    "   \n",
    "\n",
    "# # print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# # print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_text_z=[]\n",
    "# for i in range(len(lista_z_sizes)):\n",
    "#     aux=[]\n",
    "#     for j in range(len(lista_z_sizes[0])):\n",
    "#         value=str(lista_z25[i][j])+\",<b>\"+str(lista_z50[i][j])+\"</b>,\"+str(lista_z75[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "#         aux.append(value)\n",
    "#     lista_text_z.append(aux)\n",
    "# # print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### using a different library\n",
    "# path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "# #fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "# fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# fig.layout.xaxis.update({'title': 'Section'})\n",
    "# # fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=48   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -3\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -20\n",
    "\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "\n",
    "\n",
    "# # fig['layout']['margin']=dict(\n",
    "# #         l=200,\n",
    "# #        # r=50,\n",
    "# #         b=150,\n",
    "# #         #t=100,\n",
    "# #        # pad=4\n",
    "# #     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1400,\n",
    "#               filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # for pair in itertools.combinations(dict_group_subset_data.keys(), 2):    \n",
    "# #     set1=dict_group_subset_data[pair[0]]\n",
    "# #     set2=dict_group_subset_data[pair[1]]\n",
    "# #     print (\"comparison\",pair, \"\\t\\t\",stats.ks_2samp(set1, set2),\"\\n\"  )\n",
    "\n",
    "    \n",
    "# # print (\"\\n\", fig_title_plot)\n",
    "\n",
    "\n",
    "# ## COMPARISON OF CELL IN THE PLOT:\n",
    "# ######################################################\n",
    "# ######################################################\n",
    "# ######################################################\n",
    "# ######################################################\n",
    "# ######################################################\n",
    "# ######################################################\n",
    "\n",
    "\n",
    "# list_keys_macro = ['intro Top 1%',   'methods Top 1%',   'results Top 1%','discussion Top 1%', \\\n",
    "#                    'intro 91% to 99%', 'methods 91% to 99%',  'results 91% to 99%', 'discussion 91% to 99%', \\\n",
    "#                    'intro 61% to 90%',  'methods 61% to 90%', 'results 61% to 90%', 'discussion 61% to 90%', \\\n",
    "#                    'intro 31% to 60%', 'methods 31% to 60%', 'results 31% to 60%', 'discussion 31% to 60%', \\\n",
    "#                    'intro Bottom 30%', 'methods Bottom 30%', 'results Bottom 30%','discussion Bottom 30%',\\\n",
    "#                    'intro ALL PLOS', 'methods ALL PLOS',    'results ALL PLOS',  'discussion ALL PLOS']\n",
    "    \n",
    "      \n",
    "    \n",
    "# list_keys_heatmap = ['intro ALL PLOS', 'methods ALL PLOS',  'results ALL PLOS',  'discussion ALL PLOS',\\\n",
    "#                    'intro Bottom 30%', 'methods Bottom 30%', 'results Bottom 30%','discussion Bottom 30%',\\\n",
    "#                    'intro 31% to 60%', 'methods 31% to 60%', 'results 31% to 60%', 'discussion 31% to 60%', \\\n",
    "#                    'intro 61% to 90%',  'methods 61% to 90%', 'results 61% to 90%', 'discussion 61% to 90%', \\\n",
    "#                    'intro 91% to 99%', 'methods 91% to 99%',  'results 91% to 99%', 'discussion 91% to 99%', \\\n",
    "#                    'intro Top 1%',     'methods Top 1%',     'results Top 1%','discussion Top 1%']\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "# test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    " \n",
    "\n",
    "# ### i create the empty subplot figure    \n",
    "# tot_rows = 6\n",
    "# tot_cols = 4                                                                          \n",
    "# #fig = tools.make_subplots(rows=tot_rows, cols=tot_cols, subplot_titles=[i+\" VS:\" for i in list_keys_macro],   specs = [[{}, {}, {}, {}], [{}, {}, {}, {}], [{}, {}, {}, {}], [{}, {}, {}, {}], [{}, {}, {}, {}], [{}, {}, {}, {}]], vertical_spacing = 0.04)\n",
    "#  # to be able to change the vertical_spacing or horizontal, i need to pass the argument specs, with a list of lists corresponding to as many columns and rows\n",
    "    \n",
    "# #fig = tools.make_subplots(rows=tot_rows, cols=tot_cols, subplot_titles=[i+\" VS:\" for i in list_keys_macro],  vertical_spacing = 0.04, horizontal_spacing = 0.06)\n",
    "# #fig = tools.make_subplots(rows=tot_rows, cols=tot_cols, subplot_titles=[i+\" VS:\" for i in list_keys_macro],  vertical_spacing = 0.02, horizontal_spacing = 0.02,  shared_xaxes=True, shared_yaxes=True )\n",
    "# fig = tools.make_subplots(rows=tot_rows, cols=tot_cols, vertical_spacing = 0.02, horizontal_spacing = 0.02,  shared_xaxes=True, shared_yaxes=True )\n",
    "\n",
    "\n",
    "#  ### i apply the bonferroni correction:  new p-_value required for significance = old_p_value /Number of comparisons = 0.001 / (24*23/2)       \n",
    "# threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# cont_rows = 1\n",
    "# cont_cols = 1\n",
    "  \n",
    "# for i in list_keys_macro:\n",
    "#     lista_listas=[]\n",
    "#     aux_lista=[]\n",
    "#     cont=1\n",
    "#     print (i)\n",
    "#     for j in list_keys_heatmap:\n",
    "        \n",
    "        \n",
    "            \n",
    "#         set1 = dict_group_subset_data[i]\n",
    "#         set2 = dict_group_subset_data[j]\n",
    "        \n",
    "#         if test == \"KS\":\n",
    "#             p_value = stats.ks_2samp(set1, set2)[1] \n",
    "#         elif test == \"MW\":\n",
    "#             p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "#         if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "#                 p_value =0.\n",
    "                \n",
    "                \n",
    "                \n",
    "#         if i == j:  # i single out manually the self comparison\n",
    "#             p_value=1.001           \n",
    "            \n",
    "            \n",
    "#         aux_lista.append(p_value)\n",
    "        \n",
    "#         cont +=1\n",
    "        \n",
    "#         if cont >4:\n",
    "#             lista_listas.append(aux_lista)                  \n",
    "#             aux_lista=[]\n",
    "#             cont=1\n",
    "            \n",
    "       \n",
    "#         print (\"  --\", j, \"\\t\\t\",stats.ks_2samp(set1, set2), stats.mannwhitneyu(set1, set2,  alternative='two-sided'),\"\\n\"  )\n",
    "\n",
    "        \n",
    "        \n",
    "#     flatten_lista = list(itertools.chain(*lista_listas))  # i flatten the list of lists to get the minimum and maximum value\n",
    "#     minimo = min(flatten_lista)\n",
    "#     maximo = max(flatten_lista)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     ### order for one single heatmap:\n",
    "#     # z = [[ALL papers (row) intro, methods, results, disc],[BOTTOM 30% (2nd row) intro, methods, results, disc],[],[],[],[TOP 1% (row) intro, methods, results, disc]]   \n",
    "#     trace = go.Heatmap(z=lista_listas,\n",
    "#                    y=['ALL papers', 'Bottom 30%', '31%-60%','61%-90%','90%-99%','Top 1%'],\n",
    "#                    x=['Introduction', 'Methods', 'Results', 'Discussion'],\n",
    "#                    colorscale=  [ [0., '#f2f2f2'],  [0.9999, '#0059b3'], [1., '#ffffff']],     #ojo! normalized 0-1 values for colorscale!!!\n",
    "# #                    colorbar = dict(\n",
    "# #                         #title = 'Surface Heat',\n",
    "# #                         #titleside = 'top',\n",
    "# #                         #tickmode = 'array',\n",
    "# #                         tickvals = [ .01],\n",
    "# #                         ticktext = ['non-significant'],\n",
    "# #                         ticks = 'outside'),\n",
    "# #                     showscale=False\n",
    "#                   )\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ### It starts counting the rowns and columns from top left corner\n",
    "#     print (cont_rows, cont_cols)\n",
    "#     fig.append_trace(trace, cont_rows, cont_cols)\n",
    "   \n",
    "   \n",
    "\n",
    "\n",
    "   \n",
    "#     cont_cols += 1\n",
    "#     if cont_cols > tot_cols:\n",
    "#         cont_cols = 1\n",
    "#         cont_rows += 1\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "  \n",
    "# # # fig['layout']['xaxis']['side'] = 'bottom'\n",
    "\n",
    "# # # # fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "# # fig['layout']['yaxis'].update({'title': 'Sections'})\n",
    "\n",
    "\n",
    "\n",
    "# fontsize=25  # 20 \n",
    "\n",
    "# if v1_string == 'cite_count':\n",
    "    \n",
    "#   v1_string = v1_string + \"_\" + string_references_age+ \"_references\" \n",
    "    \n",
    "    \n",
    "# elif v1_string == 'diff_year_plos_ref':\n",
    "    \n",
    "#   v1_string = \"age difference PLOS-references\" \n",
    "\n",
    "    \n",
    "    \n",
    "# ### i alter the layout    \n",
    "# #fig['layout']['font'].update({'size': fontsize})\n",
    "# fig['layout']['font']['size'] = fontsize\n",
    "# fig['layout'].update(title='Pairwise comparisons on '+v1_string.replace(\"_\",\" \").replace(\"cite\",\"citation\")+\", \"+test+\" test\")#, xaxis=dict(title=\"Transcript Length (%)\"),  yaxis=dict(title=\"Normalised Mean Coverage\"))\n",
    "\n",
    "# fig['layout'].update(titlefont = dict( size = fontsize+30))   # dict(color = \"rgb(204, 204, 204)\", size = 25))\n",
    "\n",
    "\n",
    "# fig['layout']['margin'].update({'l': 275})   # i add extra space to the margins of the plots\n",
    "# fig['layout']['margin'].update({'b': 200})  \n",
    "# fig['layout']['margin'].update({'t': 150})   \n",
    "\n",
    "\n",
    "# # fig['layout'].update(title='Normalized Mean Coverage', height=600,   \n",
    "# #                         width=800,showlegend=False,font=dict(size=14),     \n",
    "# #                         xaxis=dict(title=\"Transcript Length (%)\"),                          \n",
    "# #                         yaxis=dict(title=\"Normalised Mean Coverage\"))\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='multiplot_comparisons_'+v1_string+\"_\"+test ,image_width=2800, image_height=2800,\n",
    "#               filename='multiplot_test.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### NEWWWW  :  ONLY bottom row and plot separately  \n",
    "### FIGURE 1D  FIGURE 1E\n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    "######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "v1_string =  'cite_count'     #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "    \n",
    "if v1_string ==  'cite_count'  :\n",
    "    colorbar_string = 'Citations'\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "string_references_age = \"\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "string_self_ref =0    #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "#  '0': 'Biology and life sciences'             6,032,537\n",
    "#  '1': 'Computer and information sciences'     1,207,799\n",
    "#  '10': 'Social sciences'                      755,899\n",
    "#  '2': 'Earth sciences'                        533,155\n",
    "#  '3': 'Ecology and environmental sciences'    624,142\n",
    "#  '4': 'Engineering and technology'            382,247 \n",
    "#  '5': 'Medicine and health sciences'          4,535,926   \n",
    "#  '6': 'People and places'                     691,523\n",
    "#  '7': 'Physical sciences'                     2,100,827\n",
    "#  '8': 'Research and analysis methods'         3,871,470\n",
    "#  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    \n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "   \n",
    "    \n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "    N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "    N_all=len(preselection_df3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    \n",
    "    fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    \n",
    "    \n",
    "    fig_colorscale = \"Reds\"\n",
    "    fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "  \n",
    "    if  v1_string ==  'log_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    elif  v1_string ==  'log2_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "elif v1_string =='ref_pub_year':\n",
    "    fig_colorscale = \"Viridis\"\n",
    "    fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "    fig_colorscale = [[0, '#dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "        \n",
    "        \n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "    \n",
    "    lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2       \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "#print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "x1_All = list(preselection_df3[v1_string])\n",
    "\n",
    "\n",
    "list_quantiles_cell=[.25,.5,.75]\n",
    "values_quantiles=list(preselection_df3[v1_string].quantile(list_quantiles_cell))\n",
    "#print (\"avg ALL:\",  preselection_df3[v1_string].mean(), values_quantiles)       \n",
    "# dont print the ALL value because it has references that are repeated in a same paper!!! see one of the early cells instead\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "#lista_sections = [\"I\",\"M\",\"R\",\"D\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "################ i also add the median values for the section across all data in the preselection\n",
    "for string_section in lista_sections:\n",
    "\n",
    "        \n",
    "    if  string_section == \"Introduction\":\n",
    "        section=0\n",
    "    elif  string_section == \"Methods\":\n",
    "        section=1\n",
    "    elif  string_section == \"Results\":\n",
    "        section=2\n",
    "    elif  string_section == \"Discussion\":\n",
    "        section=3\n",
    "\n",
    "\n",
    "    df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "    tupla=values_quantiles + [len(df_select)]\n",
    "    \n",
    "    print (\"avg:\", string_section,  df_select[v1_string].mean(),  \"  STD:\",df_select[v1_string].std(), values_quantiles)\n",
    "    \n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    " \n",
    "    dict_group_quantiles_size[string_section+\" \"]=tupla   #### ojo!!! truco trapero para q siga funcionando pero sin escribir \"ALL PAPER\" en el eje y\n",
    "    dict_group_subset_data[string_section+\" \"]=x1_All    \n",
    " \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "#lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "              #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "lista_bin_names=[\" \"]     #### ojo!!! truco trapero para q siga funcionando pero sin escribir \"ALL PAPER\" en el eje y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "    \n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "    \n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "        \n",
    "        \n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "    \n",
    "   \n",
    "\n",
    "# print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):\n",
    "       # value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        \n",
    "        aux.append(value)\n",
    "    lista_text_z.append(aux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### using a different library\n",
    "path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    " \n",
    "    \n",
    "#     fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors =\n",
    "    \n",
    "    \n",
    "\n",
    "if v1_string =='diff_year_plos_ref':\n",
    "    fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True,\\\n",
    "                                      colorbar=dict(title=colorbar_string, titleside='right',tickvals=[6,7,8,9], ticktext=[\" 6  \",\" 7  \",\" 8  \",\" 9  \"] ),)\n",
    "else:\n",
    "    fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True,\\\n",
    "                                  colorbar=dict(title=colorbar_string, titleside='right',),)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)   \n",
    "###  colorbar=dict(title=colorbar_string, titleside='right', tickvals=[6,7,8,9], ticktext=[\"6  \",\"7  \",\"8  \",\"9  \"] )\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "\n",
    "# fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=48   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -3\n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -20\n",
    "\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "\n",
    "# i add some space between the axis label and the axis ticks\n",
    "fig['layout']['margin']=dict(\n",
    "        #l=200,\n",
    "       # r=50,\n",
    "        b=100,\n",
    "        #t=100,\n",
    "       # pad=4\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = go.Layout(\n",
    "#     title='GitHub commits per day',\n",
    "#     xaxis = dict(ticks='', nticks=36),\n",
    "#     yaxis = dict(ticks='' )\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=500, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "## extra FIGURE where i collapse all sections, and show olnly differences by impact group\n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    "######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "v1_string = 'diff_year_plos_ref'   #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "  \n",
    "string_references_age = \"\"   #co\"#old\"  # young # all   for the selection of what references i include\n",
    "  \n",
    "  \n",
    "  \n",
    "top_space = 150\n",
    "if v1_string ==  'cite_count'  :\n",
    "    colorbar_string = 'Citations'\n",
    "    if string_references_age == \"old\" :\n",
    "        colorbar_string = ''\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "    top_space = 100\n",
    "    text_abc = '(b)'\n",
    "\n",
    "years=[2011] \n",
    "#years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "    \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#list_strings = [1,0]\n",
    "#for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "#list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "#for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "#   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "#  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "#  '0': 'Biology and life sciences'             6,032,537 --\n",
    "#  '1': 'Computer and information sciences'     1,207,799 --\n",
    "#  '10': 'Social sciences'                      755,899 --\n",
    "#  '2': 'Earth sciences'                        533,155 --\n",
    "#  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "#  '4': 'Engineering and technology'            382,247 --\n",
    "#  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "#  '6': 'People and places'                     691,523 --\n",
    "#  '7': 'Physical sciences'                     2,100,827 --\n",
    "#  '8': 'Research and analysis methods'         3,871,470 --\n",
    "#  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "#list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "#for string_journal in list_strings:\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "    N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    \n",
    "    \n",
    "    factor_color_rescale =1#.439    \n",
    "\n",
    "    fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "                           [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "                           [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "                           [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "                           [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "                           [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "                           [0.6*factor_color_rescale, '#53c653'], \\\n",
    "                           [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "                           [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "                           [0.8*factor_color_rescale, '#339933'],\\\n",
    "                           [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "                           [0.9*factor_color_rescale, '#267326'],\\\n",
    "                           [1.*factor_color_rescale, '#267326']]\n",
    "#     ,\\\n",
    "#                            [1.0, '#000000']]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "\n",
    "\n",
    "   # fig_colorscale = \"Reds\"\n",
    "    fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "    if  v1_string ==  'log_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    elif  v1_string ==  'log2_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='ref_pub_year':\n",
    "    fig_colorscale = \"Viridis\"\n",
    "    fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "    \n",
    "    factor_color_rescale =1#.439    \n",
    "\n",
    "    fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "                           [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "                           [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "                           [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "                           [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "                           [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "                           [0.6*factor_color_rescale, '#53c653'], \\\n",
    "                           [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "                           [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "                           [0.8*factor_color_rescale, '#339933'],\\\n",
    "                           [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "                           [0.9*factor_color_rescale, '#267326'],\\\n",
    "                           [1.*factor_color_rescale, '#267326']]\n",
    "#     ,\\\n",
    "#                            [1.0, '#000000']]\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "   # fig_colorscale = [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "\n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2       \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "#print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_titulos_sets = []\n",
    "\n",
    "\n",
    "#lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "lista_sections = [\" \"]\n",
    "\n",
    "cont=0\n",
    "for item in lista_bins_plos_citations:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]\n",
    "\n",
    "\n",
    "    preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "    #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "    x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    string_section = \" \"\n",
    "\n",
    "\n",
    "\n",
    "#     for string_section in lista_sections:\n",
    "\n",
    "\n",
    "#         ##### preselection to include only occurences in a section of the paper\n",
    "#         if  string_section == \"Introduction\":\n",
    "#             section=0\n",
    "#         elif  string_section == \"Methods\":\n",
    "#             section=1\n",
    "#         elif  string_section == \"Results\":\n",
    "#             section=2\n",
    "#         elif  string_section == \"Discussion\":\n",
    "#             section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df_select = preselection_df4   #[preselection_df4['regex_sect_index']== section]   \n",
    "        #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "    x1 = list(df_select[v1_string])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if cont ==0:            \n",
    "        #group=string_section+\" Bottom \"+str(int(100.*list_q[0]))+\"%\"  \n",
    "        group=string_section+\" Bottom\" \n",
    "    elif cont ==1:            \n",
    "        #group=string_section+\" \"+str(int(100.*list_q[0]+1))+\"%-\"+str(int(100.*list_q[-4]))+\"%\"         \n",
    "        group=string_section+\" Typical\"       \n",
    "    elif cont==2:\n",
    "         #group=string_section+\" \"+str(int(100.*list_q[1]+1))+\"%-\"+str(int(100.*list_q[-3]))+\"%\"     \n",
    "         group=string_section+\" Good\"    \n",
    "    elif cont==3: \n",
    "        #group=string_section+\" \"+str(int(100.*list_q[2]+1))+\"%-\"+str(int(100.*list_q[-2]))+\"%\"    \n",
    "        group=string_section+\" High\"\n",
    "    elif cont==4:\n",
    "        #group=string_section+\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"    \n",
    "        group=string_section+\" Top\"\n",
    "\n",
    "\n",
    "    lista_titulos_sets.append(group)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### i get also quantiles for each cell:    \n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "    tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "    dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "    dict_group_subset_data[group]=x1\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ################ i also add the median values for the section across all data in the preselection\n",
    "# for string_section in lista_sections:\n",
    "\n",
    "\n",
    "#     if  string_section == \"Introduction\":\n",
    "#         section=0\n",
    "#     elif  string_section == \"Methods\":\n",
    "#         section=1\n",
    "#     elif  string_section == \"Results\":\n",
    "#         section=2\n",
    "#     elif  string_section == \"Discussion\":\n",
    "#         section=3\n",
    "\n",
    "\n",
    "#     df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "\n",
    "#     list_quantiles_cell=[.25,.5,.75]\n",
    "#     values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "#     tupla=values_quantiles + [len(df_select)]\n",
    "\n",
    "\n",
    "\n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "#lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "              #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "\n",
    "lista_bin_names=[\" Bottom\",\" Typical\",\" Good\",\" High\",\" Top\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "\n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "\n",
    "\n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "\n",
    "\n",
    "\n",
    "# print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):\n",
    "        #value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "\n",
    "\n",
    "\n",
    "        aux.append(value)\n",
    "    lista_text_z.append(aux)\n",
    "# print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### using a different library\n",
    "path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        fig.layout.yaxis.update({'title': ''})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "        fig['layout']['title'] = \"Young references\"\n",
    "    elif string_references_age == \"old\":  \n",
    "        fig.layout.update({'title': 'Old references'})\n",
    "\n",
    "    fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "# fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=top_space,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text=text_abc,\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# old, age >=15 years; Tot # records included: 88209    # number of plos papers: 12895    # unique ref: 61867 \n",
    "# young, age <=2 years;  Tot # records included: 104113    # number of plos papers: 13442    # unique ref: 66644 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tot # records included: 564251    # number of plos papers: 14351    # unique ref: 357866"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  MORE TEXT IN EACH CELL AND FLIPPING COLUMNS-ROWS IN THE HEATMAP ...........\n",
    "#  annotated heatmap plot for median publication year (OR CITATIONS) of the references used in the different sections, and separating by citation category of the plos\n",
    "### NEWWWW  :  removed bottom row \n",
    "\n",
    "## FIGURE 2B  AND FIGURE 3A AND FIGURE 3B\n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    "######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "v1_string = 'cite_count'   #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "  \n",
    "string_references_age = \"young\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "  \n",
    "  \n",
    "  \n",
    "top_space = 150\n",
    "if v1_string ==  'cite_count'  :\n",
    "    colorbar_string = 'Citations'\n",
    "    if string_references_age == \"old\" :\n",
    "        colorbar_string = ''\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "    top_space = 100\n",
    "    text_abc = '(b)'\n",
    "\n",
    "years=[2009] \n",
    "\n",
    "\n",
    "\n",
    "#years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "    \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#list_strings = [1,0]\n",
    "#for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "#list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "#for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "#   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "#  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "#  '0': 'Biology and life sciences'             6,032,537 --\n",
    "#  '1': 'Computer and information sciences'     1,207,799 --\n",
    "#  '10': 'Social sciences'                      755,899 --\n",
    "#  '2': 'Earth sciences'                        533,155 --\n",
    "#  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "#  '4': 'Engineering and technology'            382,247 --\n",
    "#  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "#  '6': 'People and places'                     691,523 --\n",
    "#  '7': 'Physical sciences'                     2,100,827 --\n",
    "#  '8': 'Research and analysis methods'         3,871,470 --\n",
    "#  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "#list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "#for string_journal in list_strings:\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "    N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    \n",
    "    \n",
    "#     factor_color_rescale =.3   \n",
    "\n",
    "#     fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "#                            [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "#                            [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "#                            [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "#                            [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "#                            [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "#                            [0.6*factor_color_rescale, '#53c653'], \\\n",
    "#                            [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "#                            [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "#                            [0.8*factor_color_rescale, '#339933'],\\\n",
    "#                            [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "#                            [0.9*factor_color_rescale, '#267326'],\\\n",
    "#                            [1.0, '#000000']]\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "\n",
    "\n",
    "    fig_colorscale = \"Reds\"\n",
    "    fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "    if  v1_string ==  'log_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    elif  v1_string ==  'log2_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='ref_pub_year':\n",
    "    fig_colorscale = \"Viridis\"\n",
    "    fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "    factor_color_rescale =.6  \n",
    "\n",
    "    fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "                           [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "                           [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "                           [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "                           [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "                           [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "                           [0.6*factor_color_rescale, '#53c653'], \\\n",
    "                           [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "                           [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "                           [0.8*factor_color_rescale, '#339933'],\\\n",
    "                           [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "                           [0.9*factor_color_rescale, '#267326'],\\\n",
    "                          [1.0, '#000000']]\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    #fig_colorscale = [[0, 'dcf0d2'], [1, '205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "\n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2       \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "#print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_titulos_sets = []\n",
    "\n",
    "\n",
    "\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "cont=0\n",
    "for item in lista_bins_plos_citations:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]\n",
    "\n",
    "\n",
    "    preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "    #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "    x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for string_section in lista_sections:\n",
    "\n",
    "\n",
    "        ##### preselection to include only occurences in a section of the paper\n",
    "        if  string_section == \"Introduction\":\n",
    "            section=0\n",
    "        elif  string_section == \"Methods\":\n",
    "            section=1\n",
    "        elif  string_section == \"Results\":\n",
    "            section=2\n",
    "        elif  string_section == \"Discussion\":\n",
    "            section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "        #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "        x1 = list(df_select[v1_string])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if cont ==0:            \n",
    "            #group=string_section+\" Bottom \"+str(int(100.*list_q[0]))+\"%\"  \n",
    "            group=string_section+\" Bottom\" \n",
    "        elif cont ==1:            \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[0]+1))+\"%-\"+str(int(100.*list_q[-4]))+\"%\"         \n",
    "            group=string_section+\" Typical\"       \n",
    "        elif cont==2:\n",
    "             #group=string_section+\" \"+str(int(100.*list_q[1]+1))+\"%-\"+str(int(100.*list_q[-3]))+\"%\"     \n",
    "             group=string_section+\" Good\"    \n",
    "        elif cont==3: \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[2]+1))+\"%-\"+str(int(100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" High\"\n",
    "        elif cont==4:\n",
    "            #group=string_section+\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" Top\"\n",
    "            \n",
    "            \n",
    "        lista_titulos_sets.append(group)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######### i get also quantiles for each cell:    \n",
    "        list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "        values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "        tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "        dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "        dict_group_subset_data[group]=x1\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ i also add the median values for the section across all data in the preselection\n",
    "for string_section in lista_sections:\n",
    "\n",
    "\n",
    "    if  string_section == \"Introduction\":\n",
    "        section=0\n",
    "    elif  string_section == \"Methods\":\n",
    "        section=1\n",
    "    elif  string_section == \"Results\":\n",
    "        section=2\n",
    "    elif  string_section == \"Discussion\":\n",
    "        section=3\n",
    "\n",
    "\n",
    "    df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "\n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "    tupla=values_quantiles + [len(df_select)]\n",
    "\n",
    "\n",
    "\n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "#lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "              #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "\n",
    "lista_bin_names=[\" Bottom\",\" Typical\",\" Good\",\" High\",\" Top\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "\n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "\n",
    "\n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "\n",
    "\n",
    "\n",
    "# print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):\n",
    "        #value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "\n",
    "\n",
    "\n",
    "        aux.append(value)\n",
    "    lista_text_z.append(aux)\n",
    "# print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### using a different library\n",
    "path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        fig.layout.yaxis.update({'title': ''})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "        fig['layout']['title'] = \"Young references\"\n",
    "    elif string_references_age == \"old\":  \n",
    "        fig.layout.update({'title': 'Old references'})\n",
    "\n",
    "    fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "# fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=top_space,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text=text_abc,\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# old, age >=15 years; Tot # records included: 88209    # number of plos papers: 12895    # unique ref: 61867 \n",
    "# young, age <=2 years;  Tot # records included: 104113    # number of plos papers: 13442    # unique ref: 66644 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "######################################################3\n",
    "########################################################\n",
    "\n",
    "\n",
    "##############  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2b, and 3a, 2b)\n",
    "\n",
    "#####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "                 [2,1],[2,2],[2,3],[2,4],\\\n",
    "                 [3,1],[3,2],[3,3],[3,4],\\\n",
    "                 [4,1],[4,2],[4,3],[4,4],\\\n",
    "                 [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "#fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tot_rows = 5\n",
    "tot_cols = 4  \n",
    "    \n",
    "test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "                   'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                   'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                   'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "                   'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "                     'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "                     'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                     'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                     'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# lista_titulos_sets  (from the previous cell)\n",
    "\n",
    "# ['Introduction Bottom',\n",
    "#  'Methods Bottom',\n",
    "#  'Results Bottom',\n",
    "#  'Discussion Bottom',\n",
    "#  'Introduction Typical',\n",
    "#  'Methods Typical',\n",
    "#  'Results Typical',\n",
    "#  'Discussion Typical',\n",
    "#  'Introduction Good',\n",
    "#  'Methods Good',\n",
    "#  'Results Good',\n",
    "#  'Discussion Good',\n",
    "#  'Introduction High',\n",
    "#  'Methods High',\n",
    "#  'Results High',\n",
    "#  'Discussion High',\n",
    "#  'Introduction Top',\n",
    "#  'Methods Top',\n",
    "#  'Results Top',\n",
    "#  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_tot_datos=[]\n",
    "\n",
    "total_cont =0\n",
    "for i in list_keys_macro:\n",
    "    lista_listas=[]\n",
    "    aux_lista=[]\n",
    "    cont=1\n",
    "   \n",
    "    for j in list_keys_heatmap:\n",
    "             \n",
    "        set1 = dict_group_subset_data[i]\n",
    "        set2 = dict_group_subset_data[j]\n",
    "        \n",
    "        if test == \"KS\":\n",
    "            p_value = stats.ks_2samp(set1, set2)[1] \n",
    "        elif test == \"MW\":\n",
    "            p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "        if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "                p_value =0.\n",
    "                \n",
    "        else:\n",
    "            p_value = .5  ### I ONLY CARFE ABOUT WHETGER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "                \n",
    "        if i == j:  # i single out manually the self comparison\n",
    "            p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "        aux_lista.append(p_value)\n",
    "        \n",
    "        cont +=1\n",
    "        \n",
    "        if cont > tot_cols:\n",
    "            lista_listas.append(aux_lista)                  \n",
    "            aux_lista=[]\n",
    "            cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    total_cont +=1\n",
    "    lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "### I create an empty plot\n",
    "fig_macro = None\n",
    "fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "for i in range(len(lista_tot_datos)):\n",
    "    datos = lista_tot_datos[i]\n",
    "    \n",
    "    cont_rows = lista_indeces[i][0]\n",
    "    cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "    trace1 = go.Heatmap(z=datos,\n",
    "                       x=lista_sections,\n",
    "                       y=lista_bin_names,                        \n",
    "                       colorscale = fig_colorscale,\n",
    "                       showscale=False,)\n",
    "                        #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # i add each plot\n",
    "    fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "fontsize=32 \n",
    "fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "# fig_macro['layout']['margin'].update({'b': 150})  \n",
    "# fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# i use the axis title of the small plots as values of the axis of the macro plot\n",
    "fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text='(b)',\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "   \n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "              filename='../plots/multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### figures 2b, 3a nd 3b BUT FOR A DIFFERENT CATEGORIZATION OF PAPERS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    "######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "v1_string = 'cite_count'   #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "  \n",
    "string_references_age = \"young\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "  \n",
    "  \n",
    "  \n",
    "top_space = 150\n",
    "if v1_string ==  'cite_count'  :\n",
    "    colorbar_string = 'Citations'\n",
    "    if string_references_age == \"old\" :\n",
    "        colorbar_string = ''\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "    top_space = 100\n",
    "    text_abc = '(b)'\n",
    "\n",
    "years=[2009] \n",
    "\n",
    "\n",
    "\n",
    "###### for the percentile sections for number of citations of the PLOS papers\n",
    "\n",
    "#list_q=[0.25, 0.75, .9, .994,1]    #  # for the percentile sections for number of citations of the PLOS papers\n",
    "  \n",
    "\n",
    "\n",
    "#list_q=[0.3,0.6,.9,.99,1]    \n",
    "\n",
    "\n",
    "#list_q=[0.2,0.4,.6,.8,1]    \n",
    "\n",
    "\n",
    "list_q=[0.25,0.5,.75,.9,1]    \n",
    "\n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "    \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#list_strings = [1,0]\n",
    "#for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "#list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "#for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "#   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "#  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "#  '0': 'Biology and life sciences'             6,032,537 --\n",
    "#  '1': 'Computer and information sciences'     1,207,799 --\n",
    "#  '10': 'Social sciences'                      755,899 --\n",
    "#  '2': 'Earth sciences'                        533,155 --\n",
    "#  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "#  '4': 'Engineering and technology'            382,247 --\n",
    "#  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "#  '6': 'People and places'                     691,523 --\n",
    "#  '7': 'Physical sciences'                     2,100,827 --\n",
    "#  '8': 'Research and analysis methods'         3,871,470 --\n",
    "#  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "#list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "#for string_journal in list_strings:\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "    N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    \n",
    "    \n",
    "#     factor_color_rescale =.3   \n",
    "\n",
    "#     fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "#                            [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "#                            [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "#                            [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "#                            [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "#                            [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "#                            [0.6*factor_color_rescale, '#53c653'], \\\n",
    "#                            [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "#                            [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "#                            [0.8*factor_color_rescale, '#339933'],\\\n",
    "#                            [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "#                            [0.9*factor_color_rescale, '#267326'],\\\n",
    "#                            [1.0, '#000000']]\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "\n",
    "\n",
    "    fig_colorscale = \"Reds\"\n",
    "    fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "    if  v1_string ==  'log_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    elif  v1_string ==  'log2_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='ref_pub_year':\n",
    "    fig_colorscale = \"Viridis\"\n",
    "    fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "    factor_color_rescale =.6  \n",
    "\n",
    "    fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "                           [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "                           [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "                           [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "                           [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "                           [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "                           [0.6*factor_color_rescale, '#53c653'], \\\n",
    "                           [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "                           [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "                           [0.8*factor_color_rescale, '#339933'],\\\n",
    "                           [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "                           [0.9*factor_color_rescale, '#267326'],\\\n",
    "                          [1.0, '#000000']]\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    #fig_colorscale = [[0, 'dcf0d2'], [1, '205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "\n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2       \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "#print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_titulos_sets = []\n",
    "\n",
    "\n",
    "\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "cont=0\n",
    "for item in lista_bins_plos_citations:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]\n",
    "\n",
    "\n",
    "    preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "    #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "    x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for string_section in lista_sections:\n",
    "\n",
    "\n",
    "        ##### preselection to include only occurences in a section of the paper\n",
    "        if  string_section == \"Introduction\":\n",
    "            section=0\n",
    "        elif  string_section == \"Methods\":\n",
    "            section=1\n",
    "        elif  string_section == \"Results\":\n",
    "            section=2\n",
    "        elif  string_section == \"Discussion\":\n",
    "            section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "        #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "        x1 = list(df_select[v1_string])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if cont ==0:            \n",
    "            #group=string_section+\" Bottom \"+str(int(100.*list_q[0]))+\"%\"  \n",
    "            group=string_section+\" Bottom\" \n",
    "        elif cont ==1:            \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[0]+1))+\"%-\"+str(int(100.*list_q[-4]))+\"%\"         \n",
    "            group=string_section+\" Typical\"       \n",
    "        elif cont==2:\n",
    "             #group=string_section+\" \"+str(int(100.*list_q[1]+1))+\"%-\"+str(int(100.*list_q[-3]))+\"%\"     \n",
    "             group=string_section+\" Good\"    \n",
    "        elif cont==3: \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[2]+1))+\"%-\"+str(int(100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" High\"\n",
    "        elif cont==4:\n",
    "            #group=string_section+\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" Top\"\n",
    "            \n",
    "            \n",
    "        lista_titulos_sets.append(group)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######### i get also quantiles for each cell:    \n",
    "        list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "        values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "        tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "        dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "        dict_group_subset_data[group]=x1\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ i also add the median values for the section across all data in the preselection\n",
    "for string_section in lista_sections:\n",
    "\n",
    "\n",
    "    if  string_section == \"Introduction\":\n",
    "        section=0\n",
    "    elif  string_section == \"Methods\":\n",
    "        section=1\n",
    "    elif  string_section == \"Results\":\n",
    "        section=2\n",
    "    elif  string_section == \"Discussion\":\n",
    "        section=3\n",
    "\n",
    "\n",
    "    df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "\n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "    tupla=values_quantiles + [len(df_select)]\n",
    "\n",
    "\n",
    "\n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "#lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "              #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "\n",
    "lista_bin_names=[\" Bottom\",\" Typical\",\" Good\",\" High\",\" Top\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "\n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "\n",
    "\n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "\n",
    "\n",
    "\n",
    "# print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):\n",
    "        #value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "\n",
    "\n",
    "\n",
    "        aux.append(value)\n",
    "    lista_text_z.append(aux)\n",
    "# print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### using a different library\n",
    "path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        fig.layout.yaxis.update({'title': ''})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "        fig['layout']['title'] = \"Young references\"\n",
    "    elif string_references_age == \"old\":  \n",
    "        fig.layout.update({'title': 'Old references'})\n",
    "\n",
    "    fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "# fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=top_space,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text=text_abc,\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# old, age >=15 years; Tot # records included: 88209    # number of plos papers: 12895    # unique ref: 61867 \n",
    "# young, age <=2 years;  Tot # records included: 104113    # number of plos papers: 13442    # unique ref: 66644 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "######################################################3\n",
    "########################################################\n",
    "\n",
    "\n",
    "##############  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2b, and 3a, 2b)\n",
    "\n",
    "#####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "                 [2,1],[2,2],[2,3],[2,4],\\\n",
    "                 [3,1],[3,2],[3,3],[3,4],\\\n",
    "                 [4,1],[4,2],[4,3],[4,4],\\\n",
    "                 [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "#fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tot_rows = 5\n",
    "tot_cols = 4  \n",
    "    \n",
    "test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "                   'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                   'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                   'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "                   'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "                     'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "                     'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                     'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                     'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# lista_titulos_sets  (from the previous cell)\n",
    "\n",
    "# ['Introduction Bottom',\n",
    "#  'Methods Bottom',\n",
    "#  'Results Bottom',\n",
    "#  'Discussion Bottom',\n",
    "#  'Introduction Typical',\n",
    "#  'Methods Typical',\n",
    "#  'Results Typical',\n",
    "#  'Discussion Typical',\n",
    "#  'Introduction Good',\n",
    "#  'Methods Good',\n",
    "#  'Results Good',\n",
    "#  'Discussion Good',\n",
    "#  'Introduction High',\n",
    "#  'Methods High',\n",
    "#  'Results High',\n",
    "#  'Discussion High',\n",
    "#  'Introduction Top',\n",
    "#  'Methods Top',\n",
    "#  'Results Top',\n",
    "#  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_tot_datos=[]\n",
    "\n",
    "total_cont =0\n",
    "for i in list_keys_macro:\n",
    "    lista_listas=[]\n",
    "    aux_lista=[]\n",
    "    cont=1\n",
    "   \n",
    "    for j in list_keys_heatmap:\n",
    "             \n",
    "        set1 = dict_group_subset_data[i]\n",
    "        set2 = dict_group_subset_data[j]\n",
    "        \n",
    "        if test == \"KS\":\n",
    "            p_value = stats.ks_2samp(set1, set2)[1] \n",
    "        elif test == \"MW\":\n",
    "            p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "        if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "                p_value =0.\n",
    "                \n",
    "        else:\n",
    "            p_value = .5  ### I ONLY CARFE ABOUT WHETGER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "                \n",
    "        if i == j:  # i single out manually the self comparison\n",
    "            p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "        aux_lista.append(p_value)\n",
    "        \n",
    "        cont +=1\n",
    "        \n",
    "        if cont > tot_cols:\n",
    "            lista_listas.append(aux_lista)                  \n",
    "            aux_lista=[]\n",
    "            cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    total_cont +=1\n",
    "    lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "### I create an empty plot\n",
    "fig_macro = None\n",
    "fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "for i in range(len(lista_tot_datos)):\n",
    "    datos = lista_tot_datos[i]\n",
    "    \n",
    "    cont_rows = lista_indeces[i][0]\n",
    "    cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "    trace1 = go.Heatmap(z=datos,\n",
    "                       x=lista_sections,\n",
    "                       y=lista_bin_names,                        \n",
    "                       colorscale = fig_colorscale,\n",
    "                       showscale=False,)\n",
    "                        #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # i add each plot\n",
    "    fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "fontsize=32 \n",
    "fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "# fig_macro['layout']['margin'].update({'b': 150})  \n",
    "# fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# i use the axis title of the small plots as values of the axis of the macro plot\n",
    "fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text='(b)',\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "   \n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "              filename='../plots/multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Figure 3b-new:   NUMBER OF EARLY CITATIONS RECEIVED BY YOUNG REFERENCES BY THE YEAR OF THE PUBLICATION OF THE PLOS \n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    "######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "      \n",
    "  \n",
    "string_references_age = \"young\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "\n",
    "\n",
    "v1_string =  'cite_count'\n",
    "  \n",
    "  \n",
    "top_space = 150\n",
    "if v1_string ==  'cite_count'  or   'num_cit_young_ref_by' in v1_string:\n",
    "    colorbar_string = 'Citations'\n",
    "    if string_references_age == \"old\" :\n",
    "        colorbar_string = ''\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "    top_space = 100\n",
    "    text_abc = '(b)'\n",
    "\n",
    "    \n",
    "    \n",
    "years=[2009]  #AVAILABLE YEARS FOR NUMBER OF EARLY CITATIONS: 2009 TO 2013 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "v1_string = 'num_cit_young_ref_by'+str(years[-1])  #    instead of 'cite_count'   #    \n",
    " \n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "    \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#list_strings = [1,0]\n",
    "#for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "#list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "#for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "#   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "#  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "#  '0': 'Biology and life sciences'             6,032,537 --\n",
    "#  '1': 'Computer and information sciences'     1,207,799 --\n",
    "#  '10': 'Social sciences'                      755,899 --\n",
    "#  '2': 'Earth sciences'                        533,155 --\n",
    "#  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "#  '4': 'Engineering and technology'            382,247 --\n",
    "#  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "#  '6': 'People and places'                     691,523 --\n",
    "#  '7': 'Physical sciences'                     2,100,827 --\n",
    "#  '8': 'Research and analysis methods'         3,871,470 --\n",
    "#  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "#list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "#for string_journal in list_strings:\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or    'num_cit_young_ref_by' in v1_string       or   v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "    N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    \n",
    "    \n",
    "#     factor_color_rescale =.3   \n",
    "\n",
    "#     fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "#                            [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "#                            [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "#                            [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "#                            [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "#                            [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "#                            [0.6*factor_color_rescale, '#53c653'], \\\n",
    "#                            [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "#                            [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "#                            [0.8*factor_color_rescale, '#339933'],\\\n",
    "#                            [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "#                            [0.9*factor_color_rescale, '#267326'],\\\n",
    "#                            [1.0, '#000000']]\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "\n",
    "\n",
    "    fig_colorscale = \"Reds\"\n",
    "    fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "    if  v1_string ==  'log_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    elif  v1_string ==  'log2_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='ref_pub_year':\n",
    "    fig_colorscale = \"Viridis\"\n",
    "    fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "    factor_color_rescale =.6  \n",
    "\n",
    "    fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "                           [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "                           [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "                           [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "                           [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "                           [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "                           [0.6*factor_color_rescale, '#53c653'], \\\n",
    "                           [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "                           [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "                           [0.8*factor_color_rescale, '#339933'],\\\n",
    "                           [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "                           [0.9*factor_color_rescale, '#267326'],\\\n",
    "                          [1.0, '#000000']]\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    #fig_colorscale = [[0, 'dcf0d2'], [1, '205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "    print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "\n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2       \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "#print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_titulos_sets = []\n",
    "\n",
    "\n",
    "\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "cont=0\n",
    "for item in lista_bins_plos_citations:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]\n",
    "\n",
    "\n",
    "    preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "    #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "    x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for string_section in lista_sections:\n",
    "\n",
    "\n",
    "        ##### preselection to include only occurences in a section of the paper\n",
    "        if  string_section == \"Introduction\":\n",
    "            section=0\n",
    "        elif  string_section == \"Methods\":\n",
    "            section=1\n",
    "        elif  string_section == \"Results\":\n",
    "            section=2\n",
    "        elif  string_section == \"Discussion\":\n",
    "            section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "        #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "        x1 = list(df_select[v1_string])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if cont ==0:            \n",
    "            #group=string_section+\" Bottom \"+str(int(100.*list_q[0]))+\"%\"  \n",
    "            group=string_section+\" Bottom\" \n",
    "        elif cont ==1:            \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[0]+1))+\"%-\"+str(int(100.*list_q[-4]))+\"%\"         \n",
    "            group=string_section+\" Typical\"       \n",
    "        elif cont==2:\n",
    "             #group=string_section+\" \"+str(int(100.*list_q[1]+1))+\"%-\"+str(int(100.*list_q[-3]))+\"%\"     \n",
    "             group=string_section+\" Good\"    \n",
    "        elif cont==3: \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[2]+1))+\"%-\"+str(int(100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" High\"\n",
    "        elif cont==4:\n",
    "            #group=string_section+\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" Top\"\n",
    "            \n",
    "            \n",
    "        lista_titulos_sets.append(group)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ######### i get also quantiles for each cell:    \n",
    "        list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "        values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "        tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "        dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "        dict_group_subset_data[group]=x1\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ i also add the median values for the section across all data in the preselection\n",
    "for string_section in lista_sections:\n",
    "\n",
    "\n",
    "    if  string_section == \"Introduction\":\n",
    "        section=0\n",
    "    elif  string_section == \"Methods\":\n",
    "        section=1\n",
    "    elif  string_section == \"Results\":\n",
    "        section=2\n",
    "    elif  string_section == \"Discussion\":\n",
    "        section=3\n",
    "\n",
    "\n",
    "    df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "\n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "    tupla=values_quantiles + [len(df_select)]\n",
    "\n",
    "\n",
    "\n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "#lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "              #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "\n",
    "lista_bin_names=[\" Bottom\",\" Typical\",\" Good\",\" High\",\" Top\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "\n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "\n",
    "\n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "\n",
    "\n",
    "\n",
    "# print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):\n",
    "        #value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "\n",
    "\n",
    "\n",
    "        aux.append(value)\n",
    "    lista_text_z.append(aux)\n",
    "# print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### using a different library\n",
    "path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        fig.layout.yaxis.update({'title': ''})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "if 'num_cit_young_ref_by' in v1_string:\n",
    "    if string_references_age == \"young\":  \n",
    "        #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "        fig['layout']['title'] = \"Early citations young references\"\n",
    "    \n",
    "\n",
    "    fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "# fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=top_space,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text=text_abc,\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# old, age >=15 years; Tot # records included: 88209    # number of plos papers: 12895    # unique ref: 61867 \n",
    "# young, age <=2 years;  Tot # records included: 104113    # number of plos papers: 13442    # unique ref: 66644 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "######################################################3\n",
    "########################################################\n",
    "\n",
    "\n",
    "##############  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2b, and 3a, 2b)\n",
    "\n",
    "#####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "                 [2,1],[2,2],[2,3],[2,4],\\\n",
    "                 [3,1],[3,2],[3,3],[3,4],\\\n",
    "                 [4,1],[4,2],[4,3],[4,4],\\\n",
    "                 [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "#fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tot_rows = 5\n",
    "tot_cols = 4  \n",
    "    \n",
    "test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "                   'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                   'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                   'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "                   'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "                     'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "                     'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                     'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                     'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# lista_titulos_sets  (from the previous cell)\n",
    "\n",
    "# ['Introduction Bottom',\n",
    "#  'Methods Bottom',\n",
    "#  'Results Bottom',\n",
    "#  'Discussion Bottom',\n",
    "#  'Introduction Typical',\n",
    "#  'Methods Typical',\n",
    "#  'Results Typical',\n",
    "#  'Discussion Typical',\n",
    "#  'Introduction Good',\n",
    "#  'Methods Good',\n",
    "#  'Results Good',\n",
    "#  'Discussion Good',\n",
    "#  'Introduction High',\n",
    "#  'Methods High',\n",
    "#  'Results High',\n",
    "#  'Discussion High',\n",
    "#  'Introduction Top',\n",
    "#  'Methods Top',\n",
    "#  'Results Top',\n",
    "#  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_tot_datos=[]\n",
    "\n",
    "total_cont =0\n",
    "for i in list_keys_macro:\n",
    "    lista_listas=[]\n",
    "    aux_lista=[]\n",
    "    cont=1\n",
    "   \n",
    "    for j in list_keys_heatmap:\n",
    "             \n",
    "        set1 = dict_group_subset_data[i]\n",
    "        set2 = dict_group_subset_data[j]\n",
    "        \n",
    "        if test == \"KS\":\n",
    "            p_value = stats.ks_2samp(set1, set2)[1] \n",
    "        elif test == \"MW\":\n",
    "            p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "        if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "                p_value =0.\n",
    "                \n",
    "        else:\n",
    "            p_value = .5  ### I ONLY CARFE ABOUT WHETGER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "                \n",
    "        if i == j:  # i single out manually the self comparison\n",
    "            p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "        aux_lista.append(p_value)\n",
    "        \n",
    "        cont +=1\n",
    "        \n",
    "        if cont > tot_cols:\n",
    "            lista_listas.append(aux_lista)                  \n",
    "            aux_lista=[]\n",
    "            cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    total_cont +=1\n",
    "    lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "### I create an empty plot\n",
    "fig_macro = None\n",
    "fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "for i in range(len(lista_tot_datos)):\n",
    "    datos = lista_tot_datos[i]\n",
    "    \n",
    "    cont_rows = lista_indeces[i][0]\n",
    "    cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "    trace1 = go.Heatmap(z=datos,\n",
    "                       x=lista_sections,\n",
    "                       y=lista_bin_names,                        \n",
    "                       colorscale = fig_colorscale,\n",
    "                       showscale=False,)\n",
    "                        #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # i add each plot\n",
    "    fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "fontsize=32 \n",
    "fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "# fig_macro['layout']['margin'].update({'b': 150})  \n",
    "# fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# i use the axis title of the small plots as values of the axis of the macro plot\n",
    "fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text='(b)',\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "   \n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "              filename='../plots/multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)\n",
    "\n",
    "\n",
    "# ['categ_codes',\n",
    "#  'cite_count',\n",
    "#  'diff_year_plos_ref',\n",
    "#  'isolated_citation',\n",
    "#  'log2_num_cit_paper',\n",
    "#  'log2_num_cit_ref',\n",
    "#  'num_cit_young_ref_by2007after8',\n",
    "#  'num_cit_young_ref_by2008after8',\n",
    "#  'num_cit_young_ref_by2009',\n",
    "#  'num_cit_young_ref_by2009after8',\n",
    "#  'num_cit_young_ref_by2010',\n",
    "#  'num_cit_young_ref_by2010after8',\n",
    "#  'num_cit_young_ref_by2011',\n",
    "#  'num_cit_young_ref_by2012',\n",
    "#  'num_cit_young_ref_by2013',\n",
    "#  'number_authors',\n",
    "#  'occurence',\n",
    "#  'paper_UT',\n",
    "#  'paper_cite_count',\n",
    "#  'plos_article_type',\n",
    "#  'plos_field',\n",
    "#  'plos_j1',\n",
    "#  'plos_pub_year',\n",
    "#  'ref_field',\n",
    "#  'ref_j1',\n",
    "#  'ref_pub_year',\n",
    "#  'reference_UT',\n",
    "#  'reference_rank',\n",
    "#  'regex_sect_index',\n",
    "#  'rel_loc_in_sect',\n",
    "#  'sect_char_pos',\n",
    "#  'sect_char_total',\n",
    "#  'self_citation',\n",
    "#  'total_refs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NUMBER OF CITATIONS YOUNG REFERENCES AFTER 8 YEARS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_group_subset_data={}\n",
    "dict_group_quantiles_size={}\n",
    "\n",
    "######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "      \n",
    "  \n",
    "string_references_age = \"young\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "\n",
    "    \n",
    "years=[2007]  #AVAILABLE YEARS FOR NUMBER OF CITATIONS AFTER 8 YEARS:  2007 2008 2009 2010\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "    \n",
    "      \n",
    "v1_string = 'num_cit_young_ref_by'+str(years[-1])+'after8'  \n",
    "     \n",
    "    \n",
    "    \n",
    "  \n",
    "\n",
    "top_space = 150\n",
    "if v1_string ==  'cite_count'  or   'num_cit_young_ref_by' in v1_string :\n",
    "    colorbar_string = 'Citations'\n",
    "    if string_references_age == \"old\" :\n",
    "        colorbar_string = ''\n",
    "else:\n",
    "    colorbar_string = 'Age [yr]'\n",
    "    top_space = 100\n",
    "    text_abc = '(b)'   \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#list_strings = [1,0]\n",
    "#for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "#list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "#for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "#   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "#  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "#  '0': 'Biology and life sciences'             6,032,537 --\n",
    "#  '1': 'Computer and information sciences'     1,207,799 --\n",
    "#  '10': 'Social sciences'                      755,899 --\n",
    "#  '2': 'Earth sciences'                        533,155 --\n",
    "#  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "#  '4': 'Engineering and technology'            382,247 --\n",
    "#  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "#  '6': 'People and places'                     691,523 --\n",
    "#  '7': 'Physical sciences'                     2,100,827 --\n",
    "#  '8': 'Research and analysis methods'         3,871,470 --\n",
    "#  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "#list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "#for string_journal in list_strings:\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "    preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    if string_self_ref ==0:\n",
    "        string_self_ref = \", no self-cit\"\n",
    "    elif string_self_ref ==1:\n",
    "        string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "    if string_isolated_ref ==0:\n",
    "        string_isolated_ref = \", group ref\"\n",
    "    elif string_isolated_ref ==1:\n",
    "        string_isolated_ref = \", isolated ref\"\n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count'  or    'num_cit_young_ref_by' in v1_string       or   v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "    string_age_selection=''\n",
    "\n",
    "    ##### preselection only young/old references:        \n",
    "    if string_references_age == \"young\":\n",
    "        time_window = 1\n",
    "        string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "    elif string_references_age == \"old\":\n",
    "        time_window = 10\n",
    "        string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "        print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "    else:\n",
    "        string_age_selection=\"young&old\"       \n",
    "        print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "    N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "    \n",
    "    \n",
    "#     factor_color_rescale =.3   \n",
    "\n",
    "#     fig_colorscale=[[0.0*factor_color_rescale, '#ffffff'],\\\n",
    "#                            [0.1*factor_color_rescale, '#d9f2d9'],\\\n",
    "#                            [0.2*factor_color_rescale, '#c6ecc6'],\\\n",
    "#                            [0.3*factor_color_rescale, '#b3e6b3'],\\\n",
    "#                            [0.4*factor_color_rescale, '#8cd98c'], \\\n",
    "#                            [0.5*factor_color_rescale, '#66cc66'], \\\n",
    "#                            [0.6*factor_color_rescale, '#53c653'], \\\n",
    "#                            [0.7*factor_color_rescale, '#40bf40'],\\\n",
    "#                            [0.75*factor_color_rescale, '#39ac39'],\\\n",
    "#                            [0.8*factor_color_rescale, '#339933'],\\\n",
    "#                            [0.85*factor_color_rescale, '#2d862d'],\\\n",
    "#                            [0.9*factor_color_rescale, '#267326'],\\\n",
    "#                            [1.0, '#000000']]\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "\n",
    "\n",
    "    fig_colorscale = \"Reds\"\n",
    "    fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "    if  v1_string ==  'log_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "    elif  v1_string ==  'log2_num_cit_ref' :\n",
    "        fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "\n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "### i modify the bins to separete the zero-one\n",
    "# lista_bins_plos_citations[0][0]=2       \n",
    "# lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "#print (lista_bins_plos_citations)\n",
    "\n",
    "################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_titulos_sets = []\n",
    "\n",
    "\n",
    "\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "cont=0\n",
    "for item in lista_bins_plos_citations:\n",
    "\n",
    "    minimo = item[0]\n",
    "    maximo = item[1]\n",
    "\n",
    "\n",
    "    preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "    #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "    x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for string_section in lista_sections:\n",
    "\n",
    "\n",
    "        ##### preselection to include only occurences in a section of the paper\n",
    "        if  string_section == \"Introduction\":\n",
    "            section=0\n",
    "        elif  string_section == \"Methods\":\n",
    "            section=1\n",
    "        elif  string_section == \"Results\":\n",
    "            section=2\n",
    "        elif  string_section == \"Discussion\":\n",
    "            section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "        #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "        x1 = list(df_select[v1_string])       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if cont ==0:            \n",
    "            #group=string_section+\" Bottom \"+str(int(100.*list_q[0]))+\"%\"  \n",
    "            group=string_section+\" Bottom\" \n",
    "        elif cont ==1:            \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[0]+1))+\"%-\"+str(int(100.*list_q[-4]))+\"%\"         \n",
    "            group=string_section+\" Typical\"       \n",
    "        elif cont==2:\n",
    "             #group=string_section+\" \"+str(int(100.*list_q[1]+1))+\"%-\"+str(int(100.*list_q[-3]))+\"%\"     \n",
    "             group=string_section+\" Good\"    \n",
    "        elif cont==3: \n",
    "            #group=string_section+\" \"+str(int(100.*list_q[2]+1))+\"%-\"+str(int(100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" High\"\n",
    "        elif cont==4:\n",
    "            #group=string_section+\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"    \n",
    "            group=string_section+\" Top\"\n",
    "            \n",
    "            \n",
    "        lista_titulos_sets.append(group)    \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         if group== \"Results Top\" :\n",
    "#             lista_raros = df_select[v1_string]\n",
    "#             print (len(lista_raros),\"  \",sorted(lista_raros))\n",
    "#             df_raros = df_select\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        ######### i get also quantiles for each cell:    \n",
    "        list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "        values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "        tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "        dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "        dict_group_subset_data[group]=x1\n",
    "\n",
    "\n",
    "\n",
    "    cont +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################ i also add the median values for the section across all data in the preselection\n",
    "for string_section in lista_sections:\n",
    "\n",
    "\n",
    "    if  string_section == \"Introduction\":\n",
    "        section=0\n",
    "    elif  string_section == \"Methods\":\n",
    "        section=1\n",
    "    elif  string_section == \"Results\":\n",
    "        section=2\n",
    "    elif  string_section == \"Discussion\":\n",
    "        section=3\n",
    "\n",
    "\n",
    "    df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "\n",
    "    list_quantiles_cell=[.25,.5,.75]\n",
    "    values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "    tupla=values_quantiles + [len(df_select)]\n",
    "\n",
    "\n",
    "\n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "lista_y=lista_sections\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "#lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "              #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "\n",
    "lista_bin_names=[\" Bottom\",\" Typical\",\" Good\",\" High\",\" Top\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "lista_x=lista_bin_names\n",
    "\n",
    "lista_z25=[]\n",
    "lista_z50=[]\n",
    "lista_z75=[]\n",
    "lista_z_sizes=[]\n",
    "\n",
    "for x_value in lista_x:    \n",
    "    aux_lista25=[]\n",
    "    aux_lista50=[]\n",
    "    aux_lista75=[]\n",
    "    aux_lista_sizes=[]\n",
    "\n",
    "    for y_value in lista_y:       \n",
    "\n",
    "        llave=y_value+x_value\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][0])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][0]\n",
    "        aux_lista25.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][1])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][1]\n",
    "        aux_lista50.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        try:\n",
    "            value=int(dict_group_quantiles_size[llave][2])\n",
    "        except:  # if it is a nan:\n",
    "            value=dict_group_quantiles_size[llave][2]\n",
    "        aux_lista75.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        value_size=dict_group_quantiles_size[llave][3]\n",
    "        aux_lista_sizes.append(value_size)\n",
    "\n",
    "\n",
    "        #print (y_value,\" \",x_value, value, value_size)\n",
    "    lista_z25.append(aux_lista25)\n",
    "    lista_z50.append(aux_lista50)\n",
    "    lista_z75.append(aux_lista75)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_z_sizes.append(aux_lista_sizes)\n",
    "\n",
    "\n",
    "\n",
    "# print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_text_z=[]\n",
    "for i in range(len(lista_z_sizes)):\n",
    "    aux=[]\n",
    "    for j in range(len(lista_z_sizes[0])):\n",
    "        #value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "\n",
    "\n",
    "\n",
    "        aux.append(value)\n",
    "    lista_text_z.append(aux)\n",
    "# print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### using a different library\n",
    "path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "if v1_string ==  'cite_count'  :\n",
    "    if string_references_age == \"young\":  \n",
    "        fig.layout.yaxis.update({'title': ''})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "if 'num_cit_young_ref_by' in v1_string:\n",
    "    if string_references_age == \"young\":  \n",
    "        #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "        fig['layout']['title'] = \"\"#Citations young references after 8 years\"\n",
    "    \n",
    "\n",
    "    fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=top_space,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text=text_abc,\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# old, age >=15 years; Tot # records included: 88209    # number of plos papers: 12895    # unique ref: 61867 \n",
    "# young, age <=2 years;  Tot # records included: 104113    # number of plos papers: 13442    # unique ref: 66644 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################################\n",
    "######################################################3\n",
    "########################################################\n",
    "\n",
    "\n",
    "##############  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2b, and 3a, 2b)\n",
    "\n",
    "#####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "                 [2,1],[2,2],[2,3],[2,4],\\\n",
    "                 [3,1],[3,2],[3,3],[3,4],\\\n",
    "                 [4,1],[4,2],[4,3],[4,4],\\\n",
    "                 [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "#fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tot_rows = 5\n",
    "tot_cols = 4  \n",
    "    \n",
    "test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "                   'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                   'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                   'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "                   'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "                     'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "                     'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                     'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                     'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# lista_titulos_sets  (from the previous cell)\n",
    "\n",
    "# ['Introduction Bottom',\n",
    "#  'Methods Bottom',\n",
    "#  'Results Bottom',\n",
    "#  'Discussion Bottom',\n",
    "#  'Introduction Typical',\n",
    "#  'Methods Typical',\n",
    "#  'Results Typical',\n",
    "#  'Discussion Typical',\n",
    "#  'Introduction Good',\n",
    "#  'Methods Good',\n",
    "#  'Results Good',\n",
    "#  'Discussion Good',\n",
    "#  'Introduction High',\n",
    "#  'Methods High',\n",
    "#  'Results High',\n",
    "#  'Discussion High',\n",
    "#  'Introduction Top',\n",
    "#  'Methods Top',\n",
    "#  'Results Top',\n",
    "#  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_tot_datos=[]\n",
    "\n",
    "total_cont =0\n",
    "for i in list_keys_macro:\n",
    "    lista_listas=[]\n",
    "    aux_lista=[]\n",
    "    cont=1\n",
    "   \n",
    "    for j in list_keys_heatmap:\n",
    "             \n",
    "        set1 = dict_group_subset_data[i]\n",
    "        set2 = dict_group_subset_data[j]\n",
    "        \n",
    "        if test == \"KS\":\n",
    "            p_value = stats.ks_2samp(set1, set2)[1] \n",
    "        elif test == \"MW\":\n",
    "            p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "        if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "                p_value =0.\n",
    "                \n",
    "        else:\n",
    "            p_value = .5  ### I ONLY CARFE ABOUT WHETGER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "                \n",
    "        if i == j:  # i single out manually the self comparison\n",
    "            p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "        aux_lista.append(p_value)\n",
    "        \n",
    "        cont +=1\n",
    "        \n",
    "        if cont > tot_cols:\n",
    "            lista_listas.append(aux_lista)                  \n",
    "            aux_lista=[]\n",
    "            cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    total_cont +=1\n",
    "    lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "### I create an empty plot\n",
    "fig_macro = None\n",
    "fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "for i in range(len(lista_tot_datos)):\n",
    "    datos = lista_tot_datos[i]\n",
    "    \n",
    "    cont_rows = lista_indeces[i][0]\n",
    "    cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "    trace1 = go.Heatmap(z=datos,\n",
    "                       x=lista_sections,\n",
    "                       y=lista_bin_names,                        \n",
    "                       colorscale = fig_colorscale,\n",
    "                       showscale=False,)\n",
    "                        #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # i add each plot\n",
    "    fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "fontsize=32 \n",
    "fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "# fig_macro['layout']['margin'].update({'b': 150})  \n",
    "# fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# i use the axis title of the small plots as values of the axis of the macro plot\n",
    "fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  annotations=[\n",
    "#         dict(\n",
    "#             x=-.05,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text='(b)',\n",
    "#             showarrow=False,\n",
    "#             font=dict(               \n",
    "#                 size=80,  \n",
    "#                 color='black'\n",
    "#             ),\n",
    "           \n",
    "#         )\n",
    "#     ],\n",
    "   \n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "              filename='../plots/multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raros[['reference_UT','ref_pub_year','paper_UT','plos_pub_year','num_cit_young_ref_by2008after8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##############  i plot a simple heatmap\n",
    "\n",
    "# z = [[.1, .3, .5, .7],   # bottom 30 percentile   [intro, methods, results, discussion]\n",
    "#      [1, .8, .6, .4],    # 31-60 percentile\n",
    "#      [0.9, .58, .6, .14],  # 61-90 percentile\n",
    "#      [.6, .4, .2, .0],    # 61-99 percentile\n",
    "#      [.9, .7, .5, .3]]  # top 1 percentile\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z, colorscale='Viridis')\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_annotated_heatmap_color' ,image_width=2000, image_height=500, filename='testing_annotated_heatmap_color.html', validate=True)\n",
    "lista_bins_plos_citations, lista_bin_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NEW FIGURE 2A, HEATMAP FOR NUMBER OF REFERENCES\n",
    "\n",
    "       \n",
    "\n",
    "lista_bin_names = [\"Bottom\", \"Typical\", \"Good\", \"High\", \"Top\"]\n",
    "lista_sections = [\"Introduction\", \"Methods\", \"Results\", \"Discussion\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    \n",
    "    \n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "quantiles=sorted(list(df_records_one_ref_per_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "\n",
    "dict_group_subset_data = {}\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "    \n",
    "    lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations, df_records_one_ref_per_plos.shape)\n",
    "\n",
    "\n",
    "lista_set_names = []\n",
    "\n",
    "lista_text_z = []\n",
    "list_of_lists =[]\n",
    "cont_citation_groups=0\n",
    "for pair_values in lista_bins_plos_citations:    # needs to correspond to    [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]   \n",
    "    #print (\"-------\",pair_values, cont_citation_groups, lista_bin_names[cont_citation_groups])\n",
    "    \n",
    "    minimo = pair_values[0]\n",
    "    maximo = pair_values[1]   \n",
    "    \n",
    "    string_cit_group = lista_bin_names[cont_citation_groups]\n",
    "    cont_citation_groups += 1\n",
    "    \n",
    "\n",
    "    df_select = df_records_one_ref_per_plos[(df_records_one_ref_per_plos['paper_cite_count'] >= minimo)  &  (df_records_one_ref_per_plos['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "    list_q_cell = [.25,.5,.75]\n",
    "    list_quantiles = []   # for [intro, methods, results, discussion]\n",
    "    for column in ['num_ref_section0', 'num_ref_section1', 'num_ref_section2', 'num_ref_section3']:   #  intro  methods results dicussion\n",
    "        quantiles_cell_aux  = sorted(list(df_select[column].quantile(list_q_cell).to_dict().items())) #mean 10.68 \n",
    "        list_quantiles.append(quantiles_cell_aux)\n",
    "        \n",
    "                \n",
    "        x1 = list(df_select[column])\n",
    "        \n",
    "\n",
    "\n",
    "        if column == 'num_ref_section0':                        \n",
    "            group= \"Introduction \"+ string_cit_group       \n",
    "            \n",
    "        elif column ==  'num_ref_section1':                              \n",
    "            group= \"Methods \"+ string_cit_group                \n",
    "            \n",
    "        elif column ==  'num_ref_section2':               \n",
    "            group= \"Results \"+ string_cit_group          \n",
    "                \n",
    "        elif column ==  'num_ref_section3':               \n",
    "            group= \"Discussion \"+ string_cit_group                   \n",
    "            \n",
    "\n",
    "        lista_set_names.append(group)   \n",
    "\n",
    "        dict_group_subset_data[group]=x1\n",
    "        \n",
    "        \n",
    "       \n",
    "    print (pair_values, df_records_one_ref_per_plos.shape, df_select.shape, list_quantiles)\n",
    "   \n",
    "\n",
    "\n",
    "    #example   list_quantiles: [[(0.25, 7.0), (0.5, 12.0), (0.75, 17.0)],      [(0.25, 1.0), (0.5, 3.0), (0.75, 6.0)],      [(0.25, 0.0), (0.5, 0.0), (0.75, 2.0)],      [(0.25, 5.0), (0.5, 11.0), (0.75, 18.0)]]\n",
    "    \n",
    "    \n",
    "     \n",
    "    aux_list = [int(df_select.num_ref_section0.median()), int(df_select.num_ref_section1.median()), int(df_select.num_ref_section2.median()), int(df_select.num_ref_section3.median()) ]      # [intro, methods, results, discussion]\n",
    "    \n",
    "    \n",
    "    \n",
    "    lista_trios = []\n",
    "    for lista_pares in list_quantiles:\n",
    "        trio_valores = [ int(i[1]) for i in lista_pares]\n",
    "        lista_trios.append(trio_valores)\n",
    "    \n",
    "    print (aux_list,\"\\n\", lista_trios,'\\n')\n",
    "    list_of_lists.append(aux_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "                                                                                                                                          \n",
    "                                                                                                                                          \n",
    "    aux_lista_text_z =  [str(lista_trios[i]).replace('[','').replace(']','').replace(', ', '-')   + \"\"     for i in range(len(aux_list))]     # [intro, methods, results, discussion]\n",
    "  \n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "    new_lista = [ item.split(\"-\")[0] + \"-<b>\"+ item.split(\"-\")[1] +\"</b>-\"+ item.split(\"-\")[2] for item in aux_lista_text_z  ]  # i add bold font for the median value of the trio\n",
    "    \n",
    "    \n",
    "    lista_text_z.append(new_lista)\n",
    "    \n",
    "  \n",
    "    \n",
    "    \n",
    "    \n",
    "print (\"\\n\\n\")\n",
    "for lista in list_of_lists:\n",
    "    print (lista)\n",
    "\n",
    "print (\"\\n\\n\")\n",
    "for lista in lista_text_z:\n",
    "    print (lista)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_colorscale = [[0, '#ecbfe0'], [1, '#66475e']]   #   [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "fig_font_colors = [ '#3c3636', '#efecee']   #['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "# fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "# fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colorbar_string = 'Number of References'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "#colorscale='Viridis', \n",
    "\n",
    "\n",
    "    \n",
    "fig = ff.create_annotated_heatmap(z=list_of_lists, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors, showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis7\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral  #+10\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7\n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=0,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_annotated_heatmap_color' ,image_width=2000, image_height=1200, filename='testing_annotated_heatmap_color.html', validate=True)\n",
    "\n",
    "\n",
    "############################################\n",
    "########################################################################\n",
    "################################################\n",
    "\n",
    "\n",
    "## COPIED CELL!!##############  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2A)\n",
    "\n",
    "#####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "                 [2,1],[2,2],[2,3],[2,4],\\\n",
    "                 [3,1],[3,2],[3,3],[3,4],\\\n",
    "                 [4,1],[4,2],[4,3],[4,4],\\\n",
    "                 [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "#fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tot_rows = 5\n",
    "tot_cols = 4  \n",
    "    \n",
    "test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "\n",
    "\n",
    "list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "                   'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                   'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                   'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "                   'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "                     'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "                     'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "                     'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "                     'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "\n",
    "threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    " \n",
    "#lista de sets en el orden obtenido en la celda anterior:\n",
    "\n",
    "# ['Introduction Bottom',\n",
    "#  'Methods Bottom',\n",
    "#  'Results Bottom',\n",
    "#  'Discussion Bottom',\n",
    "#  'Introduction Typical',\n",
    "#  'Methods Typical',\n",
    "#  'Results Typical',\n",
    "#  'Discussion Typical',\n",
    "#  'Introduction Good',\n",
    "#  'Methods Good',\n",
    "#  'Results Good',\n",
    "#  'Discussion Good',\n",
    "#  'Introduction High',\n",
    "#  'Methods High',\n",
    "#  'Results High',\n",
    "#  'Discussion High',\n",
    "#  'Introduction Top',\n",
    "#  'Methods Top',\n",
    "#  'Results Top',\n",
    "#  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_tot_datos=[]\n",
    "\n",
    "total_cont =0\n",
    "for i in list_keys_macro:\n",
    "    lista_listas=[]\n",
    "    aux_lista=[]\n",
    "    cont=1\n",
    "   \n",
    "    for j in list_keys_heatmap:\n",
    "             \n",
    "        set1 = dict_group_subset_data[i]\n",
    "        set2 = dict_group_subset_data[j]\n",
    "        \n",
    "        if test == \"KS\":\n",
    "            p_value = stats.ks_2samp(set1, set2)[1] \n",
    "        elif test == \"MW\":\n",
    "            p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "        if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "                p_value =0.\n",
    "                \n",
    "        else:\n",
    "            p_value = .5  ### I ONLY CARFE ABOUT WHEThER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "            \n",
    "            \n",
    "                \n",
    "        if i == j:  # i single out manually the self comparison\n",
    "            p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "        aux_lista.append(p_value)\n",
    "        \n",
    "        cont +=1\n",
    "        \n",
    "        if cont > tot_cols:\n",
    "            lista_listas.append(aux_lista)                  \n",
    "            aux_lista=[]\n",
    "            cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "    total_cont +=1\n",
    "    lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "### I create an empty plot\n",
    "fig_macro = None\n",
    "fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "for i in range(len(lista_tot_datos)):\n",
    "    datos = lista_tot_datos[i]\n",
    "    \n",
    "    cont_rows = lista_indeces[i][0]\n",
    "    cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "    trace1 = go.Heatmap(z=datos,\n",
    "                       x=lista_sections,\n",
    "                       y=lista_bin_names,                        \n",
    "                       colorscale = fig_colorscale,\n",
    "                       showscale=False,)\n",
    "                        #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # i add each plot\n",
    "    fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "fontsize=32 \n",
    "fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "fig_macro['layout']['margin'].update({'b': 150})  \n",
    "fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# i use the axis title of the small plots as values of the axis of the macro plot\n",
    "fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "              filename='multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dict_group_subset_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_lista = ['7-12-17', '1-3-6', '0-0-2', '5-11-18']\n",
    "print (aux_lista)\n",
    "\n",
    "new_lista = [ item.split(\"-\")[0] + \"-<b>\"+ item.split(\"-\")[1] +\"</b>-\"+ item.split(\"-\")[2] for item in aux_lista  ]\n",
    "print (new_lista)\n",
    "\n",
    "\n",
    "\n",
    "#value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_records_one_ref_per_plos.paper_cite_count.unique())\n",
    "sorted(df_records_one_ref_per_plos.plos_pub_year.unique())\n",
    "len(df_merged.paper_UT.unique())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_window = 1\n",
    "\n",
    "preselection_df_old = df_merged[df_merged['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "print (\"old:\",preselection_df_old.shape, len(preselection_df_old.paper_UT.unique()),  len(preselection_df_old.reference_UT.unique()))\n",
    "\n",
    "time_window = 10\n",
    "preselection_df_young = df_merged[df_merged['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "print (\"young\",preselection_df_young.shape, ,len(preselection_df3.paper_UT.unique()),  len(preselection_df3.reference_UT.unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### FIGURE 2C, HEATMAP FOR NUMBER OF young  REFERENCES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_bin_names=[\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "# lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1]    \n",
    "    \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "# #quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles=sorted(list(df_records_one_ref_per_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     try:\n",
    "#         pair=[old_value, int(item[1])]    \n",
    "#     except:  # if it is a nan:\n",
    "#         pair=[old_value, item[1]]\n",
    "    \n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "#     try:\n",
    "#         old_value = int(item[1])\n",
    "#     except:\n",
    "#         old_value = item[1]\n",
    "\n",
    "# print (lista_bins_plos_citations, \" tot # records:\",df_records_one_ref_per_plos.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_set_names = []\n",
    "# lista_text_z = []\n",
    "# list_of_lists =[]\n",
    "# cont_cit_group = 0\n",
    " \n",
    "# for pair_values in lista_bins_plos_citations:    # needs to correspond to    [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]   \n",
    "    \n",
    "#     minimo = pair_values[0]\n",
    "#     maximo = pair_values[1]   \n",
    "    \n",
    "#     string_cit_group = lista_bin_names[cont_cit_group]        \n",
    "#     cont_cit_group +=1\n",
    "    \n",
    "#     print (\"-------\",pair_values, cont_cit_group, lista_bin_names[cont_cit_group])\n",
    "    \n",
    "    \n",
    "    \n",
    "#     df_select = df_records_one_ref_per_plos[(df_records_one_ref_per_plos['paper_cite_count'] >= minimo)  &  (df_records_one_ref_per_plos['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "        \n",
    "    \n",
    "#     list_q_cell = [.25,.5,.75]\n",
    "#     list_quantiles = []   #  [intro, methods, results, discussion]\n",
    "#     for column in ['fract_young_ref_section0', 'fract_young_ref_section1', 'fract_young_ref_section2', 'fract_young_ref_section3']:\n",
    "#         quantiles_cell_aux  = sorted(list(df_select[column].quantile(list_q_cell).to_dict().items())) #mean 10.68 \n",
    "#         list_quantiles.append(quantiles_cell_aux)\n",
    "                                        \n",
    "#         x1 = list(df_select[column])\n",
    "        \n",
    "\n",
    "\n",
    "#         if column == 'num_ref_section0':                        \n",
    "#             group= \"Introduction \"+ string_cit_group       \n",
    "            \n",
    "#         elif column ==  'num_ref_section1':                              \n",
    "#             group= \"Methods \"+ string_cit_group                \n",
    "            \n",
    "#         elif column ==  'num_ref_section2':               \n",
    "#             group= \"Results \"+ string_cit_group          \n",
    "                \n",
    "#         elif column ==  'num_ref_s ection3':               \n",
    "#             group= \"Discussion \"+ string_cit_group                   \n",
    "            \n",
    "\n",
    "#         lista_set_names.append(group)   \n",
    "\n",
    "#         dict_group_subset_data[group]=x1\n",
    "        \n",
    "#         print (\" \",group)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#     print (pair_values, \"  # records in impact group:\", df_select.shape)\n",
    "   \n",
    "\n",
    "\n",
    "#     #example   list_quantiles: [[(0.25, 7.0), (0.5, 12.0), (0.75, 17.0)],      [(0.25, 1.0), (0.5, 3.0), (0.75, 6.0)],      [(0.25, 0.0), (0.5, 0.0), (0.75, 2.0)],      [(0.25, 5.0), (0.5, 11.0), (0.75, 18.0)]]\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "#     aux_list = [df_select.fract_young_ref_section0.mean(), df_select.fract_young_ref_section1.mean(), df_select.fract_young_ref_section2.mean(), df_select.fract_young_ref_section3.mean() ]      # [intro, methods, results, discussion]\n",
    "    \n",
    "     \n",
    "#     list_of_lists.append(aux_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     aux_z  = [ round(i,2) for i in aux_list]\n",
    "#     lista_text_z.append(aux_z)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# colorbar_string = 'Fraction of references'\n",
    "\n",
    "\n",
    "\n",
    "# print (dict_group_subset_data.keys())\n",
    "\n",
    "# print (lista_set_names)\n",
    "\n",
    "\n",
    "    \n",
    "# fig = ff.create_annotated_heatmap(z=list_of_lists, x=lista_sections, y=lista_bin_names, annotation_text = lista_text_z,  colorscale='Viridis', showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ))\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "# font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    " \n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral  +20\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -7\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "# fig['layout']['title'] = \"Young references\"\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         t=150,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_annotated_heatmap_color' ,image_width=2000, image_height=1200, filename='testing_annotated_heatmap_color.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #################################################\n",
    "# #####################################################\n",
    "# ################################################\n",
    "# #######################################################\n",
    "\n",
    "# ##  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2A)\n",
    "\n",
    "# #####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "# lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "#                  [2,1],[2,2],[2,3],[2,4],\\\n",
    "#                  [3,1],[3,2],[3,3],[3,4],\\\n",
    "#                  [4,1],[4,2],[4,3],[4,4],\\\n",
    "#                  [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "# #fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "# fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "# lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tot_rows = 5\n",
    "# tot_cols = 4  \n",
    "    \n",
    "# test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "# ### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "# threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "#                    'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "#                    'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "#                    'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "#                    'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "# list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "#                      'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "#                      'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "#                      'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "#                      'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "# #lista de sets en el orden obtenido en la celda anterior:\n",
    "\n",
    "# # ['Introduction Bottom',\n",
    "# #  'Methods Bottom',\n",
    "# #  'Results Bottom',\n",
    "# #  'Discussion Bottom',\n",
    "# #  'Introduction Typical',\n",
    "# #  'Methods Typical',\n",
    "# #  'Results Typical',\n",
    "# #  'Discussion Typical',\n",
    "# #  'Introduction Good',\n",
    "# #  'Methods Good',\n",
    "# #  'Results Good',\n",
    "# #  'Discussion Good',\n",
    "# #  'Introduction High',\n",
    "# #  'Methods High',\n",
    "# #  'Results High',\n",
    "# #  'Discussion High',\n",
    "# #  'Introduction Top',\n",
    "# #  'Methods Top',\n",
    "# #  'Results Top',\n",
    "# #  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_tot_datos=[]\n",
    "\n",
    "# total_cont =0\n",
    "# for i in list_keys_macro:\n",
    "#     lista_listas=[]\n",
    "#     aux_lista=[]\n",
    "#     cont=1\n",
    "   \n",
    "#     for j in list_keys_heatmap:\n",
    "             \n",
    "#         set1 = dict_group_subset_data[i]\n",
    "#         set2 = dict_group_subset_data[j]\n",
    "        \n",
    "#         if test == \"KS\":\n",
    "#             p_value = stats.ks_2samp(set1, set2)[1] \n",
    "#         elif test == \"MW\":\n",
    "#             p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "#         if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "#                 p_value =0.\n",
    "                \n",
    "#         else:\n",
    "#             p_value = .5  ### I ONLY CARFE ABOUT WHEThER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "            \n",
    "            \n",
    "                \n",
    "#         if i == j:  # i single out manually the self comparison\n",
    "#             p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "#         aux_lista.append(p_value)\n",
    "        \n",
    "#         cont +=1\n",
    "        \n",
    "#         if cont > tot_cols:\n",
    "#             lista_listas.append(aux_lista)                  \n",
    "#             aux_lista=[]\n",
    "#             cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "#     total_cont +=1\n",
    "#     lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "# ##############################\n",
    "\n",
    "# ### I create an empty plot\n",
    "# fig_macro = None\n",
    "# fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "# for i in range(len(lista_tot_datos)):\n",
    "#     datos = lista_tot_datos[i]\n",
    "    \n",
    "#     cont_rows = lista_indeces[i][0]\n",
    "#     cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "#     trace1 = go.Heatmap(z=datos,\n",
    "#                        x=lista_sections,\n",
    "#                        y=lista_bin_names,                        \n",
    "#                        colorscale = fig_colorscale,\n",
    "#                        showscale=False,)\n",
    "#                         #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     # I add each plot\n",
    "#     fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# fontsize=32 \n",
    "# fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "# fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "# fig_macro['layout']['margin'].update({'b': 150})  \n",
    "# fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "# lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "# lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# # i use the axis title of the small plots as values of the axis of the macro plot\n",
    "# fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "# fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "# fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "# fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "# fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "# fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "# fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "# fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "# fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "#               filename='multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###########################################\n",
    "# ######################################################3\n",
    "# ########################################################\n",
    "\n",
    "\n",
    "# ##############  I RUN THE TESTS FOR THE COMPARISON OF ALL PAIRS OF SUB-SETS:  (all pair-wise comparison cells in figure 2b, and 3a, 2b)\n",
    "\n",
    "# #####  mann-whitney U   test\n",
    "\n",
    "\n",
    "\n",
    "# lista_indeces = [[1,1],[1,2],[1,3],[1,4],\\\n",
    "#                  [2,1],[2,2],[2,3],[2,4],\\\n",
    "#                  [3,1],[3,2],[3,3],[3,4],\\\n",
    "#                  [4,1],[4,2],[4,3],[4,4],\\\n",
    "#                  [5,1],[5,2],[5,3],[5,4]]\n",
    "\n",
    "# #fig_colorscale=  [ [0., '#f2f2f2'], [threshold_zero,'#cce6ff'], [0.9999, '#0059b3'], [1.,'#bdbdbd']]# '#ffffff']]  0: white,  .99999: blue,  1: grey\n",
    "# fig_colorscale=  [ [0., '#0059b3'], [.5,'#c7dcf1'], [1.,'#bdbdbd']] #  0 or anything significant: blue,   .5 or anithing NON signif: light-blue,     1: grey\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "# lista_sections  =  ['Intro', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tot_rows = 5\n",
    "# tot_cols = 4  \n",
    "    \n",
    "# test = \"MW\"  # KS to test if the distributions are different or  MW for testing just whether the medians are different \n",
    "    \n",
    "\n",
    "# ### i apply the bonferroni correction:  new p-_value_threshold  required for significance = old_p_value /Number of comparisons = 0.001 / (20*19/2)     \n",
    "    \n",
    "# threshold_zero = 0.0001 / (float(len(list_keys_macro))*float(len(list_keys_macro)-1)/2.)    # to round up to zero the very small p_values\n",
    "\n",
    "\n",
    "\n",
    "# list_keys_macro = ['Introduction Top',    'Methods Top',    'Results Top',   'Discussion Top', \\\n",
    "#                    'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "#                    'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "#                    'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical', \\\n",
    "#                    'Introduction Bottom',  'Methods Bottom',  'Results Bottom', 'Discussion Bottom']\n",
    "    \n",
    "      \n",
    "# list_keys_heatmap = ['Introduction Bottom', 'Methods Bottom',  'Results Bottom', 'Discussion Bottom',\\\n",
    "#                      'Introduction Typical', 'Methods Typical', 'Results Typical', 'Discussion Typical',\\\n",
    "#                      'Introduction Good',   'Methods Good',   'Results Good',  'Discussion Good', \\\n",
    "#                      'Introduction High',   'Methods High',  'Results High',  'Discussion High', \\\n",
    "#                      'Introduction Top',   'Methods Top',    'Results Top',   'Discussion Top'  ]\n",
    "\n",
    "\n",
    "# # lista_titulos_sets  (from the previous cell)\n",
    "\n",
    "# # ['Introduction Bottom',\n",
    "# #  'Methods Bottom',\n",
    "# #  'Results Bottom',\n",
    "# #  'Discussion Bottom',\n",
    "# #  'Introduction Typical',\n",
    "# #  'Methods Typical',\n",
    "# #  'Results Typical',\n",
    "# #  'Discussion Typical',\n",
    "# #  'Introduction Good',\n",
    "# #  'Methods Good',\n",
    "# #  'Results Good',\n",
    "# #  'Discussion Good',\n",
    "# #  'Introduction High',\n",
    "# #  'Methods High',\n",
    "# #  'Results High',\n",
    "# #  'Discussion High',\n",
    "# #  'Introduction Top',\n",
    "# #  'Methods Top',\n",
    "# #  'Results Top',\n",
    "# #  'Discussion Top']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_tot_datos=[]\n",
    "\n",
    "# total_cont =0\n",
    "# for i in list_keys_macro:\n",
    "#     lista_listas=[]\n",
    "#     aux_lista=[]\n",
    "#     cont=1\n",
    "   \n",
    "#     for j in list_keys_heatmap:\n",
    "             \n",
    "#         set1 = dict_group_subset_data[i]\n",
    "#         set2 = dict_group_subset_data[j]\n",
    "        \n",
    "#         if test == \"KS\":\n",
    "#             p_value = stats.ks_2samp(set1, set2)[1] \n",
    "#         elif test == \"MW\":\n",
    "#             p_value = stats.mannwhitneyu(set1, set2,  alternative='two-sided')[1]  \n",
    "        \n",
    "        \n",
    "#         if p_value <= threshold_zero:  #i round up to zero the very small p_values\n",
    "#                 p_value =0.\n",
    "                \n",
    "#         else:\n",
    "#             p_value = .5  ### I ONLY CARFE ABOUT WHETGER IT IS SIGNIFICANT OR NOT, I DONT CARE ABOUT THE EXACT P-VALUE ONCE IT IS NOT SIGNIFICANT\n",
    "                \n",
    "#         if i == j:  # i single out manually the self comparison\n",
    "#             p_value=1.001  \n",
    "            \n",
    "            \n",
    "            \n",
    "#         aux_lista.append(p_value)\n",
    "        \n",
    "#         cont +=1\n",
    "        \n",
    "#         if cont > tot_cols:\n",
    "#             lista_listas.append(aux_lista)                  \n",
    "#             aux_lista=[]\n",
    "#             cont=1\n",
    "            \n",
    "    \n",
    "    \n",
    "#     total_cont +=1\n",
    "#     lista_tot_datos.append(lista_listas)\n",
    "    \n",
    "    \n",
    "   \n",
    "\n",
    "\n",
    "# ##############################\n",
    "\n",
    "# ### I create an empty plot\n",
    "# fig_macro = None\n",
    "# fig_macro = tools.make_subplots(rows=tot_rows, cols=tot_cols, shared_xaxes=True, shared_yaxes=True, vertical_spacing = 0.01, horizontal_spacing = 0.01,   )\n",
    "\n",
    "\n",
    "# for i in range(len(lista_tot_datos)):\n",
    "#     datos = lista_tot_datos[i]\n",
    "    \n",
    "#     cont_rows = lista_indeces[i][0]\n",
    "#     cont_cols = lista_indeces[i][1]\n",
    "    \n",
    "#     trace1 = go.Heatmap(z=datos,\n",
    "#                        x=lista_sections,\n",
    "#                        y=lista_bin_names,                        \n",
    "#                        colorscale = fig_colorscale,\n",
    "#                        showscale=False,)\n",
    "#                         #reversescale=True, )#    )\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     # i add each plot\n",
    "#     fig_macro.append_trace(trace1, cont_rows, cont_cols)\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# fontsize=32 \n",
    "# fig_macro['layout']['font'].update({'size': fontsize})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_macro['layout']['margin'].update({'l': 250})   # i add extra space to the margins of the plots\n",
    "# fig_macro['layout']['margin'].update({'b': 150})  \n",
    "# fig_macro['layout']['margin'].update({'t': 50})   \n",
    "\n",
    "\n",
    "\n",
    "# lista_bin_names = [\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "# lista_sections  =  ['Introduction', 'Methods', 'Results', 'Discussion']\n",
    "\n",
    "\n",
    "\n",
    "# # i use the axis title of the small plots as values of the axis of the macro plot\n",
    "# fig_macro['layout']['xaxis1'].update(title=lista_sections[0])  \n",
    "# fig_macro['layout']['xaxis2'].update(title=lista_sections[1])\n",
    "# fig_macro['layout']['xaxis3'].update(title=lista_sections[2])\n",
    "# fig_macro['layout']['xaxis4'].update(title=lista_sections[3])\n",
    "\n",
    "\n",
    "# fig_macro['layout']['yaxis1'].update(title=lista_bin_names[4])   # ojo con el orden aqui!!\n",
    "# fig_macro['layout']['yaxis2'].update(title=lista_bin_names[3])\n",
    "# fig_macro['layout']['yaxis3'].update(title=lista_bin_names[2])\n",
    "# fig_macro['layout']['yaxis4'].update(title=lista_bin_names[1])\n",
    "# fig_macro['layout']['yaxis5'].update(title=lista_bin_names[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# offline.plot(fig_macro, auto_open=True, image = 'png', image_filename='multiplot_comparisons' ,image_width=3000, image_height=2200,\n",
    "#               filename='multiplot_comparisons.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_records_one_ref_per_plos.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE 2C, HEATMAP FOR NUMBER OF YOUNG  REFERENCES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    \n",
    "    \n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(df_records_one_ref_per_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "    \n",
    "    lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations, df_records_one_ref_per_plos.shape)\n",
    "\n",
    "\n",
    "lista_text_z = []\n",
    "list_of_lists =[]\n",
    "for pair_values in lista_bins_plos_citations:       \n",
    "    \n",
    "    minimo = pair_values[0]\n",
    "    maximo = pair_values[1]   \n",
    "\n",
    "    df_select = df_records_one_ref_per_plos[(df_records_one_ref_per_plos['paper_cite_count'] >= minimo)  &  (df_records_one_ref_per_plos['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "    list_q_cell = [.25,.5,.75]\n",
    "    list_quantiles = []   # for [intro, methods, results, discussion]\n",
    "    for column in ['fract_young_ref_section0', 'fract_young_ref_section1', 'fract_young_ref_section2', 'fract_young_ref_section3']:\n",
    "        quantiles_cell_aux  = sorted(list(df_select[column].quantile(list_q_cell).to_dict().items())) #mean 10.68 \n",
    "        list_quantiles.append(quantiles_cell_aux)\n",
    "    print (pair_values, df_records_one_ref_per_plos.shape, df_select.shape, list_quantiles)\n",
    "   \n",
    "\n",
    "\n",
    "    #example   list_quantiles: [[(0.25, 7.0), (0.5, 12.0), (0.75, 17.0)],      [(0.25, 1.0), (0.5, 3.0), (0.75, 6.0)],      [(0.25, 0.0), (0.5, 0.0), (0.75, 2.0)],      [(0.25, 5.0), (0.5, 11.0), (0.75, 18.0)]]\n",
    "    \n",
    "    \n",
    "  \n",
    "    \n",
    "    #aux_list = [df_select.fract_young_ref_section0.mean(), df_select.fract_young_ref_section1.mean(), df_select.fract_young_ref_section2.mean(), df_select.fract_young_ref_section3.mean() ]      # [intro, methods, results, discussion]\n",
    "    aux_list = [df_select.fract_young_ref_section0.mean()*100., df_select.fract_young_ref_section1.mean()*100., df_select.fract_young_ref_section2.mean()*100., df_select.fract_young_ref_section3.mean()*100. ]      # [intro, methods, results, discussion]\n",
    "    \n",
    "    \n",
    "    list_of_lists.append(aux_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    aux_z  = [ round(i,2) for i in aux_list]\n",
    "    lista_text_z.append(aux_z)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     lista_trios = []\n",
    "#     for lista_pares in list_quantiles:\n",
    "#         trio_valores = [ int(i[1]) for i in lista_pares]\n",
    "#         lista_trios.append(trio_valores)\n",
    "    \n",
    "#     print (aux_list,\"\\n\", lista_trios,'\\n')\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "     #aux_lista_text_z =  [str(lista_trios[i]).replace('[','').replace(']','').replace(', ', '-')   + \"\"     for i in range(len(aux_list))]     # [intro, methods, results, discussion]\n",
    "                                                                                                                                       \n",
    "                                                                                                                                          \n",
    "                                                                                                                                          \n",
    "#     aux_lista_text_z =  [str(lista_trios[i]).replace('[','').replace(']','').replace(', ', '-')   + \"\"     for i in range(len(aux_list))]     # [intro, methods, results, discussion]\n",
    "  \n",
    "#     lista_text_z.append(aux_lista_text_z)\n",
    "    \n",
    "# print (\"\\n\\n\")\n",
    "# for lista in list_of_lists:\n",
    "#     print (lista)\n",
    "\n",
    "# print (\"\\n\\n\")\n",
    "# for lista in lista_text_z:\n",
    "#     print (lista)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig_colorscale = [[0, '#dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "# fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "# fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "# fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "colorbar_string = 'Percentage young references'     #Relative fraction of References'\n",
    "\n",
    "\n",
    "lista_bin_names=[\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "\n",
    "###  i round up the values for the labels of each cell to one single decimal\n",
    "print (lista_text_z)\n",
    "for i in range(len(lista_text_z)):\n",
    "    item_lista= lista_text_z[i]\n",
    "    for j in range(len(item_lista)):\n",
    "        item = item_lista[j]\n",
    "        lista_text_z[i][j] =  round(item,1) \n",
    "\n",
    "print (\"\\n\\n\")\n",
    "print (lista_text_z)\n",
    "\n",
    "\n",
    "    \n",
    "fig = ff.create_annotated_heatmap(z=list_of_lists, x=lista_sections, y=lista_bin_names, annotation_text = lista_text_z,  colorscale='Viridis', showscale=True,\\\n",
    "                                  colorbar=dict(title=colorbar_string,titleside='right', )) #colorbar=dict(title=colorbar_string,x=.0,y=.95, titleside='right' ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "#fig.layout.title = 'Percentage Young references'\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "\n",
    "\n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis7\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral  +15\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7\n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=150,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# fig.layout.update({      \n",
    "#      'annotations' : [\n",
    "         \n",
    "#          dict(  # this is for the xaxis label\n",
    "#             x=0.,\n",
    "#             y=1.05,\n",
    "#             xref='paper',\n",
    "#             yref='paper',\n",
    "#             text=\"(c)\",\n",
    "#             showarrow=False,  \n",
    "#             font=dict(               \n",
    "#                 size=45,),\n",
    "#            ),  ]  \n",
    "#     })\n",
    "         \n",
    "         \n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_annotated_heatmap_color' ,image_width=2000, image_height=1200, filename='testing_annotated_heatmap_color.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2.95\n",
    "round(i,1)\n",
    "\n",
    "lista_de_listas = []\n",
    "print (lista_text_z)\n",
    "for i in range(len(lista_text_z)):\n",
    "    item_lista= lista_text_z[i]\n",
    "    for j in range(len(item_lista)):\n",
    "        item = item_lista[j]\n",
    "        lista_text_z[i][j] =  round(item,1) \n",
    "\n",
    "print (\"\\n\\n\")\n",
    "print (lista_text_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIGURE 2D, HEATMAP FOR NUMBER OF OLD  REFERENCES\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_q=[0.3,0.6,.9,.99,1]    \n",
    "    \n",
    "#### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "#quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "quantiles=sorted(list(df_records_one_ref_per_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    try:\n",
    "        pair=[old_value, int(item[1])]    \n",
    "    except:  # if it is a nan:\n",
    "        pair=[old_value, item[1]]\n",
    "    \n",
    "    lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "    try:\n",
    "        old_value = int(item[1])\n",
    "    except:\n",
    "        old_value = item[1]\n",
    "\n",
    "print (lista_bins_plos_citations, df_records_one_ref_per_plos.shape)\n",
    "\n",
    "\n",
    "lista_text_z = []\n",
    "list_of_lists =[]\n",
    "for pair_values in lista_bins_plos_citations:       \n",
    "    \n",
    "    minimo = pair_values[0]\n",
    "    maximo = pair_values[1]   \n",
    "\n",
    "    df_select = df_records_one_ref_per_plos[(df_records_one_ref_per_plos['paper_cite_count'] >= minimo)  &  (df_records_one_ref_per_plos['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "    list_q_cell = [.25,.5,.75]\n",
    "    list_quantiles = []   # for [intro, methods, results, discussion]\n",
    "    for column in ['fract_old_ref_section0', 'fract_old_ref_section1', 'fract_old_ref_section2', 'fract_old_ref_section3']:\n",
    "        quantiles_cell_aux  = sorted(list(df_select[column].quantile(list_q_cell).to_dict().items())) #mean 10.68 \n",
    "        list_quantiles.append(quantiles_cell_aux)\n",
    "    print (pair_values, df_records_one_ref_per_plos.shape, df_select.shape, list_quantiles)\n",
    "   \n",
    "\n",
    "   \n",
    "    \n",
    "  \n",
    "    \n",
    "    #aux_list = [df_select.fract_old_ref_section0.mean(), df_select.fract_old_ref_section1.mean(), df_select.fract_old_ref_section2.mean(), df_select.fract_old_ref_section3.mean() ]      # [intro, methods, results, discussion]\n",
    "    aux_list = [df_select.fract_old_ref_section0.mean()*100., df_select.fract_old_ref_section1.mean()*100., df_select.fract_old_ref_section2.mean()*100., df_select.fract_old_ref_section3.mean()*100. ]      # [intro, methods, results, discussion]\n",
    "    \n",
    "    \n",
    "    list_of_lists.append(aux_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    aux_z  = [ round(i,2) for i in aux_list]\n",
    "    lista_text_z.append(aux_z)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "colorbar_string = 'Percentage old references'     #Relative fraction of References'\n",
    "\n",
    "\n",
    "lista_bin_names=[\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "\n",
    "###  i round up the values for the labels of each cell to one single decimal\n",
    "print (lista_text_z)\n",
    "for i in range(len(lista_text_z)):\n",
    "    item_lista= lista_text_z[i]\n",
    "    for j in range(len(item_lista)):\n",
    "        item = item_lista[j]\n",
    "        lista_text_z[i][j] =  round(item,1) \n",
    "\n",
    "print (\"\\n\\n\")\n",
    "print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "fig = ff.create_annotated_heatmap(z=list_of_lists, x=lista_sections, y=lista_bin_names, annotation_text = lista_text_z,  colorscale='Viridis', showscale=True,\\\n",
    "                                  colorbar=dict(title=colorbar_string,titleside='right', )) #colorbar=dict(title=colorbar_string,x=.0,y=.95, titleside='right' ))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "#fig.layout.title = 'Old references'\n",
    "\n",
    "fig['layout']['xaxis']['side'] = 'bottom'\n",
    "fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# Altering x axis7\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "fig['layout']['yaxis']['tickangle'] = -90\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral  +15\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -7\n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=150,\n",
    "        t=150,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_annotated_heatmap_color' ,image_width=2000, image_height=1200, filename='testing_annotated_heatmap_color.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Learn about API authentication here: https://plot.ly/python/getting-started\n",
    "# # Find your api_key here: https://plot.ly/settings/api\n",
    "\n",
    "# import plotly.plotly as py\n",
    "# import plotly.graph_objs as go\n",
    "\n",
    "# # Scientific libraries\n",
    "# from numpy import arange,array,ones\n",
    "# from scipy import stats\n",
    "\n",
    "\n",
    "# xi = arange(0,9)\n",
    "# y = [19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24]\n",
    "\n",
    "# # Generated linear fit\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "# line = slope*xi+intercept\n",
    "# print (line)\n",
    "\n",
    "# print ('slope:', slope, 'intercept:', intercept, 'r_value:', r_value, 'p_value:', p_value, 'std_err:', std_err )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Creating the dataset, and generating the plot\n",
    "# trace1 = go.Scatter(\n",
    "#                   x=xi,\n",
    "#                   y=y,\n",
    "#                   mode='markers',\n",
    "#                   marker=go.Marker(color='rgb(255, 127, 14)'),\n",
    "#                   name='Data'\n",
    "#                   )\n",
    "\n",
    "# trace2 = go.Scatter(\n",
    "#                   x=xi,\n",
    "#                   y=line,\n",
    "#                   mode='lines',\n",
    "#                   marker=go.Marker(color='rgb(31, 119, 180)'),\n",
    "#                   name='Fit'\n",
    "#                   )\n",
    "\n",
    "# annotation = go.Annotation(\n",
    "#                   x=3.5,\n",
    "#                   y=23.5,\n",
    "#                   text='R^2 = '+str(r_value**2)+' <br>Y = '+str(round(slope,2))+'X + '+ str(round(intercept,2)),\n",
    "#                   showarrow=False,\n",
    "#                   font=go.Font(size=16)\n",
    "#                   )\n",
    "# layout = go.Layout(\n",
    "#                 title='Linear Fit in Python',\n",
    "#                 plot_bgcolor='rgb(229, 229, 229)',\n",
    "#                   xaxis=go.XAxis(zerolinecolor='rgb(255,255,255)', gridcolor='rgb(255,255,255)'),\n",
    "#                   yaxis=go.YAxis(zerolinecolor='rgb(255,255,255)', gridcolor='rgb(255,255,255)'),\n",
    "#                   annotations=[annotation]\n",
    "#                 )\n",
    "\n",
    "# data = [trace1, trace2]\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_scatter_plot' ,image_width=2000, image_height=1200, filename='testing_scatter_plot.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### for SI.:  SCATTER PLOT FOR NUMBER OF CIATIONS OF A PAPER VS # CITATIONS OF ITS YOUNG REFERENCES\n",
    "\n",
    "\n",
    "  \n",
    "string_references_age = \"young\"   #  only for young references (because for them, the number of citations is comparable at those of the citing paper itself)\n",
    "  \n",
    "\n",
    "\n",
    "years=[2013] \n",
    "#years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#list_strings = [1,0]\n",
    "#for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### plos ONE categories. \n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "#list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "#for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "#   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "#  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "#  '0': 'Biology and life sciences'             6,032,537 --\n",
    "#  '1': 'Computer and information sciences'     1,207,799 --\n",
    "#  '10': 'Social sciences'                      755,899 --\n",
    "#  '2': 'Earth sciences'                        533,155 --\n",
    "#  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "#  '4': 'Engineering and technology'            382,247 --\n",
    "#  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "#  '6': 'People and places'                     691,523 --\n",
    "#  '7': 'Physical sciences'                     2,100,827 --\n",
    "#  '8': 'Research and analysis methods'         3,871,470 --\n",
    "#  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "######### plos journals \n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "#list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "#for string_journal in list_strings:\n",
    "\n",
    "    # PLOS ONE       6,367,070\n",
    "    # PLOS GENET      149,923\n",
    "    # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "    # PLOS PATHOG     109,803\n",
    "    # PLOS COMPUT      77,924\n",
    "    # PLOS BIOL        56,754\n",
    "    # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "######### WoS subject categories. \n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df = df_merged.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "print (\"size of preselection (each ref. only used once per plos paper):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = preselection_df[preselection_df['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "        string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        print (list_codes)\n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "        string_code_categ = \"\" \n",
    "        for code in list_codes:\n",
    "            string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_window = 1\n",
    "preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "print (\" size of preselection3 (only young ref):\",preselection_df3.shape)\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # plos papers:\",len(preselection_df3.paper_UT.unique()),\\\n",
    "       \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "selection_df = preselection_df3\n",
    "\n",
    "\n",
    "\n",
    "tag_exclude_outliers = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "legend_position=[500,10000]\n",
    "\n",
    "if tag_exclude_outliers == 1:\n",
    "\n",
    "    #### i remove outliers for the linear plot\n",
    "\n",
    "    ##### outlier references\n",
    "    list_q=[0,.25,.5,.75,.999,1]\n",
    "    var = 'cite_count'\n",
    "    df_for_ref_quantiles = preselection_df3.drop_duplicates(subset=['reference_UT'])\n",
    "    quantiles=sorted(list(df_for_ref_quantiles[var].quantile(list_q).to_dict().items()))\n",
    "    print (var, quantiles)    \n",
    "\n",
    "    preselection_no_outliers = preselection_df3[preselection_df3['cite_count'] <= quantiles[-2][1] ]   \n",
    "    pos_y = quantiles[-2][1]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ##### outlier plos papers\n",
    "    list_q=[0,.25,.5,.75,.999,1]\n",
    "    var = 'paper_cite_count'\n",
    "    df_for_plos_quantiles = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "    quantiles=sorted(list(df_for_plos_quantiles[var].quantile(list_q).to_dict().items()))\n",
    "    print (var, quantiles)\n",
    "\n",
    "\n",
    "    selection_df = preselection_no_outliers[preselection_no_outliers['paper_cite_count'] <= quantiles[-2][1] ]   \n",
    "    pos_x = quantiles[-2][1]\n",
    "\n",
    "\n",
    "#     cite_count [(0.0, 1.0), (0.25, 21.0), (0.5, 44.0), (0.75, 102.0), (0.99, 1642.0), (1.0, 21063.0)]\n",
    "#     paper_cite_count [(0.0, 0.0), (0.25, 9.0), (0.5, 17.0), (0.75, 29.0), (0.99, 126.0), (1.0, 856.0)]\n",
    "\n",
    "    \n",
    "    \n",
    "    legend_position=[pos_x, pos_y]\n",
    "    #legend_position=[120,1500]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "y = selection_df['cite_count']   # ref\n",
    "xi = selection_df['paper_cite_count']   # plos papers\n",
    "\n",
    "\n",
    "\n",
    "# Generated linear fit\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "line = slope*xi+intercept\n",
    "\n",
    "\n",
    "print ('slope:', slope, 'intercept:', intercept, 'r_value:', r_value, 'r_2:', r_value*r_value,'p_value:', p_value, 'std_err:', std_err )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tag_exclude_outliers = 0\n",
    "\n",
    "\n",
    "\n",
    "if tag_exclude_outliers == 1:\n",
    "\n",
    "    #### i remove outliers for the linear plot\n",
    "\n",
    "    ##### outlier references\n",
    "    list_q=[0,.25,.5,.75,.999,1]\n",
    "    var = 'cite_count'\n",
    "    df_for_ref_quantiles = preselection_df3.drop_duplicates(subset=['reference_UT'])\n",
    "    quantiles=sorted(list(df_for_ref_quantiles[var].quantile(list_q).to_dict().items()))\n",
    "    print (var, quantiles)    \n",
    "\n",
    "    preselection_no_outliers = preselection_df3[preselection_df3['cite_count'] <= quantiles[-2][1] ]   \n",
    "    pos_y = quantiles[-2][1]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ##### outlier plos papers\n",
    "    list_q=[0,.25,.5,.75,.999,1]\n",
    "    var = 'paper_cite_count'\n",
    "    df_for_plos_quantiles = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "    quantiles=sorted(list(df_for_plos_quantiles[var].quantile(list_q).to_dict().items()))\n",
    "    print (var, quantiles)\n",
    "\n",
    "\n",
    "    selection_df = preselection_no_outliers[preselection_no_outliers['paper_cite_count'] <= quantiles[-2][1] ]   \n",
    "    pos_x = quantiles[-2][1]\n",
    "\n",
    "\n",
    "#     cite_count [(0.0, 1.0), (0.25, 21.0), (0.5, 44.0), (0.75, 102.0), (0.99, 1642.0), (1.0, 21063.0)]\n",
    "#     paper_cite_count [(0.0, 0.0), (0.25, 9.0), (0.5, 17.0), (0.75, 29.0), (0.99, 126.0), (1.0, 856.0)]\n",
    "\n",
    "    \n",
    "    \n",
    "    legend_position=[pos_x, pos_y]\n",
    "    #legend_position=[120,1500]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #### for the scatter plot + linear fit\n",
    "trace1 = go.Scatter(\n",
    "                  x=xi,\n",
    "                  y=y,\n",
    "                  mode='markers',\n",
    "                  name ='data'+str(years[0]),\n",
    "                  marker = dict(\n",
    "                      size = 20,\n",
    "                      color = '#99d8c9',\n",
    "                      opacity = 0.95,\n",
    "                      line = dict(\n",
    "                            width = 1,\n",
    "                            color = '#404040')\n",
    "                  #name='Data'\n",
    "                          )\n",
    "                  )\n",
    "trace2 = go.Scatter(\n",
    "                  x=xi,\n",
    "                  y=line,\n",
    "                  name ='fit',\n",
    "                  mode='lines',\n",
    "                  marker=go.Marker(color='#238b45'),\n",
    "                  #name='Fit'\n",
    "                  )\n",
    "\n",
    "annotation = go.Annotation(\n",
    "                  x=150,#legend_position[0],\n",
    "                  y=4000,#legend_position[1],\n",
    "                  text='Y = '+str(round(slope,2))+'X + '+ str(round(intercept,2))+' <br>R^2 = '+str(round(r_value**2, 3)),\n",
    "                  showarrow=False,\n",
    "                  font=go.Font(size=50)\n",
    "                  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "layout = Layout(  \n",
    "    \n",
    "    showlegend=False,\n",
    "    annotations=[annotation],    \n",
    "    xaxis=dict(\n",
    "        title= '# citations for papers' ,  \n",
    "       # type='log',\n",
    "        range = [0,300],       \n",
    "        titlefont=dict(            \n",
    "            size=font_gral,            \n",
    "            ),  \n",
    "        tickfont=dict(              \n",
    "            size=font_gral -15,           \n",
    "            ),              \n",
    "    ),\n",
    "\n",
    "    yaxis=dict(\n",
    "        title='# citations for young references', \n",
    "        range = [0,7000],\n",
    "        exponentformat=\"power\",\n",
    "        showexponent='all',\n",
    "      #  type='log',\n",
    "        titlefont=dict(\n",
    "            size=font_gral,           \n",
    "             ),  \n",
    "        tickangle = 1,\n",
    "        tickfont=dict(               \n",
    "            size=font_gral -20,           \n",
    "            ),\n",
    "    ),                \n",
    "\n",
    "    margin=go.Margin(\n",
    "        l=250,\n",
    "        r=75,\n",
    "        b=150,\n",
    "        t=150,\n",
    "        pad = 0\n",
    "        \n",
    "    ),\n",
    "       \n",
    "\n",
    ")       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data = [trace1, trace2]\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_scatter_plot' ,image_width=2000, image_height=1000, filename='testing_scatter_plot.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(xi,y)\n",
    "# print(\"r-squared:\", r_value**2,\"\\n\",slope, intercept, r_value, p_value, std_err)\n",
    "\n",
    "\n",
    "# import statsmodels.api as sm\n",
    "\n",
    "\n",
    "\n",
    "# # Note the difference in argument order\n",
    "# model = sm.OLS(xi,y).fit()\n",
    "# predictions = model.predict(y) # make the predictions by the model\n",
    "\n",
    "# # Print out the statistics\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from scipy import stats\n",
    "# np.random.seed(12345678)\n",
    "# x = np.random.random(10000)\n",
    "# y = np.random.random(10000)\n",
    "# slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "# #To get coefficient of determination (r_squared)\n",
    "\n",
    "# print(\"r-squared:\", r_value**2,\"\\n\",slope, intercept, r_value, p_value, std_err)\n",
    "# # r-squared: 0.08040226853902833\n",
    "# # Plot the data along with the fitted line\n",
    "\n",
    "# plt.plot(x, y, 'o', label='original data')\n",
    "# plt.plot(x, intercept + slope*x, 'r', label='fitted line')\n",
    "# plt.legend()\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preselection_df['log_cite_count']= np.log10(preselection_df['cite_count'])\n",
    "preselection_df['log_paper_cite_count']= np.log10(preselection_df['paper_cite_count'])\n",
    "\n",
    "\n",
    "\n",
    "preselection_df[['paper_UT','reference_UT','cite_count','log_cite_count','paper_cite_count','log_paper_cite_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_subsets_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of preselection (each ref. only used once per plos paper): (4849307, 34)\n",
      "[2014]\n",
      "size of preselection1 (by plos years): (904413, 34)\n",
      "size of preselection (no self-cit): (904413, 34)\n",
      "size of preselection1 (by isolated/group ref): (904413, 34) \n",
      " size of preselection2 (by plos journal): (904413, 34) \n",
      " size of preselection2 (by plos field): (904413, 34) \n",
      " size of preselection3 (only young ref): (63466, 34)    # unique plos papers: 21299\n",
      "This is the format of your plot grid:\n",
      "[ (1,1) x1,y1 ]  [ (1,2) x2,y2 ]  [ (1,3) x3,y3 ]  [ (1,4) x4,y4 ]\n",
      "\n",
      "min, max in the top bin: 12 147\n",
      "top 12 162 (8069, 34)  # plos 2246  #ref: 7480 bins within top-plos: [[12, 13], [13, 16], [16, 20], [20, 147]]\n",
      "   bin: 12 13 (1198, 34) 52.8697829716 131.390531436\n",
      "   bin: 13 16 (2618, 34) 77.3181818182 416.973648198\n",
      "   bin: 16 20 (1889, 34) 99.2636315511 523.493164805\n",
      "   bin: 20 147 (2363, 34) 94.8887008041 334.94282483\n",
      "median: [20.0, 20.0, 24.0, 28.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/histogram.html'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "####  FIGURE 4B AND 4c:  histograms  NUMBER OF CIATIONS OF A PAPER VS # CITATIONS OF ITS YOUNGGGGG REFERENCES\n",
    "#### and NEW distplot (smoothing with kernel density estimation) with collapsed histograms of the same data\n",
    "\n",
    "\n",
    "\n",
    "string_top_bottom_plos  = 'top'  # top or bottom (plos papers)\n",
    "      \n",
    "\n",
    "years=[2014]      #[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "  \n",
    "    \n",
    "   \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df = df_merged.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "print (\"size of preselection (each ref. only used once per plos paper):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "##### preselection by plos year\n",
    "print (years)\n",
    "preselection_df = preselection_df[preselection_df['plos_pub_year'].isin(years)]  \n",
    "print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i remove self-citations\n",
    "preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "print (\"size of preselection (no self-cit):\",preselection_df.shape)\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "######### preselection by isolated or group references:\n",
    "if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "    preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "else:    \n",
    "    preselection_df0 = preselection_df   \n",
    "    print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos ONE subject category:\n",
    "if string_code_categ==\"\": \n",
    "    preselection_df111 = preselection_df0\n",
    "else:    \n",
    "    if \" \" not in string_code_categ:  # to include one single category\n",
    "        preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "     \n",
    "    else:  # if multiple codes-categories\n",
    "        list_codes = string_code_categ.split(\" \")\n",
    "        \n",
    "\n",
    "        if len(list_codes) >= 2:              \n",
    "            preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!       \n",
    "\n",
    "\n",
    "    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos journal:\n",
    "if string_journal==\"\": \n",
    "    preselection_df1 = preselection_df111\n",
    "else:    \n",
    "    preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######### preselection by plos field:\n",
    "if string_plos_field==\"\": \n",
    "    preselection_df2 = preselection_df1\n",
    "else:    \n",
    "    preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### i select only young ref!\n",
    "time_window = 1\n",
    "preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "print (\" size of preselection3 (only young ref):\",preselection_df3.shape, \"   # unique plos papers:\", len(preselection_df3.paper_UT.unique()))\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # plos papers:\",len(preselection_df3.paper_UT.unique()),\\\n",
    "#        \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "#### ojo!!! bins for plos on unique plos records\n",
    "df_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "#list_q=[0.3,0.6,.9,.99,1]\n",
    "list_q=[0.1,0.6,.9,1]\n",
    "\n",
    "quantiles=sorted(list(df_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (quantiles)   \n",
    "\n",
    "lista_bins_plos_citations=[]\n",
    "old_value=0\n",
    "for item in quantiles:\n",
    "    pair=[old_value, int(item[1])]\n",
    "    lista_bins_plos_citations.append(pair)\n",
    "    old_value = int(item[1])\n",
    "    \n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "    \n",
    "   \n",
    "list_subsets_data = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig = tools.make_subplots(rows=1, cols=4, vertical_spacing=0.001, horizontal_spacing=0.001)\n",
    "cont_columns =1\n",
    "\n",
    "list_median =[]\n",
    "list_mean =[]\n",
    "list_x_pos = []\n",
    "list_annot_x = []\n",
    "lista_x_series = []\n",
    "\n",
    "if string_top_bottom_plos  == 'top' :\n",
    "    \n",
    "    color_hist =' #4d88ff'\n",
    "    left_space = 200\n",
    "    \n",
    "    text_title = 'Top 10% papers'\n",
    "    list_x_pos = [132,165,130,173]\n",
    "    \n",
    "    text_y_axis=\"\"#Number citations <br>young references\"\n",
    "    \n",
    "    #### first, i select only the top-plos papers\n",
    "    item = lista_bins_plos_citations[-1]\n",
    "    \n",
    "    tot_minimo = item[0]\n",
    "    tot_maximo = item[1]\n",
    "\n",
    "    selection_df_top = preselection_df3[(preselection_df3['paper_cite_count'] >= tot_minimo)  &  (preselection_df3['paper_cite_count'] < tot_maximo)]\n",
    "\n",
    "  \n",
    "  \n",
    "    print (\"min, max in the top bin:\",selection_df_top.paper_cite_count.min(), selection_df_top.paper_cite_count.max())\n",
    "    \n",
    "    \n",
    "    ### i get 4 bins for the top-plos selection\n",
    "    df_top_plos_only = selection_df_top.drop_duplicates(subset=['paper_UT'])\n",
    "    \n",
    "    list_q=[0.25,.5,.75,1]\n",
    "\n",
    "    quantiles_top=sorted(list(df_top_plos_only['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68    \n",
    "    list_bins_histogr=[]\n",
    "    old_value=selection_df_top.paper_cite_count.min()\n",
    "    for item in quantiles_top:\n",
    "        pair=[old_value, int(item[1])]\n",
    "        list_bins_histogr.append(pair)\n",
    "        old_value = int(item[1])    \n",
    "    \n",
    "    print (string_top_bottom_plos, tot_minimo, tot_maximo, selection_df_top.shape, \" # plos\",   len(selection_df_top.paper_UT.unique()),\\\n",
    "             \" #ref:\",len(selection_df_top.reference_UT.unique()), \"bins within top-plos:\", list_bins_histogr)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "    ## one histogram per bin of top-plos\n",
    "    for item in list_bins_histogr :\n",
    "\n",
    "        minimo = item[0]\n",
    "        maximo = item[1]\n",
    "        \n",
    "        etiqueta =str(item[0])+\"-\"+str(item[1]-1)+\" Citations\"  #str(item).replace(\"]\",'').replace(\"[\",'').replace(\", \",'-')+\" Citations\"\n",
    "        list_annot_x.append(etiqueta)# = ['00 Citations', '01 Citation','02 Citations','03 Citations']\n",
    "\n",
    "        df_selection = selection_df_top[(selection_df_top['paper_cite_count'] >= minimo)  &  (selection_df_top['paper_cite_count'] < maximo)]\n",
    "\n",
    "        y = df_selection['cite_count']\n",
    "        \n",
    "    \n",
    "        lista_x_series.append(y.tolist())\n",
    "        \n",
    "        \n",
    "        \n",
    "        trace1 = go.Histogram(\n",
    "                    y=y,\n",
    "                    name= str(minimo)+\"-\"+str(maximo)+ \" citations, N:\"+str(len(y)),                    \n",
    "                    ybins=dict(\n",
    "                           start=0,\n",
    "                           end=selection_df_top.cite_count.max(),\n",
    "                           size=10),\n",
    "                     marker=dict(\n",
    "                         color=color_hist ),                   \n",
    "            showlegend = False, \n",
    "                   )\n",
    "\n",
    "        fig.append_trace(trace1, 1, cont_columns)\n",
    "\n",
    "        cont_columns +=1\n",
    "\n",
    "        print (\"   bin:\",minimo, maximo, df_selection.shape, df_selection['cite_count'].mean(), df_selection['cite_count'].std())\n",
    "\n",
    "        list_median.append(df_selection.cite_count.median())\n",
    "        list_mean.append(df_selection.cite_count.mean())\n",
    "\n",
    "        list_subsets_data.append(list(df_selection.cite_count.values))\n",
    "\n",
    "\n",
    "elif string_top_bottom_plos  == 'bottom' :\n",
    "    \n",
    "    left_space = 350\n",
    "    \n",
    "    text_title = 'Bottom 10% papers'\n",
    "    list_annot_x = ['0 Citations', '1 Citation','2 Citations','3 Citations']\n",
    "    list_x_pos = [40,80,130,140]\n",
    "    \n",
    "    text_y_axis=\"Number of citations <br>of young references\"\n",
    "    \n",
    "    color_hist = '#ff4d4d'\n",
    "    \n",
    "    ### first i select the bottom plos papers\n",
    "    item = lista_bins_plos_citations[0]\n",
    "    \n",
    "    minimo = 0\n",
    "    maximo = 4\n",
    "\n",
    "    selection_df_bottom = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "\n",
    "    print (string_top_bottom_plos, minimo, maximo, \"  unique values in select\", selection_df_bottom['paper_cite_count'].unique(), selection_df_bottom.shape, \\\n",
    "           \" # plos:\",len(selection_df_bottom.paper_UT.unique()), \" # ref\",len(selection_df_bottom.reference_UT.unique()))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### then i get one histogram per value\n",
    "    for  value in range(4):#number of ciations of papers 0, 1,2,3, 4: #####################\n",
    "\n",
    "                \n",
    "\n",
    "        df_selection = selection_df_bottom[selection_df_bottom['paper_cite_count'] == value]   \n",
    "        y = df_selection['cite_count']\n",
    "\n",
    "        lista_x_series.append(y.tolist())\n",
    "        \n",
    "        trace1 = go.Histogram(\n",
    "                    y=y,\n",
    "                   # name= str(value)+ \" citations, N:\"+str(len(y)),\n",
    "                    ybins=dict(\n",
    "                           start=0,\n",
    "                           end=selection_df_bottom.cite_count.max(),\n",
    "                           size=10),\n",
    "                    marker=dict(\n",
    "                        color=color_hist,),                   \n",
    "            showlegend = False, \n",
    "           \n",
    "                )\n",
    "\n",
    "        fig.append_trace(trace1, 1, cont_columns)\n",
    "        cont_columns +=1\n",
    "\n",
    "        print (\"   bin:\",value, df_selection.shape, df_selection['cite_count'].mean(), df_selection['cite_count'].std())\n",
    "\n",
    "        \n",
    "        list_median.append(df_selection.cite_count.median())\n",
    "        list_mean.append(df_selection.cite_count.mean())\n",
    "        list_subsets_data.append(list(df_selection.cite_count.values))\n",
    "        \n",
    "#fig.layout.yaxis.update({'title':'Number of citations of young references'}) \n",
    "#fig.layout.xaxis3.update({'title':'Number of citations of papers') \n",
    "\n",
    "\n",
    "\n",
    "fig.layout.yaxis1.update({'range':[-5,450]})  \n",
    "fig.layout.yaxis2.update({'range':[-5,450]})  \n",
    "fig.layout.yaxis3.update({'range':[-5,450]})  \n",
    "fig.layout.yaxis4.update({'range':[-5,450]})  \n",
    "        \n",
    "    \n",
    "     \n",
    "    \n",
    "fig.layout.yaxis2.update({'showticklabels':False})     \n",
    "fig.layout.yaxis3.update({'showticklabels':False})     \n",
    "fig.layout.yaxis4.update({'showticklabels':False})        \n",
    "\n",
    "\n",
    "fig.layout.yaxis1.update({'gridwidth':5,'showgrid':True})       \n",
    "fig.layout.yaxis2.update({'gridwidth':5,'showgrid':True})     \n",
    "fig.layout.yaxis3.update({'gridwidth':5,'showgrid':True})     \n",
    "fig.layout.yaxis4.update({'gridwidth':5,'showgrid':True})  \n",
    "\n",
    "\n",
    "fig.layout.xaxis1.update({'showgrid':False})       \n",
    "fig.layout.xaxis2.update({'showgrid':False})     \n",
    "fig.layout.xaxis3.update({'showgrid':False})     \n",
    "fig.layout.xaxis4.update({'showgrid':False})   \n",
    "\n",
    "\n",
    "# layout = go.Layout(\n",
    "#     xaxis=dict( ticks='', showgrid=False, zeroline=False, nticks=20 ),\n",
    "\n",
    "\n",
    "\n",
    "fig.layout.update(\n",
    "    {\n",
    "   \n",
    "     'shapes': [\n",
    "        # Line Horizontal --  median\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_median[0],\n",
    "            'x1': list_x_pos[0]+7,\n",
    "            'y1': list_median[0],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                \n",
    "            },\n",
    "        },\n",
    "      # Line Horizontal --  median\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x2',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_median[1],\n",
    "            'x1': list_x_pos[1]+7,\n",
    "            'y1': list_median[1],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "               \n",
    "            },\n",
    "        },\n",
    "        # Line Horizontal --  median\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x3',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_median[2],\n",
    "            'x1': list_x_pos[2]+15,\n",
    "            'y1': list_median[2],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                \n",
    "            },\n",
    "        },\n",
    "      # Line Horizontal --  median\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x4',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_median[3],\n",
    "            'x1': list_x_pos[3]+15,\n",
    "            'y1': list_median[3],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                \n",
    "            },\n",
    "        },  \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "          # Line Horizontal --  mean\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_mean[0],\n",
    "            'x1': list_x_pos[0]+7,\n",
    "            'y1': list_mean[0],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                'dash': 'dot',\n",
    "            },\n",
    "        },\n",
    "      # Line Horizontal --  mean\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x2',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_mean[1],\n",
    "            'x1': list_x_pos[1]+7,\n",
    "            'y1': list_mean[1],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                'dash': 'dot',\n",
    "            },\n",
    "        },\n",
    "        # Line Horizontal --  mean\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x3',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_mean[2],\n",
    "            'x1': list_x_pos[2]+15,\n",
    "            'y1': list_mean[2],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                'dash': 'dot',\n",
    "            },\n",
    "        },\n",
    "      # Line Horizontal --  mean\n",
    "        {\n",
    "            'type': 'line',\n",
    "            'xref': 'x4',\n",
    "            'yref': 'y',\n",
    "            'x0': 0,\n",
    "            'y0': list_mean[3],\n",
    "            'x1': list_x_pos[3]+15,\n",
    "            'y1': list_mean[3],\n",
    "            'line': {\n",
    "                'color': 'rgb(0,0,0)',\n",
    "                'width': 8.5,\n",
    "                'dash': 'dot',\n",
    "            },\n",
    "        },  \n",
    "     \n",
    "     \n",
    "     \n",
    "     ]  ,\n",
    "        \n",
    "     'annotations' : [   \n",
    "         dict(  # title\n",
    "            x=.5,\n",
    "            y=1.3,  #1.15,\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            text=text_title,\n",
    "            showarrow=False,  \n",
    "            font=dict(               \n",
    "                size=65,),\n",
    "           ),    \n",
    "         \n",
    "         \n",
    "         dict(  # this is for the xaxis label\n",
    "            x=.5,\n",
    "            y=-.25,\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            text=\"Number of citations\",\n",
    "            showarrow=False,  \n",
    "            font=dict(               \n",
    "                size=65,),\n",
    "           ),    \n",
    "         \n",
    "         dict(  # this is for the yaxis label\n",
    "            x=-.2,\n",
    "            y=.5,\n",
    "            xref='paper',\n",
    "            yref='paper',\n",
    "            text=text_y_axis,\n",
    "            showarrow=False, \n",
    "            textangle=-90,\n",
    "            font=dict(               \n",
    "                size=65,),\n",
    "           ),    \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "          dict(\n",
    "            x=list_x_pos[0]/2.,\n",
    "            y=1.1, #-.15,\n",
    "            xref='x',\n",
    "            yref='paper',\n",
    "            text=list_annot_x[0],\n",
    "            showarrow=False,  \n",
    "              font=dict(               \n",
    "                size=40,),\n",
    "           ),    \n",
    "         \n",
    "          dict(\n",
    "            x=list_x_pos[1]/2.,\n",
    "            y=1.1, #-.15,\n",
    "            xref='x2',\n",
    "            yref='paper',\n",
    "            text=list_annot_x[1],\n",
    "            showarrow=False,\n",
    "              font=dict(               \n",
    "                size=40,),\n",
    "           ),    \n",
    "         \n",
    "          dict(\n",
    "            x=list_x_pos[2]/2.,\n",
    "            y=1.1, #-.15,\n",
    "            xref='x3',\n",
    "            yref='paper',\n",
    "            text=list_annot_x[2],\n",
    "            showarrow=False,\n",
    "              font=dict(               \n",
    "                size=40,),\n",
    "           ),    \n",
    "         \n",
    "          dict(\n",
    "            x=list_x_pos[3]/2.,\n",
    "            y=1.1, #-.15,\n",
    "            xref='x4',\n",
    "            yref='paper',\n",
    "            text=list_annot_x[3],\n",
    "            showarrow=False,\n",
    "              font=dict(               \n",
    "                size=40,),\n",
    "           ),    \n",
    "         \n",
    "         \n",
    "         \n",
    "         \n",
    "       ]\n",
    "                 \n",
    "                 \n",
    "                 \n",
    "    } )\n",
    "\n",
    "\n",
    "print (\"median:\",list_median)\n",
    "\n",
    "font_gral=35   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral \n",
    "      \n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=left_space,\n",
    "       # r=50,\n",
    "        b=200,\n",
    "        t=250,        \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_multiple_vertical_histogr' ,image_width=1800, image_height=1200, filename='../plots/testing_multiple_vertical_histogr.html', validate=True)\n",
    "\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "###############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list_colors = ['#67a9cf','#91cf60','#f1a340','#d7191c']\n",
    "\n",
    "#list_colors = ['#a6611a','#dfc27d','#80cdc1','#018571']\n",
    "# if string_top_bottom_plos  == 'top' :\n",
    "#     list_colors = ['#bdd7e7','#6baed6','#3182bd','#08519c']\n",
    "# elif string_top_bottom_plos  == 'bottom' :\n",
    "#     list_colors = ['#fdcc8a','#fc8d59','#e34a33','#b30000']\n",
    "\n",
    "\n",
    "############  Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(lista_x_series, list_annot_x, bin_size=10,  show_hist=False,  show_rug=False, show_curve=True, colors =  list_colors)#,curve_type='lognormal')#, curve_type='kde', show_rug=False, bin_size=.2, , show_hist=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################  Layout\n",
    "\n",
    "\n",
    "fig['layout']['xaxis']['range'] = [1,500]  #[.001,3.5]  ## because the axis is log, the range needs to be in log units too! \n",
    "  \n",
    "#fig['layout']['xaxis']['tickvals'] =  [1, 10, 100, 1000,2000]   \n",
    " \n",
    "fig['layout']['xaxis']['title'] = \"Number of citations\"\n",
    "fig['layout']['yaxis']['title'] = \"Probability density\"\n",
    "fig['layout']['title'] = string_top_bottom_plos.capitalize()+\" 10% papers\"    \n",
    "      \n",
    "\n",
    "    \n",
    "    \n",
    "font_gral=50   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral \n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral \n",
    "\n",
    "fig['layout']['legend']=dict( x=0.75, y=.9,  ) #font=dict(size=20) \n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=250,\n",
    "        r=50,\n",
    "        b=150,\n",
    "        t=150,  \n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Plot!\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='multiple_distplot' ,image_width=1800, image_height=1200, filename='../plots/multiple_distplot.html', validate=True)\n",
    "\n",
    "### NOTE:  this figures is requires some touch up from the plotly platform (lines width cant be customized from here for some reason)\n",
    "\n",
    "##################################################3\n",
    "###################################################3\n",
    "\n",
    "#### run old Figure 4b cell first to get the list of values!!!!\n",
    "\n",
    "\n",
    "list_colors = ['#67a9cf','#91cf60','#f1a340','#d7191c']\n",
    "\n",
    "data = []\n",
    "for i in  range(len(lista_x_series)):\n",
    "    \n",
    "    a =  lista_x_series[i]\n",
    "    c = list_colors[i]\n",
    "    \n",
    "    ### equivalent to normalized cumulative distribution  !!!\n",
    "    x = np.sort(a)\n",
    "    y = np.linspace(0, 1, len(a), endpoint=False)\n",
    "    \n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        mode = 'lines',\n",
    "        name = list_annot_x[i],\n",
    "        marker = dict(         \n",
    "                color = c,\n",
    "                line = dict(\n",
    "                        width = 50,\n",
    "                        color = c\n",
    "                )\n",
    "            )\n",
    "    )\n",
    "\n",
    "    data.append(trace0)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "layout = Layout(    \n",
    "\n",
    "\n",
    "    xaxis=dict(\n",
    "        title= \"Number of citations received\",   \n",
    "#         titlefont=dict(\n",
    "#             #size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             #family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "         #range = [1, 500],#range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "         type='log',            \n",
    "        ),\n",
    "    \n",
    "    yaxis=dict(\n",
    "        title='Probability density',\n",
    "        #type='log',\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             #family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "        ),                \n",
    "\n",
    "\n",
    "\n",
    ")       \n",
    "\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "    \n",
    "fig['layout']['xaxis']['title'] = \"Number of citations\"\n",
    "fig['layout']['yaxis']['title'] = \"Cumulative probability density\"\n",
    "fig['layout']['title'] = string_top_bottom_plos.capitalize()+\" 10% papers\"    \n",
    "      \n",
    "\n",
    "    \n",
    "    \n",
    "font_gral=50   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral +10\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral +10\n",
    "\n",
    "fig['layout']['legend']=dict( x=0.75, y=.9,  ) #font=dict(size=20) \n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=250,\n",
    "        r=50,\n",
    "        b=150,\n",
    "        t=150,  \n",
    "    )    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Plot!\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='histogram_num_cit_ref_'+string_top_bottom_plos+str(10)+'_'+str(years[0]) ,image_width=1800, image_height=1200, filename='../plots/histogram.html', validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/multiple_distplot.html'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/histogram.html'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### run old Figure 4b cell first to get the list of values!!!!\n",
    "\n",
    "\n",
    "list_colors = ['#67a9cf','#91cf60','#f1a340','#d7191c']\n",
    "\n",
    "data = []\n",
    "for i in  range(len(lista_x_series)):\n",
    "    \n",
    "    a =  lista_x_series[i]\n",
    "    c = list_colors[i]\n",
    "    \n",
    "    ### equivalent to normalized cumulative distribution  !!!\n",
    "    x = np.sort(a)\n",
    "    y = np.linspace(0, 1, len(a), endpoint=False)\n",
    "    \n",
    "    \n",
    "    trace0 = go.Scatter(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        mode = 'lines',\n",
    "        name = list_annot_x[i],\n",
    "        marker = dict(         \n",
    "                color = c,\n",
    "                line = dict(\n",
    "                        width = 50,\n",
    "                        color = c\n",
    "                )\n",
    "            )\n",
    "    )\n",
    "\n",
    "    data.append(trace0)\n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "layout = Layout(    \n",
    "\n",
    "\n",
    "    xaxis=dict(\n",
    "        title= \"Number of citations received\",   \n",
    "#         titlefont=dict(\n",
    "#             #size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             #family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "         #range = [1, 500],#range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "         type='log',            \n",
    "        ),\n",
    "    \n",
    "    yaxis=dict(\n",
    "        title='Probability density',\n",
    "        #type='log',\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             #family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "        ),                \n",
    "\n",
    "\n",
    "\n",
    ")       \n",
    "\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "    \n",
    "fig['layout']['xaxis']['title'] = \"Number of citations\"\n",
    "fig['layout']['yaxis']['title'] = \"Cumulative probability density\"\n",
    "fig['layout']['title'] = string_top_bottom_plos.capitalize()+\" 10% papers\"    \n",
    "      \n",
    "\n",
    "    \n",
    "    \n",
    "font_gral=50   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral +10\n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral +10\n",
    "\n",
    "fig['layout']['legend']=dict( x=0.75, y=.9,  ) #font=dict(size=20) \n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=250,\n",
    "        r=50,\n",
    "        b=150,\n",
    "        t=150,  \n",
    "    )    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Plot!\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='histogram_num_cit_ref_'+string_top_bottom_plos+str(10)+'_'+str(years[0]) ,image_width=1800, image_height=1200, filename='../plots/histogram.html', validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/histogram.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "\n",
    "# a= lista_x_series[0]\n",
    "\n",
    "# stats.cumfreq(a, numbins=100)\n",
    "\n",
    "\n",
    "# result = stats.cumfreq(a, numbins=100)\n",
    "# y = result.cumcount\n",
    "# # Calculate space of values for x\n",
    "# x = result.lowerlimit + np.linspace(0, result.binsize*result.cumcount.size, result.cumcount.size)\n",
    "\n",
    "\n",
    "\n",
    "# trace0 = go.Scatter(\n",
    "#     x = x,\n",
    "#     y = y,\n",
    "#     mode = 'lines',\n",
    "#     name = 'lines'\n",
    "# )\n",
    "\n",
    "# data = [trace0]\n",
    "# fig = dict(data=data)#, layout=layout)\n",
    "# offline.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/notebooks/temp-plot.html'"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = []\n",
    "# for a in  lista_x_series:        \n",
    "    \n",
    "#     trace0 = go.Histogram(\n",
    "#                 x=a,\n",
    "#                 histnorm='probability',\n",
    "#                 cumulative=dict(enabled=True)\n",
    "#             )\n",
    "    \n",
    "\n",
    "#     data.append(trace0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# layout = Layout(    \n",
    "\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= \"Number of citations received\",   \n",
    "# #         titlefont=dict(\n",
    "# #             #size=font_axes,\n",
    "# #             color='black'),  \n",
    "# #         tickfont=dict(   \n",
    "# #             #family=font,\n",
    "# #             size=font_ticks,\n",
    "# #             color='black'),\n",
    "#          range = [0, 1000],#range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#          #type='log',            \n",
    "#         ),\n",
    "    \n",
    "#     yaxis=dict(\n",
    "#         title='Probability density',\n",
    "#        # type='log',\n",
    "# #         titlefont=dict(            \n",
    "# #             size=font_axes,\n",
    "# #             color='black'\n",
    "# #         #    color='lightgrey'\n",
    "# #         ),  \n",
    "# #         tickfont=dict(   \n",
    "# #             #family=font,\n",
    "# #             size=font_ticks,\n",
    "# #             color='black'\n",
    "# #         ),\n",
    "#         ),                \n",
    "\n",
    "\n",
    "\n",
    "# )       \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# fig = dict(data=data, layout=layout)\n",
    "# offline.plot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa93d4556d8>]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9140b5080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(np.sort(a), np.linspace(0, 1, len(a), endpoint=False))\n",
    "x = np.sort(a)\n",
    "y = np.linspace(0, 1, len(a), endpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   108.49,    215.98,    323.47,    430.96,    538.45,    645.94,\n",
       "          753.43,    860.92,    968.41,   1075.9 ,   1183.39,   1290.88,\n",
       "         1398.37,   1505.86,   1613.35,   1720.84,   1828.33,   1935.82,\n",
       "         2043.31,   2150.8 ,   2258.29,   2365.78,   2473.27,   2580.76,\n",
       "         2688.25,   2795.74,   2903.23,   3010.72,   3118.21,   3225.7 ,\n",
       "         3333.19,   3440.68,   3548.17,   3655.66,   3763.15,   3870.64,\n",
       "         3978.13,   4085.62,   4193.11,   4300.6 ,   4408.09,   4515.58,\n",
       "         4623.07,   4730.56,   4838.05,   4945.54,   5053.03,   5160.52,\n",
       "         5268.01,   5375.5 ,   5482.99,   5590.48,   5697.97,   5805.46,\n",
       "         5912.95,   6020.44,   6127.93,   6235.42,   6342.91,   6450.4 ,\n",
       "         6557.89,   6665.38,   6772.87,   6880.36,   6987.85,   7095.34,\n",
       "         7202.83,   7310.32,   7417.81,   7525.3 ,   7632.79,   7740.28,\n",
       "         7847.77,   7955.26,   8062.75,   8170.24,   8277.73,   8385.22,\n",
       "         8492.71,   8600.2 ,   8707.69,   8815.18,   8922.67,   9030.16,\n",
       "         9137.65,   9245.14,   9352.63,   9460.12,   9567.61,   9675.1 ,\n",
       "         9782.59,   9890.08,   9997.57,  10105.06,  10212.55,  10320.04,\n",
       "        10427.53,  10535.02,  10642.51,  10750.  ])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import plotly.plotly as py\n",
    "# import plotly.tools as tls\n",
    "\n",
    "# import numpy as np\n",
    "# import matplotlib.mlab as mlab\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# fig = plt.figure()\n",
    "\n",
    "# # example data\n",
    "# mu = 100 # mean of distribution\n",
    "# sigma = 15 # standard deviation of distribution\n",
    "# x = mu + sigma * np.random.randn(10000)\n",
    "\n",
    "# num_bins = 50\n",
    "# # the histogram of the data\n",
    "# n, bins, patches = plt.hist(x, num_bins, normed=1, facecolor='green', alpha=0.5)\n",
    "# # add a 'best fit' line\n",
    "# y = mlab.normpdf(bins, mu, sigma)\n",
    "# plt.plot(bins, y, 'r--')\n",
    "# plt.xlabel('Smarts')\n",
    "# plt.ylabel('Probability')\n",
    "\n",
    "# # Tweak spacing to prevent clipping of ylabel\n",
    "# plt.subplots_adjust(left=0.15)\n",
    "\n",
    "# plotly_fig = tls.mpl_to_plotly( fig )\n",
    "# py.iplot(plotly_fig, filename='histogram-mpl-legend')\n",
    "# # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_annot_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function create_distplot in module plotly.figure_factory._distplot:\n",
      "\n",
      "create_distplot(hist_data, group_labels, bin_size=1.0, curve_type='kde', colors=None, rug_text=None, histnorm='probability density', show_hist=True, show_curve=True, show_rug=True)\n",
      "    BETA function that creates a distplot similar to seaborn.distplot\n",
      "    \n",
      "    The distplot can be composed of all or any combination of the following\n",
      "    3 components: (1) histogram, (2) curve: (a) kernel density estimation\n",
      "    or (b) normal curve, and (3) rug plot. Additionally, multiple distplots\n",
      "    (from multiple datasets) can be created in the same plot.\n",
      "    \n",
      "    :param (list[list]) hist_data: Use list of lists to plot multiple data\n",
      "        sets on the same plot.\n",
      "    :param (list[str]) group_labels: Names for each data set.\n",
      "    :param (list[float]|float) bin_size: Size of histogram bins.\n",
      "        Default = 1.\n",
      "    :param (str) curve_type: 'kde' or 'normal'. Default = 'kde'\n",
      "    :param (str) histnorm: 'probability density' or 'probability'\n",
      "        Default = 'probability density'\n",
      "    :param (bool) show_hist: Add histogram to distplot? Default = True\n",
      "    :param (bool) show_curve: Add curve to distplot? Default = True\n",
      "    :param (bool) show_rug: Add rug to distplot? Default = True\n",
      "    :param (list[str]) colors: Colors for traces.\n",
      "    :param (list[list]) rug_text: Hovertext values for rug_plot,\n",
      "    :return (dict): Representation of a distplot figure.\n",
      "    \n",
      "    Example 1: Simple distplot of 1 data set\n",
      "    ```\n",
      "    import plotly.plotly as py\n",
      "    from plotly.figure_factory import create_distplot\n",
      "    \n",
      "    hist_data = [[1.1, 1.1, 2.5, 3.0, 3.5,\n",
      "                  3.5, 4.1, 4.4, 4.5, 4.5,\n",
      "                  5.0, 5.0, 5.2, 5.5, 5.5,\n",
      "                  5.5, 5.5, 5.5, 6.1, 7.0]]\n",
      "    \n",
      "    group_labels = ['distplot example']\n",
      "    \n",
      "    fig = create_distplot(hist_data, group_labels)\n",
      "    \n",
      "    url = py.plot(fig, filename='Simple distplot', validate=False)\n",
      "    ```\n",
      "    \n",
      "    Example 2: Two data sets and added rug text\n",
      "    ```\n",
      "    import plotly.plotly as py\n",
      "    from plotly.figure_factory import create_distplot\n",
      "    \n",
      "    # Add histogram data\n",
      "    hist1_x = [0.8, 1.2, 0.2, 0.6, 1.6,\n",
      "               -0.9, -0.07, 1.95, 0.9, -0.2,\n",
      "               -0.5, 0.3, 0.4, -0.37, 0.6]\n",
      "    hist2_x = [0.8, 1.5, 1.5, 0.6, 0.59,\n",
      "               1.0, 0.8, 1.7, 0.5, 0.8,\n",
      "               -0.3, 1.2, 0.56, 0.3, 2.2]\n",
      "    \n",
      "    # Group data together\n",
      "    hist_data = [hist1_x, hist2_x]\n",
      "    \n",
      "    group_labels = ['2012', '2013']\n",
      "    \n",
      "    # Add text\n",
      "    rug_text_1 = ['a1', 'b1', 'c1', 'd1', 'e1',\n",
      "          'f1', 'g1', 'h1', 'i1', 'j1',\n",
      "          'k1', 'l1', 'm1', 'n1', 'o1']\n",
      "    \n",
      "    rug_text_2 = ['a2', 'b2', 'c2', 'd2', 'e2',\n",
      "          'f2', 'g2', 'h2', 'i2', 'j2',\n",
      "          'k2', 'l2', 'm2', 'n2', 'o2']\n",
      "    \n",
      "    # Group text together\n",
      "    rug_text_all = [rug_text_1, rug_text_2]\n",
      "    \n",
      "    # Create distplot\n",
      "    fig = create_distplot(\n",
      "        hist_data, group_labels, rug_text=rug_text_all, bin_size=.2)\n",
      "    \n",
      "    # Add title\n",
      "    fig['layout'].update(title='Dist Plot')\n",
      "    \n",
      "    # Plot!\n",
      "    url = py.plot(fig, filename='Distplot with rug text', validate=False)\n",
      "    ```\n",
      "    \n",
      "    Example 3: Plot with normal curve and hide rug plot\n",
      "    ```\n",
      "    import plotly.plotly as py\n",
      "    from plotly.figure_factory import create_distplot\n",
      "    import numpy as np\n",
      "    \n",
      "    x1 = np.random.randn(190)\n",
      "    x2 = np.random.randn(200)+1\n",
      "    x3 = np.random.randn(200)-1\n",
      "    x4 = np.random.randn(210)+2\n",
      "    \n",
      "    hist_data = [x1, x2, x3, x4]\n",
      "    group_labels = ['2012', '2013', '2014', '2015']\n",
      "    \n",
      "    fig = create_distplot(\n",
      "        hist_data, group_labels, curve_type='normal',\n",
      "        show_rug=False, bin_size=.4)\n",
      "    \n",
      "    url = py.plot(fig, filename='hist and normal curve', validate=False)\n",
      "    \n",
      "    Example 4: Distplot with Pandas\n",
      "    ```\n",
      "    import plotly.plotly as py\n",
      "    from plotly.figure_factory import create_distplot\n",
      "    import numpy as np\n",
      "    import pandas as pd\n",
      "    \n",
      "    df = pd.DataFrame({'2012': np.random.randn(200),\n",
      "                       '2013': np.random.randn(200)+1})\n",
      "    py.iplot(create_distplot([df[c] for c in df.columns], df.columns),\n",
      "                             filename='examples/distplot with pandas',\n",
      "                             validate=False)\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ff.create_distplot)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_data_for_plotting(df_merged, v1_string, years, string_filtering_x, string_references_age, string_isolated_ref, string_self_ref, string_code_categ, string_journal, string_plos_field):\n",
    "    \"\"\"\n",
    "    This function selects a subset of the total dataset acording to a set of parameters.\n",
    "\n",
    "            \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_merged : pandas dataframe\n",
    "        Original dataframe to select from\n",
    "        \n",
    "    v1_string : str\n",
    "        Main variable \n",
    "        \n",
    "    years : list of int\n",
    "        List of selected years\n",
    "        \n",
    "    string_filtering_x : str\n",
    "        Variable for heatmap plot\n",
    "        \n",
    "    string_references_age : str\n",
    "        Selected 'young' or 'old' or 'all' references for the analysis\n",
    "        \n",
    "    string_isolated_ref : str\n",
    "        Selected isolated (1) or group (0) or 'all' references for the analysis\n",
    "        \n",
    "    string_self_ref : str\n",
    "         Selected self references (1) or not self references (0) or 'all' references for the analysis\n",
    "         \n",
    "    string_code_categ : str\n",
    "        Selected PLos category: from 0 to 10 or multiple ones\n",
    "        \n",
    "    string_journal : int\n",
    "        Selected PLoS journal\n",
    "        \n",
    "    string_plos_field : str\n",
    "        Selected PLoS field\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dataframe\n",
    "        Selected subset of rows from the original dataset (all columns).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "    ##### preselection by plos publication year\n",
    "    print (years)\n",
    "    preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "    print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### i remove self-citations\n",
    "    if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "        preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "        if string_self_ref ==0:\n",
    "            string_self_ref = \", no self-cit\"\n",
    "        elif string_self_ref ==1:\n",
    "            string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by isolated or group references:\n",
    "    if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "        preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "        if string_isolated_ref ==0:\n",
    "            string_isolated_ref = \", group ref\"\n",
    "        elif string_isolated_ref ==1:\n",
    "            string_isolated_ref = \", isolated ref\"\n",
    "    else:    \n",
    "        preselection_df0 = preselection_df   \n",
    "        print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos ONE subject category:\n",
    "    if string_code_categ==\"\": \n",
    "        preselection_df111 = preselection_df0\n",
    "    else:    \n",
    "        if \" \" not in string_code_categ:  # to include one single category\n",
    "            preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "            string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "        else:  # if multiple codes-categories\n",
    "            list_codes = string_code_categ.split(\" \")\n",
    "            print (list_codes)\n",
    "\n",
    "            if len(list_codes) >= 2:              \n",
    "                preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "            string_code_categ = \"\" \n",
    "            for code in list_codes:\n",
    "                string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "        print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos journal:\n",
    "    if string_journal==\"\": \n",
    "        preselection_df1 = preselection_df111\n",
    "    else:    \n",
    "        preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "    print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos field:\n",
    "    if string_plos_field==\"\": \n",
    "        preselection_df2 = preselection_df1\n",
    "    else:    \n",
    "        preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "    print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "    preselection_df3 = preselection_df2\n",
    "\n",
    "    if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "        string_age_selv1_stringection=''\n",
    "\n",
    "        ##### preselection only young/old references:        \n",
    "        if string_references_age == \"young\":\n",
    "            time_window = 1\n",
    "            string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "            preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "            print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "        elif string_references_age == \"old\":\n",
    "            time_window = 10\n",
    "            string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "            preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "            print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "        else:\n",
    "            string_age_selection=\"young&old\"       \n",
    "            print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return preselection_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: (5787630, 34)\n",
      "[2011]\n",
      "size of preselection1 (by plos years): (564251, 34)\n",
      "size of preselection1 (by isolated/group ref): (564251, 34) \n",
      " size of preselection2 (by plos journal): (564251, 34) \n",
      " size of preselection2 (by plos field): (564251, 34) \n",
      "  No preselection by age of references: (564251, 34)\n",
      "\n",
      "Tot # records included: 564251    # number of plos papers: 14351    # unique ref: 357866 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "v1_string =  'cite_count'\n",
    "\n",
    "years= [2011]  #[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017]   #### SELECT THE PLOS YEARS TO BE INCLUDED\n",
    "\n",
    "string_filtering_x = 'paper_cite_count'   \n",
    "\n",
    "string_references_age = \"\"     # AGE OF REFERENCES TO BE INCLUDED:  'young'     'old'   or   \"\" for including all\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "string_journal=\"\"#   PLOS ONE\"\n",
    "string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "preselection_df3 = select_data_for_plotting(df_merged, v1_string, years, string_filtering_x, string_references_age, string_isolated_ref, string_self_ref, string_code_categ, string_journal, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "####################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "lista_x_series = []\n",
    "    \n",
    "data = []   \n",
    "list_subsets_data = []\n",
    "\n",
    "\n",
    "list_median =[]\n",
    "list_mean =[]\n",
    "list_x_pos = []\n",
    "list_annot_x = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bottom 0 4   unique values in select [2 1 0 3] (42202, 34)  # plos: 1278  # ref 33571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/testing_multiple_vertical_histogr.html'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # ADDING SMOOTHING BY KERNEL-DENSITY-ESTIMATOR using distplot\n",
    "\n",
    "# ####  NEW FIGURE 4B AND 4c:  histograms  NUMBER OF CIATIONS OF A PAPER VS # CITATIONS OF ITS YOUNG REFERENCES  separating top and bottom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_top_bottom_plos  = 'bottom'   # top or bottom (plos papers)\n",
    "      \n",
    "\n",
    "# years=[2011]      #[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# #### ojo!!! bins for plos on unique plos records\n",
    "# df_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "# #list_q=[0.3,0.6,.9,.99,1]\n",
    "# list_q=[0.1,0.6,.9,1]\n",
    "\n",
    "# quantiles=sorted(list(df_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# # print (quantiles)   \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "# # print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "   \n",
    "# lista_x_series = []\n",
    "    \n",
    "# data = []   \n",
    "# list_subsets_data = []\n",
    "\n",
    "\n",
    "# list_median =[]\n",
    "# list_mean =[]\n",
    "# list_x_pos = []\n",
    "# list_annot_x = []\n",
    "\n",
    "# if string_top_bottom_plos  == 'top' :\n",
    "    \n",
    "#     #color_hist =' #4d88ff'\n",
    "#     left_space = 200\n",
    "    \n",
    "#     text_title = 'Top 10% papers'\n",
    "  \n",
    "    \n",
    "#     text_y_axis=\"\"#Number citations <br>young references\"\n",
    "    \n",
    "#     #### first, i select only the top-plos papers\n",
    "#     item = lista_bins_plos_citations[-1]\n",
    "    \n",
    "#     tot_minimo = item[0]\n",
    "#     tot_maximo = item[1]\n",
    "\n",
    "#     selection_df_top = preselection_df3[(preselection_df3['paper_cite_count'] >= tot_minimo)  &  (preselection_df3['paper_cite_count'] < tot_maximo)]\n",
    "\n",
    "  \n",
    "  \n",
    "#     print (\"min, max in the top bin:\",selection_df_top.paper_cite_count.min(), selection_df_top.paper_cite_count.max())\n",
    "    \n",
    "    \n",
    "#     ### i get 4 bins for the top-plos selection\n",
    "#     df_top_plos_only = selection_df_top.drop_duplicates(subset=['paper_UT'])\n",
    "    \n",
    "#     list_q=[0.25,.5,.75,1]\n",
    "\n",
    "#     quantiles_top=sorted(list(df_top_plos_only['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68    \n",
    "#     list_bins_histogr=[]\n",
    "#     old_value=selection_df_top.paper_cite_count.min()\n",
    "#     for item in quantiles_top:\n",
    "#         pair=[old_value, int(item[1])]\n",
    "#         list_bins_histogr.append(pair)\n",
    "#         old_value = int(item[1])    \n",
    "    \n",
    "#     print (string_top_bottom_plos, tot_minimo, tot_maximo, selection_df_top.shape, \" # plos\",   len(selection_df_top.paper_UT.unique()),\\\n",
    "#              \" #ref:\",len(selection_df_top.reference_UT.unique()), \"bins within top-plos:\", list_bins_histogr)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "#     ## one histogram per bin of top-plos\n",
    "#     for item in list_bins_histogr :\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]\n",
    "        \n",
    "#         etiqueta =str(item[0])+\"-\"+str(item[1]-1)+\" Citations\"  #str(item).replace(\"]\",'').replace(\"[\",'').replace(\", \",'-')+\" Citations\"\n",
    "#         list_annot_x.append(etiqueta)# = ['00 Citations', '01 Citation','02 Citations','03 Citations']\n",
    "\n",
    "        \n",
    "#         df_selection = selection_df_top[(selection_df_top['paper_cite_count'] >= minimo)  &  (selection_df_top['paper_cite_count'] < maximo)]\n",
    "\n",
    "#         x = df_selection['cite_count']\n",
    "        \n",
    "        \n",
    "#         lista_x_series.append(x.tolist())#(x[:1000])\n",
    "        \n",
    "#         trace1 = go.Histogram(\n",
    "#                     x=x,\n",
    "#                     histnorm='probability',\n",
    "#                     name= str(minimo)+\"-\"+str(maximo)+ \" citations, N:\"+str(len(x)),                    \n",
    "#                     xbins=dict(\n",
    "#                            start=0,\n",
    "#                            end=selection_df_top.cite_count.max(),\n",
    "#                            size=10),\n",
    "# #                      marker=dict(\n",
    "# #                          color=color_hist ),                   \n",
    "#             #showlegend = False, \n",
    "#                    )\n",
    "\n",
    "#         data.append(trace1)  \n",
    "#         print (\"   bin:\",minimo, maximo, df_selection.shape, df_selection['cite_count'].mean(), df_selection['cite_count'].std())\n",
    "\n",
    "        \n",
    "# elif string_top_bottom_plos  == 'bottom' :\n",
    "    \n",
    "  \n",
    "    \n",
    "#     text_title = 'Bottom 10% papers'\n",
    "#     list_annot_x = ['0 Citations', '1 Citation','2 Citations','3 Citations']\n",
    "#     list_x_pos = [40,80,130,140]\n",
    "    \n",
    "#     text_y_axis=\"Number of citations <br>of young references\"\n",
    "    \n",
    "   \n",
    "    \n",
    "#     ### first i select the bottom plos papers\n",
    "#     item = lista_bins_plos_citations[0]\n",
    "    \n",
    "#     minimo = 0\n",
    "#     maximo = 4\n",
    "\n",
    "#     selection_df_bottom = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "\n",
    "#     print (string_top_bottom_plos, minimo, maximo, \"  unique values in select\", selection_df_bottom['paper_cite_count'].unique(), selection_df_bottom.shape, \\\n",
    "#            \" # plos:\",len(selection_df_bottom.paper_UT.unique()), \" # ref\",len(selection_df_bottom.reference_UT.unique()))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     for  value in range(4):  #number of ciations of papers 0, 1,2,3, 4: #####################\n",
    "\n",
    "\n",
    "#         df_selection = selection_df_bottom[selection_df_bottom['paper_cite_count'] == value]   \n",
    "    \n",
    "#         etiqueta =str(value)+\" Citations\"  #str(item).replace(\"]\",'').replace(\"[\",'').replace(\", \",'-')+\" Citations\"\n",
    "#         list_annot_x.append(etiqueta)# = ['00 Citations', '01 Citation','02 Citations','03 Citations']\n",
    "\n",
    "        \n",
    "\n",
    "#         x = df_selection['cite_count']                \n",
    "#         lista_x_series.append(x.tolist())#(x[:1000])\n",
    "        \n",
    "        \n",
    "        \n",
    "#         trace1 = go.Histogram(\n",
    "#                     x=x,\n",
    "#                    # name= str(value)+ \" citations, N:\"+str(len(y)),\n",
    "#                     xbins=dict(\n",
    "#                            start=0,\n",
    "#                            end=selection_df_bottom.cite_count.max(),\n",
    "#                            size=10),\n",
    "#                     marker=dict(\n",
    "#                         color=color_hist,),                   \n",
    "#                 showlegend = False, \n",
    "           \n",
    "#                 )\n",
    "\n",
    "#         data.append(trace1)  \n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# ##################  Layout\n",
    "\n",
    "\n",
    "# font=18\n",
    "# font_ticks=50\n",
    "# font_axes=60\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# layout = Layout(    \n",
    "\n",
    "\n",
    "#     xaxis=dict(\n",
    "#         title= \"Number of citations received\",   \n",
    "#         titlefont=dict(\n",
    "#             size=font_axes,\n",
    "#             color='black'),  \n",
    "#         tickfont=dict(   \n",
    "#             #family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'),\n",
    "#          # range = [minimo, maximo],#range=[np.log10(0.1),np.log10(max_x)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "#          type='log',            \n",
    "#     ),\n",
    "    \n",
    "#     yaxis=dict(\n",
    "#         title='PDF',\n",
    "#         #type='log',\n",
    "#         titlefont=dict(            \n",
    "#             size=font_axes,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(   \n",
    "#             #family=font,\n",
    "#             size=font_ticks,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#     ),                \n",
    "\n",
    "\n",
    "\n",
    "# )       \n",
    "\n",
    "\n",
    "# fig = Figure(data=data, layout=layout)\n",
    "\n",
    "# font_gral=35   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "# # fig['layout']['yaxis']['titlefont']['size'] = font_gral \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# fig['layout']['xaxis']['type'] = 'log'    \n",
    "# fig['layout']['xaxis']['range'] = [.1,4]  #np.log10(1),np.log10(50000)],   # because the axis is log, the range needs to be in log units too!  :(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=150,\n",
    "#        # r=50,\n",
    "#         b=200,\n",
    "#         t=250,        \n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_multiple_vertical_histogr' ,image_width=1800, image_height=1200, filename='../plots/testing_multiple_vertical_histogr.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "# # top 40 856 (5741, 25)  # plos 1164  #ref: 5271 bins within top-plos: [[40, 45], [45, 54], [54, 70], [70, 564]]\n",
    "# # bottom 0 4   unique values in select [1 2 0 3] (2461, 25)  # plos: 859  # ref 2395"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([lista_x_series])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### run previous cell first (the one that gets the old red and blue figure), to get the data groups\n",
    "\n",
    "# # Group data together\n",
    "# hist_data = lista_x_series #[lista_x_series[indx]]\n",
    "\n",
    "# group_labels =  list_annot_x  #[list_annot_x[indx]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Group data together\n",
    "# hist_data = lista_x_series\n",
    "\n",
    "# group_labels = list_annot_x\n",
    "\n",
    "\n",
    "# # Create distplot with custom bin_size\n",
    "# fig = ff.create_distplot(hist_data, group_labels, bin_size=1, show_hist=True,  show_rug=False)#,curve_type='lognormal')#, curve_type='kde', show_rug=False, bin_size=.2, , show_hist=False)\n",
    "\n",
    "\n",
    "\n",
    "# #fig['layout']['xaxis']['type'] = 'log'    \n",
    "# fig['layout']['xaxis']['range'] = [1,2000]  #[.001,3.5]  ## because the axis is log, the range needs to be in log units too! \n",
    "  \n",
    "# #fig['layout']['xaxis']['tickvals'] =  [1, 10, 100, 1000,2000]   \n",
    " \n",
    "# fig['layout']['xaxis']['title'] = \"Number of citations\"\n",
    "# fig['layout']['yaxis']['title'] = \"PDF\"\n",
    "# fig['layout']['title'] = string_top_bottom_plos.capitalize()+\" 10% papers\"    \n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "# font_gral=40   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral \n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral \n",
    "\n",
    "\n",
    "# # fig['layout']['legend']=dict(x=0.5, y=.5,                 \n",
    "# #                font=dict(\n",
    "# #                     #family='sans-serif',\n",
    "# #                     size=40,\n",
    "# #                     #color='#000'\n",
    "# #                     ),\n",
    "# #                 ),\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['legend']=dict( x=0.75, y=.9,  #font=dict(size=20) \n",
    "#                             )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#         r=50,\n",
    "#         b=150,\n",
    "#         t=150,  \n",
    "#     )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# # Plot!\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='multiple_distplot' ,image_width=1800, image_height=1200, filename='../plots/multiple_distplot.html', validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/multiple_distplot.html'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#REFACTORING!!! NEW FIGURE 4B\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_top_bottom_plos  = 'bottom'   # top or bottom (plos papers)\n",
    "      \n",
    "\n",
    "# years=[2011]      #[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "# lista_x_series = []  \n",
    "# list_annot_x = []\n",
    "\n",
    "  \n",
    "\n",
    "# #### bins for plos on unique plos records\n",
    "# df_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "# list_q=[0.1,0.6,.9,1]\n",
    "# quantiles=sorted(list(df_plos['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# if string_top_bottom_plos  == 'top' :                 \n",
    "    \n",
    "#     #### first, i select only the top plos papers\n",
    "#     item = lista_bins_plos_citations[-1]\n",
    "    \n",
    "#     tot_minimo = item[0]\n",
    "#     tot_maximo = item[1]\n",
    "\n",
    "#     selection_df_top = preselection_df3[(preselection_df3['paper_cite_count'] >= tot_minimo)  &  (preselection_df3['paper_cite_count'] < tot_maximo)]\n",
    "  \n",
    "#     print (\"min, max in the top bin:\",selection_df_top.paper_cite_count.min(), selection_df_top.paper_cite_count.max())\n",
    "\n",
    "    \n",
    "    \n",
    "#     ### i get 4 bins for the top-plos selection\n",
    "#     df_top_plos_only = selection_df_top.drop_duplicates(subset=['paper_UT'])\n",
    "    \n",
    "#     list_q=[0.25,.5,.75,1]\n",
    "\n",
    "#     quantiles_top=sorted(list(df_top_plos_only['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68    \n",
    "#     list_bins_histogr=[]\n",
    "#     old_value=selection_df_top.paper_cite_count.min()\n",
    "#     for item in quantiles_top:\n",
    "#         pair=[old_value, int(item[1])]\n",
    "#         list_bins_histogr.append(pair)\n",
    "#         old_value = int(item[1])    \n",
    "    \n",
    "#     print (string_top_bottom_plos, tot_minimo, tot_maximo, selection_df_top.shape, \" # plos\",   len(selection_df_top.paper_UT.unique()),\\\n",
    "#              \" #ref:\",len(selection_df_top.reference_UT.unique()), \"bins within top-plos:\", list_bins_histogr)\n",
    "     \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "#     for item in list_bins_histogr:  ### each one of the series\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]\n",
    "        \n",
    "#         etiqueta =str(item[0])+\"-\"+str(item[1]-1)+\" Citations\"  #str(item).replace(\"]\",'').replace(\"[\",'').replace(\", \",'-')+\" Citations\"\n",
    "#         list_annot_x.append(etiqueta)\n",
    "\n",
    "        \n",
    "#         df_selection = selection_df_top[(selection_df_top['paper_cite_count'] >= minimo)  &  (selection_df_top['paper_cite_count'] < maximo)]\n",
    "\n",
    "#         x = df_selection['cite_count']       \n",
    "        \n",
    "#         lista_x_series.append(x.tolist())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "# elif string_top_bottom_plos  == 'bottom' :  ### first i select the bottom plos papers\n",
    "       \n",
    "#     minimo = 0\n",
    "#     maximo = 4\n",
    "\n",
    "#     selection_df_bottom = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "\n",
    "#     print (string_top_bottom_plos, minimo, maximo, \"  unique values in select\", selection_df_bottom['paper_cite_count'].unique(), selection_df_bottom.shape, \\\n",
    "#            \" # plos:\",len(selection_df_bottom.paper_UT.unique()), \" # ref\",len(selection_df_bottom.reference_UT.unique()))\n",
    "\n",
    "  \n",
    "    \n",
    "#     for  value in range(4):  ### each one of the series\n",
    "\n",
    "#         df_selection = selection_df_bottom[selection_df_bottom['paper_cite_count'] == value]   \n",
    "    \n",
    "#         etiqueta =str(value)+\" Citations\"  #str(item).replace(\"]\",'').replace(\"[\",'').replace(\", \",'-')+\" Citations\"\n",
    "#         list_annot_x.append(etiqueta)# = ['00 Citations', '01 Citation','02 Citations','03 Citations']\n",
    "      \n",
    "\n",
    "#         x = df_selection['cite_count']                \n",
    "#         lista_x_series.append(x.tolist())\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "############  Create distplot with custom bin_size\n",
    "fig = ff.create_distplot(lista_x_series, list_annot_x, bin_size=1, show_hist=False,  show_rug=False)#,curve_type='lognormal')#, curve_type='kde', show_rug=False, bin_size=.2, , show_hist=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################  Layout\n",
    "\n",
    "\n",
    "fig['layout']['xaxis']['range'] = [1,3000]  #[.001,3.5]  ## because the axis is log, the range needs to be in log units too! \n",
    "  \n",
    "#fig['layout']['xaxis']['tickvals'] =  [1, 10, 100, 1000,2000]   \n",
    " \n",
    "fig['layout']['xaxis']['title'] = \"Number of citations\"\n",
    "fig['layout']['yaxis']['title'] = \"PDF\"\n",
    "fig['layout']['title'] = string_top_bottom_plos.capitalize()+\" 10% papers\"    \n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "font_gral=40   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral\n",
    "fig['layout']['xaxis']['titlefont']['size'] = font_gral \n",
    "fig['layout']['yaxis']['titlefont']['size'] = font_gral \n",
    "\n",
    "fig['layout']['legend']=dict( x=0.75, y=.9,  ) #font=dict(size=20) \n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "        r=50,\n",
    "        b=150,\n",
    "        t=150,  \n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "# Plot!\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='multiple_distplot' ,image_width=1800, image_height=1200, filename='../plots/multiple_distplot.html', validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_annot_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list_lists_bottom = list_subsets_data\n",
    "list_lists_top = list_subsets_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_de_listas = list_lists_bottom\n",
    "#lista_de_listas = list_lists_top\n",
    "\n",
    "\n",
    "for i in range(len(lista_de_listas)):\n",
    "    for j in range(len(lista_de_listas)):\n",
    "        if j>=i:\n",
    "\n",
    "            lista1 = lista_de_listas[i]\n",
    "            lista2 = lista_de_listas[j]\n",
    "            print (i, \"lista1:\", np.mean(lista1), np.median(lista1), \"  --  \",j,\"lista2:\", np.mean(lista2), np.median(lista2))\n",
    "            #print (stats.ttest_ind(lista1, lista2))\n",
    "            print (stats.mannwhitneyu(lista1, lista2,  alternative='two-sided'))\n",
    "            #print (stats.ks_2samp(lista1, lista2))\n",
    "        print (\"\")\n",
    "    print (\"\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ## Top 10% plos from 2011, pairwise comparisons:\n",
    "\n",
    "\n",
    "# 0 lista1: 143.691960253 58.0   --   0 lista2: 143.691960253 58.0\n",
    "# MannwhitneyuResult(statistic=612724.5, pvalue=0.99997347346447962)\n",
    "\n",
    "# 0 lista1: 143.691960253 58.0   --   1 lista2: 173.94478951 58.0\n",
    "# MannwhitneyuResult(statistic=783061.0, pvalue=0.30509155685225609)\n",
    "\n",
    "# 0 lista1: 143.691960253 58.0   --   2 lista2: 167.18701482 62.0\n",
    "# MannwhitneyuResult(statistic=748145.0, pvalue=0.046522460815123221)\n",
    "\n",
    "# 0 lista1: 143.691960253 58.0   --   3 lista2: 188.704416761 76.0\n",
    "# MannwhitneyuResult(statistic=841886.0, pvalue=3.6911043571997744e-10)\n",
    "\n",
    "\n",
    "\n",
    "# 1 lista1: 173.94478951 58.0   --   1 lista2: 173.94478951 58.0\n",
    "# MannwhitneyuResult(statistic=1049800.5, pvalue=0.99998228581580129)\n",
    "\n",
    "# 1 lista1: 173.94478951 58.0   --   2 lista2: 167.18701482 62.0\n",
    "# MannwhitneyuResult(statistic=1003489.0, pvalue=0.29638826468990387)\n",
    "\n",
    "# 1 lista1: 173.94478951 58.0   --   3 lista2: 188.704416761 76.0\n",
    "# MannwhitneyuResult(statistic=1134561.5, pvalue=3.1400628052478831e-08)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2 lista1: 167.18701482 62.0   --   2 lista2: 167.18701482 62.0\n",
    "# MannwhitneyuResult(statistic=1003944.5, pvalue=0.9999816825040011)\n",
    "\n",
    "# 2 lista1: 167.18701482 62.0   --   3 lista2: 188.704416761 76.0\n",
    "# MannwhitneyuResult(statistic=1139277.5, pvalue=1.3990818243241034e-05)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3 lista1: 188.704416761 76.0   --   3 lista2: 188.704416761 76.0\n",
    "# MannwhitneyuResult(statistic=1559378.0, pvalue=0.99998683423211232)\n",
    "\n",
    "\n",
    "\n",
    "#####################################3\n",
    "\n",
    "\n",
    "# # #### bottom 10%, 2011, pairwise comparisions:\n",
    "\n",
    "\n",
    "# 0 lista1: 127.786786787 39.0   --   0 lista2: 127.786786787 39.0\n",
    "# MannwhitneyuResult(statistic=55444.5, pvalue=0.99983929552816309)\n",
    "\n",
    "# 0 lista1: 127.786786787 39.0   --   1 lista2: 139.124721604 37.0\n",
    "# MannwhitneyuResult(statistic=74917.0, pvalue=0.959653373168598)\n",
    "\n",
    "# 0 lista1: 127.786786787 39.0   --   2 lista2: 106.874673629 31.0\n",
    "# MannwhitneyuResult(statistic=136165.0, pvalue=0.07443484482714309)\n",
    "\n",
    "# 0 lista1: 127.786786787 39.0   --   3 lista2: 111.549835706 35.0\n",
    "# MannwhitneyuResult(statistic=157425.5, pvalue=0.33572371752587393)\n",
    "\n",
    "\n",
    "\n",
    "# 1 lista1: 139.124721604 37.0   --   1 lista2: 139.124721604 37.0\n",
    "# MannwhitneyuResult(statistic=100800.5, pvalue=0.99989733703558603)\n",
    "\n",
    "# 1 lista1: 139.124721604 37.0   --   2 lista2: 106.874673629 31.0\n",
    "# MannwhitneyuResult(statistic=183489.0, pvalue=0.050964193374854437)\n",
    "\n",
    "# 1 lista1: 139.124721604 37.0   --   3 lista2: 111.549835706 35.0\n",
    "# MannwhitneyuResult(statistic=212165.5, pvalue=0.29155228754341056)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2 lista1: 106.874673629 31.0   --   2 lista2: 106.874673629 31.0\n",
    "# MannwhitneyuResult(statistic=293378.0, pvalue=0.99995391656961385)\n",
    "\n",
    "# 2 lista1: 106.874673629 31.0   --   3 lista2: 111.549835706 35.0\n",
    "# MannwhitneyuResult(statistic=338981.0, pvalue=0.27961116895506288)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3 lista1: 111.549835706 35.0   --   3 lista2: 111.549835706 35.0\n",
    "# MannwhitneyuResult(statistic=416784.5, pvalue=0.99996458433590751)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista_de_listas = list_lists_bottom\n",
    "#lista_de_listas = list_lists_top\n",
    "\n",
    "\n",
    "for i in range(len(list_lists_bottom)):\n",
    "    for j in range(len(list_lists_top)):\n",
    "        if j>=i:\n",
    "\n",
    "            lista1 = list_lists_bottom[i]\n",
    "            lista2 = list_lists_top[j]\n",
    "            print (i, \"lista1:\", np.mean(lista1), np.median(lista1), \"  --  \",j,\"lista2:\", np.mean(lista2), np.median(lista2))\n",
    "            #print (stats.ttest_ind(lista1, lista2))\n",
    "            print (stats.mannwhitneyu(lista1, lista2,  alternative='two-sided'))\n",
    "            #print (stats.ks_2samp(lista1, lista2))\n",
    "        print (\"\")\n",
    "    print (\"\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for lista in list_subsets_data:\n",
    "#     print (np.mean(lista), np.median(lista))\n",
    "    \n",
    "#     new_list_bottom\n",
    "#     new_list_top\n",
    "    \n",
    "for pair in list(itertools.combinations(new_list_top, 2))   :\n",
    "    lista1 = pair[0]\n",
    "    lista2 = pair[1]\n",
    "    print (\"lista1:\", np.mean(lista1), np.median(lista1), \"   lista2:\", np.mean(lista2), np.median(lista2))\n",
    "    print (scipy.stats.ttest_ind(lista1, lista2))\n",
    "    print (scipy.stats.mannwhitneyu(lista1, lista2,  alternative='two-sided'),\"\\n\")\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "### for Bottom 10 data:\n",
    "# lista1: 127.786786787 39.0    lista2: 139.124721604 37.0\n",
    "# Ttest_indResult(statistic=-0.36292559858190454, pvalue=0.71675878809169558)    ==\n",
    "# MannwhitneyuResult(statistic=74917.0, pvalue=0.959653373168598) \n",
    "\n",
    "# lista1: 127.786786787 39.0    lista2: 106.874673629 31.0\n",
    "# Ttest_indResult(statistic=0.87089088386071845, pvalue=0.38400427325859243)   ==\n",
    "# MannwhitneyuResult(statistic=136165.0, pvalue=0.07443484482714309) \n",
    "\n",
    "# lista1: 127.786786787 39.0    lista2: 111.549835706 35.0\n",
    "# Ttest_indResult(statistic=0.81823090335378734, pvalue=0.41338205157425878)   ==\n",
    "# MannwhitneyuResult(statistic=157425.5, pvalue=0.33572371752587393) \n",
    "\n",
    "# lista1: 139.124721604 37.0    lista2: 106.874673629 31.0\n",
    "# Ttest_indResult(statistic=1.357754437966161, pvalue=0.17479413483086129)  ==\n",
    "# MannwhitneyuResult(statistic=183489.0, pvalue=0.050964193374854437) \n",
    "\n",
    "# lista1: 139.124721604 37.0    lista2: 111.549835706 35.0\n",
    "# Ttest_indResult(statistic=1.3674794755026654, pvalue=0.17170100913942724)  ==\n",
    "# MannwhitneyuResult(statistic=212165.5, pvalue=0.29155228754341056) \n",
    "\n",
    "# lista1: 106.874673629 31.0    lista2: 111.549835706 35.0\n",
    "# Ttest_indResult(statistic=-0.30173068843822559, pvalue=0.76289478333324157)  ==\n",
    "# MannwhitneyuResult(statistic=338981.0, pvalue=0.27961116895506288) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### for Top 10 data:\n",
    "# lista1: 143.691960253 58.0    lista2: 173.94478951 58.0\n",
    "# Ttest_indResult(statistic=-1.3519113625871182, pvalue=0.17652337927672612)  ==\n",
    "# MannwhitneyuResult(statistic=783061.0, pvalue=0.30509155685225609) \n",
    "\n",
    "# lista1: 143.691960253 58.0    lista2: 167.18701482 62.0\n",
    "# Ttest_indResult(statistic=-1.2768178019137157, pvalue=0.20178412576089941) ==\n",
    "# MannwhitneyuResult(statistic=748145.0, pvalue=0.046522460815123221) \n",
    "\n",
    "# lista1: 143.691960253 58.0    lista2: 188.704416761 76.0\n",
    "# Ttest_indResult(statistic=-2.3973626887126258, pvalue=0.0165770990556409)  ~~\n",
    "# MannwhitneyuResult(statistic=841886.0, pvalue=3.6911043571997744e-10) \n",
    "\n",
    "# lista1: 173.94478951 58.0    lista2: 167.18701482 62.0\n",
    "# Ttest_indResult(statistic=0.33615952123859666, pvalue=0.73677520159121646) ==\n",
    "# MannwhitneyuResult(statistic=1003489.0, pvalue=0.29638826468990387) \n",
    "\n",
    "# lista1: 173.94478951 58.0    lista2: 188.704416761 76.0\n",
    "# Ttest_indResult(statistic=-0.75123236538118632, pvalue=0.4525678429834209)  ==\n",
    "# MannwhitneyuResult(statistic=1134561.5, pvalue=3.1400628052478831e-08) \n",
    "\n",
    "# lista1: 167.18701482 62.0    lista2: 188.704416761 76.0\n",
    "# Ttest_indResult(statistic=-1.2723281114848348, pvalue=0.20334956013618063)  ==\n",
    "# MannwhitneyuResult(statistic=1139277.5, pvalue=1.3990818243241034e-05) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########\n",
    "#t-test: If we observe a large p-value then we cannot reject the null hypothesis of IDENTICAL AVERAGES\n",
    "\n",
    "# Mann-Whitnes U test:  if p-value large, we cannot reject the null hypothesis of identical median (that it is equally likely that a randomly selected value from one sample will be less than or greater than a randomly selected value from a second sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_top = list_subsets_data   #fig['layout']['xaxis3'].update(title='xaxis 3 title', showgrid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_list_bottom = list_subsets_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparisons for each top category with each bottom category:\n",
    "print (\"comparisons for each bottom category with each top category:\\n\")\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "\n",
    "        print (i,j, \"  \",scipy.stats.ttest_ind(new_list_bottom[i], new_list_top[j]))   \n",
    "        print (\"    \",scipy.stats.mannwhitneyu(new_list_bottom[i], new_list_top[j],  alternative='two-sided'),\"\\n\")\n",
    "        \n",
    "\n",
    "        \n",
    "for i in range(4):\n",
    "     print (scipy.stats.kstest(new_list_bottom[i], cdf ='norm')   , scipy.stats.kstest(new_list_top[i], cdf ='norm'))\n",
    "        \n",
    "#         comparisons for each bottom category with each top category:\n",
    "\n",
    "# 0 0    Ttest_indResult(statistic=-0.55585545233045353, pvalue=0.57839614511765935)   ==\n",
    "#      MannwhitneyuResult(statistic=148256.5, pvalue=5.9654639334085347e-08) \n",
    "\n",
    "# 0 1    Ttest_indResult(statistic=-1.308381175086365, pvalue=0.19091297090843487)   ==\n",
    "#      MannwhitneyuResult(statistic=189318.5, pvalue=8.5497246097993233e-10) \n",
    "\n",
    "# 0 2    Ttest_indResult(statistic=-1.493641349977251, pvalue=0.1354498614333014)   ==\n",
    "#      MannwhitneyuResult(statistic=181499.5, pvalue=5.3854853881520728e-11) \n",
    "\n",
    "# 0 3    Ttest_indResult(statistic=-2.118168666496175, pvalue=0.034278127298814083)   ==\n",
    "#      MannwhitneyuResult(statistic=201553.5, pvalue=7.7356402048985389e-20) \n",
    "\n",
    "# 1 0    Ttest_indResult(statistic=-0.17241981376530283, pvalue=0.86312994793474318)   ==\n",
    "#      MannwhitneyuResult(statistic=199775.5, pvalue=1.2778236999257127e-09) \n",
    "\n",
    "# 1 1    Ttest_indResult(statistic=-1.1037780907810519, pvalue=0.26982947688630554)   ==\n",
    "#      MannwhitneyuResult(statistic=255268.5, pvalue=5.1250774832697765e-12) \n",
    "\n",
    "# 1 2    Ttest_indResult(statistic=-1.1554615442623093, pvalue=0.24804955966635769)   ==\n",
    "#      MannwhitneyuResult(statistic=245123.5, pvalue=2.1889420978573054e-13) \n",
    "\n",
    "# 1 3    Ttest_indResult(statistic=-1.9117051504996625, pvalue=0.056043046919740221)   ==\n",
    "#      MannwhitneyuResult(statistic=273545.0, pvalue=3.0408403158237902e-24) \n",
    "\n",
    "# 2 0    Ttest_indResult(statistic=-1.8123436532932014, pvalue=0.070093342747301224)\n",
    "#      MannwhitneyuResult(statistic=312022.0, pvalue=2.2553108707356818e-22) \n",
    "\n",
    "# 2 1    Ttest_indResult(statistic=-2.7744894078782041, pvalue=0.0055752728035918554)\n",
    "#      MannwhitneyuResult(statistic=397624.5, pvalue=4.2448892563010137e-28) \n",
    "\n",
    "# 2 2    Ttest_indResult(statistic=-3.2384393652162005, pvalue=0.0012199006066814873)\n",
    "#      MannwhitneyuResult(statistic=381696.0, pvalue=2.1857413919173956e-30) \n",
    "\n",
    "# 2 3    Ttest_indResult(statistic=-4.1165344468729756, pvalue=3.9692831222398867e-05)\n",
    "#      MannwhitneyuResult(statistic=421223.0, pvalue=1.6184187926839323e-51) \n",
    "\n",
    "# 3 0    Ttest_indResult(statistic=-1.7990535873966296, pvalue=0.072159560116086818)   ==\n",
    "#      MannwhitneyuResult(statistic=390394.5, pvalue=1.2399471785422892e-18) \n",
    "\n",
    "# 3 1    Ttest_indResult(statistic=-2.8792239643699507, pvalue=0.0040224512605971582)   ==\n",
    "#      MannwhitneyuResult(statistic=498662.0, pvalue=6.2864549417584148e-24) \n",
    "\n",
    "# 3 2    Ttest_indResult(statistic=-3.3867022868615759, pvalue=0.00071913309407043878)\n",
    "#      MannwhitneyuResult(statistic=479118.5, pvalue=3.6240021713291035e-26) \n",
    "\n",
    "# 3 3    Ttest_indResult(statistic=-4.3500704280305778, pvalue=1.4118701294840231e-05)\n",
    "#      MannwhitneyuResult(statistic=532651.0, pvalue=4.1845304839012004e-47) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_subsets_data[1])\n",
    "np.std(list_subsets_data[0])\n",
    "\n",
    "scipy.stats.kstest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_ind(list_subsets_data[0], list_subsets_data[1])  # \n",
    "\n",
    "import numpy\n",
    "lista_A = []\n",
    "lista_B = []\n",
    "\n",
    "mu1 = 10\n",
    "sigma1 = 50\n",
    "\n",
    "mu2 = 13\n",
    "sigma2 = 50\n",
    "for i in range(30000):\n",
    "    \n",
    "    lista_A.append(numpy.random.normal(mu1, sigma1))\n",
    "    lista_B.append(numpy.random.normal(mu2, sigma2))\n",
    "    \n",
    "scipy.stats.ttest_ind(lista_A, lista_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(lista_A)\n",
    "scipy.stats.kstest(lista_A, cdf='norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "   \n",
    "    yaxis=dict(\n",
    "        type='log',\n",
    "        autorange=True\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_multiple_vertical_histogr' ,image_width=1500, image_height=1200, filename='../testing_multiple_vertical_histogr.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/violin_data.csv\")\n",
    "\n",
    "# for i in range(0,len(pd.unique(df['day']))):\n",
    "#     trace = {\n",
    "#             \"type\": 'violin',\n",
    "#             \"x\": df['day'][df['day'] == pd.unique(df['day'])[i]],\n",
    "#             \"y\": df['total_bill'][df['day'] == pd.unique(df['day'])[i]],\n",
    "#             \"name\": pd.unique(df['day'])[i],\n",
    "#             \"box\": {\n",
    "#                 \"visible\": True\n",
    "#             },\n",
    "#             \"meanline\": {\n",
    "#                 \"visible\": True\n",
    "#             }\n",
    "#         }\n",
    "#     data.append(trace)\n",
    "\n",
    "    \n",
    "data =[]    \n",
    " \n",
    "    \n",
    "item = lista_bins_plos_citations[0]\n",
    "    \n",
    "minimo = item[0]\n",
    "maximo = item[1]\n",
    "\n",
    "selection_df_bottom = preselection_df3[(preselection_df3[variable_plos] >= minimo)  &  (preselection_df3[variable_plos] < maximo)]\n",
    "\n",
    "print (string_top_bottom_plos, minimo, maximo, \"  unique values in select\", selection_df_bottom[variable_plos].unique(), selection_df_bottom.shape, \\\n",
    "       \" # plos:\",len(selection_df_bottom.paper_UT.unique()), \" # ref\",len(selection_df_bottom.reference_UT.unique()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### then i get one histogram per value\n",
    "for  value in range(4):#number of ciations of papers 0, 1,2,3, 4: #####################\n",
    "\n",
    "\n",
    "    df_selection = selection_df_bottom[selection_df_bottom['paper_cite_count'] == value]   \n",
    "    y = df_selection['cite_count']\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    trace = {\n",
    "                \"type\": 'violin',\n",
    "                \"x\": [value]*len(df_selection),    \n",
    "                \"y\": y,  \n",
    "                \"name\": value,\n",
    "                \"box\": {\n",
    "                    \"visible\": True\n",
    "                },\n",
    "                \"meanline\": {\n",
    "                    \"visible\": True\n",
    "                }\n",
    "            }\n",
    "    data.append(trace)\n",
    "        \n",
    "fig = {\n",
    "    \"data\": data,\n",
    "    \"layout\" : {'showlegend':False,\n",
    "        \"title\": \"\",\n",
    "        \"yaxis\": {'title' :'Number of citations of young references',},\n",
    "        \"xaxis\": {'title' :'Number of citations of PLOS papers',}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "#offline.plot(fig, auto_open=True, image = 'png', image_filename='testing_multiple_vertical_histogr' ,image_width=1500, image_height=1200, filename='../testing_multiple_vertical_histogr.html', validate=True)\n",
    "\n",
    "iplot(fig, filename = 'violin_basic', validate = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['day'][df['day'] == pd.unique(df['day'])[0]]\n",
    "[value]*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### old FIGURE 2A\n",
    "\n",
    "\n",
    "# ###  MORE TEXT IN EACH CELL AND FLIPPING COLUMNS-ROWS IN THE HEATMAP ...........\n",
    "# #  annotated heatmap plot for median publication year (OR CITATIONS) of the references used in the different sections, and separating by citation category of the plos\n",
    "# ### NEWWWW  :  ONLY bottom row and plot separately\n",
    "\n",
    "\n",
    "\n",
    "# %time plos_df = pickle.load(open('../data/plos_paper_dataframe_ONLY_ARTICLES_num_ref_sect.pkl', 'rb'))\n",
    "# print (\"done loading plos_df\", plos_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string =  'num_ref_section0'     #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "    \n",
    "\n",
    "# colorbar_string = 'number of references'\n",
    "\n",
    "\n",
    "\n",
    "# years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "    \n",
    "# string_filtering_x = 'paper_cite_count'   # bins by plos' citations on the x-axis ###      \n",
    "  \n",
    "# string_references_age = \"\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    \n",
    "# string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "# string_self_ref =0    #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    \n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "   \n",
    "    \n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "# fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    \n",
    "    \n",
    "#     fig_colorscale = \"Reds\"\n",
    "#     fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "  \n",
    "#     if  v1_string ==  'log_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     elif  v1_string ==  'log2_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# elif v1_string =='ref_pub_year':\n",
    "#     fig_colorscale = \"Viridis\"\n",
    "#     fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "# elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "#     fig_colorscale = [[0, '#dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "#     fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "           \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "# #quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     try:\n",
    "#         pair=[old_value, int(item[1])]    \n",
    "#     except:  # if it is a nan:\n",
    "#         pair=[old_value, item[1]]\n",
    "    \n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "#     try:\n",
    "#         old_value = int(item[1])\n",
    "#     except:\n",
    "#         old_value = item[1]\n",
    "\n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# # lista_bins_plos_citations[0][0]=2       \n",
    "# # lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "# ################################################\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "# x1_All = list(preselection_df3[v1_string])\n",
    "\n",
    "\n",
    "# list_quantiles_cell=[.25,.5,.75]\n",
    "# values_quantiles=list(preselection_df3[v1_string].quantile(list_quantiles_cell))\n",
    "# #print (\"avg ALL:\",  preselection_df3[v1_string].mean(), values_quantiles)       # dont print the ALL value because it has references that are repeated in a same paper!!! see one of the early cells instead\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_sections = [\"Introduction\",\"Methods\",\"Results\",\"Discussion\"]\n",
    "# #lista_sections = [\"I\",\"M\",\"R\",\"D\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ################ i also add the median values for the section across all data in the preselection\n",
    "# for string_section in lista_sections:\n",
    "\n",
    "        \n",
    "#     if  string_section == \"Introduction\":\n",
    "#         section=0\n",
    "#     elif  string_section == \"Methods\":\n",
    "#         section=1\n",
    "#     elif  string_section == \"Results\":\n",
    "#         section=2\n",
    "#     elif  string_section == \"Discussion\":\n",
    "#         section=3\n",
    "\n",
    "\n",
    "#     df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "#     list_quantiles_cell=[.25,.5,.75]\n",
    "#     values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "#     tupla=values_quantiles + [len(df_select)]\n",
    "    \n",
    "#     print (\"avg:\", string_section,  df_select[v1_string].mean(),  \"  STD:\",df_select[v1_string].std(), values_quantiles)\n",
    "    \n",
    "# #     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "# #     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    " \n",
    "#     dict_group_quantiles_size[string_section+\" \"]=tupla   #### ojo!!! truco trapero para q siga funcionando pero sin escribir \"ALL PAPER\" en el eje y\n",
    "#     dict_group_subset_data[string_section+\" \"]=x1_All    \n",
    " \n",
    "     \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# ########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "# lista_y=lista_sections\n",
    "# #lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "# #lista_bin_names=[\" ALL PLOS\",\" Bottom \"+str(int(100.*list_q[0]))+\"%\",\" \"+str(int(100.*list_q[0]+1))+\"% to \"+str(int(100.*list_q[-4]))+\"%\",\\\n",
    "#               #   \" \"+str(int(100.*list_q[1]+1))+\"% to \"+str(int(100.*list_q[-3]))+\"%\" ,\" \"+str(int(100.*list_q[2]+1))+\"% to \"+str(int(100.*list_q[-2]))+\"%\",\" Top \"+str(int(100.-100.*list_q[-2]))+\"%\"]\n",
    "\n",
    "# lista_bin_names=[\" \"]     #### ojo!!! truco trapero para q siga funcionando pero sin escribir \"ALL PAPER\" en el eje y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "\n",
    "# #lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "# lista_x=lista_bin_names\n",
    "\n",
    "# lista_z25=[]\n",
    "# lista_z50=[]\n",
    "# lista_z75=[]\n",
    "# lista_z_sizes=[]\n",
    "\n",
    "# for x_value in lista_x:    \n",
    "#     aux_lista25=[]\n",
    "#     aux_lista50=[]\n",
    "#     aux_lista75=[]\n",
    "#     aux_lista_sizes=[]\n",
    "    \n",
    "#     for y_value in lista_y:       \n",
    "\n",
    "#         llave=y_value+x_value\n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][0])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][0]\n",
    "#         aux_lista25.append(value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][1])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][1]\n",
    "#         aux_lista50.append(value)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][2])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][2]\n",
    "#         aux_lista75.append(value)\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "#         value_size=dict_group_quantiles_size[llave][3]\n",
    "#         aux_lista_sizes.append(value_size)\n",
    "        \n",
    "        \n",
    "#         #print (y_value,\" \",x_value, value, value_size)\n",
    "#     lista_z25.append(aux_lista25)\n",
    "#     lista_z50.append(aux_lista50)\n",
    "#     lista_z75.append(aux_lista75)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     lista_z_sizes.append(aux_lista_sizes)\n",
    "    \n",
    "   \n",
    "\n",
    "# # print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# # print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_text_z=[]\n",
    "# for i in range(len(lista_z_sizes)):\n",
    "#     aux=[]\n",
    "#     for j in range(len(lista_z_sizes[0])):\n",
    "#        # value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(lista_z_sizes[i][j])+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "#         value=str(lista_z25[i][j])+\"-<b>\"+str(lista_z50[i][j])+\"</b>-\"+str(lista_z75[i][j])+\"<br>(\"+str(format(lista_z_sizes[i][j], ',d'))+\")\"            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "        \n",
    "#         aux.append(value)\n",
    "#     lista_text_z.append(aux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### using a different library\n",
    "# path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    " \n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True, colorbar=dict(title=colorbar_string, titleside='right' ),)#, reversescale=True)\n",
    "# #fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "# fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# #fig.layout.xaxis.update({'title': 'Section'})\n",
    "# # fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=48   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -3\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -20\n",
    "\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # i add some space between the axis label and the axis ticks\n",
    "# fig['layout']['margin']=dict(\n",
    "#         #l=200,\n",
    "#        # r=50,\n",
    "#         b=100,\n",
    "#         #t=100,\n",
    "#        # pad=4\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # layout = go.Layout(\n",
    "# #     title='GitHub commits per day',\n",
    "# #     xaxis = dict(ticks='', nticks=36),\n",
    "# #     yaxis = dict(ticks='' )\n",
    "# # )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=500, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_group_subset_data.keys()\n",
    "# list_keys=['intro Bottom 30%', 'methods Bottom 30%', 'results Bottom 30%', 'discussion Bottom 30%', 'intro 31% to 60%', 'methods 31% to 60%', 'results 31% to 60%', 'discussion 31% to 60%', 'intro 61% to 90%', 'methods 61% to 90%', 'results 61% to 90%', 'discussion 61% to 90%', 'intro 91% to 99%', 'methods 91% to 99%', 'results 91% to 99%', 'discussion 91% to 99%', 'intro Top 1%', 'methods Top 1%', 'results Top 1%', 'discussion Top 1%', 'intro ALL PLOS', 'methods ALL PLOS', 'results ALL PLOS', 'discussion ALL PLOS']\n",
    "\n",
    "# sorted(list_keys)\n",
    "# threshold_p_value = 0.001 / (24.*23./2.)\n",
    "# threshold_p_value\n",
    "v1_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_listas\n",
    "list_keys_macro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_listas\n",
    "list2d = [[1,2,3],[4,5,6], [7], [8,9]]\n",
    "flatten_lista = list(itertools.chain(*lista_listas))\n",
    "min(flatten_lista)\n",
    "v1_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## i get the percentiles by section, for all data \n",
    "\n",
    "lista_sections = [\"intro\",\"methods\",\"results\",\"discussion\"]\n",
    "\n",
    "v1_string =  'total_refs'#      cite_count    diff_year_plos_ref    total_refs\n",
    "list_quantiles_cell=[.25,.5,.75]\n",
    "    \n",
    "dict_group_subset_data={}    \n",
    "\n",
    "list_years = [[2005],[2006],[2007],[2008],[2009],[2010],[2011],[2012],[2013],[2014],[2015],[2016]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if v1_string ==  'cite_count':  # for the mean/median of this particular variable, i need to control for year!!!\n",
    "\n",
    "    for years in list_years:\n",
    "\n",
    "        dict_group_subset_data={}\n",
    "        \n",
    "        preselection_df = df_merged\n",
    "        preselection_df_ref = df_references    \n",
    "        \n",
    "        print (years)\n",
    "\n",
    "        preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]     \n",
    "\n",
    "        list_ref_year=list(preselection_df.reference_UT.unique())\n",
    "        preselection_df_ref = df_references[df_references['reference_UT'].isin(list_ref_year)]  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ################ get median values for the entire dataset\n",
    "        for string_section in lista_sections:\n",
    "\n",
    "\n",
    "            if  string_section == \"intro\":\n",
    "                section=0\n",
    "            elif  string_section == \"methods\":\n",
    "                section=1\n",
    "            elif  string_section == \"results\":\n",
    "                section=2\n",
    "            elif  string_section == \"discussion\":\n",
    "                section=3\n",
    "\n",
    "\n",
    "            df_select = preselection_df[preselection_df['regex_sect_index']== section]       \n",
    "            df_select = df_select[df_select[v1_string] != -1]   ### i dont know why i have some -1 values!!! (7 occurrences total in df_merged)\n",
    "\n",
    "\n",
    "            values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))\n",
    "\n",
    "            print (v1_string, string_section, values_quantiles,  \" avg:\", df_select[v1_string].mean() ,\" N:\",len(df_select[v1_string]))\n",
    "\n",
    "            dict_group_subset_data[string_section]=list(df_select[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "        ### ojo! calculo el valor para todas las referencias sin ninguna repeticion!!!\n",
    "        values_quantiles=list(preselection_df_ref[v1_string].quantile(list_quantiles_cell))      \n",
    "        print (v1_string,\" all sect:\", values_quantiles, preselection_df_ref[v1_string].median(), preselection_df_ref[v1_string].mean() ,len(preselection_df_ref[v1_string])     )\n",
    "\n",
    "        dict_group_subset_data[\"All\"]=list(preselection_df_ref[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        print (\"\\n\")\n",
    "\n",
    "        for pair in itertools.combinations(dict_group_subset_data.keys(), 2):    \n",
    "            set1=dict_group_subset_data[pair[0]]\n",
    "            set2=dict_group_subset_data[pair[1]]\n",
    "            print (\"comparison\",pair, \"\\t\\t\",stats.ks_2samp(set1, set2)  )\n",
    "\n",
    "        print (\"\\n\\n\")\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "elif v1_string ==  'diff_year_plos_ref'  :  \n",
    "        \n",
    "    dict_group_subset_data={}\n",
    "    \n",
    "    preselection_df = df_merged\n",
    "    preselection_df_ref = df_references    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ################ get median values for the entire dataset\n",
    "    for string_section in lista_sections:\n",
    "\n",
    "\n",
    "        if  string_section == \"intro\":\n",
    "            section=0\n",
    "        elif  string_section == \"methods\":\n",
    "            section=1\n",
    "        elif  string_section == \"results\":\n",
    "            section=2\n",
    "        elif  string_section == \"discussion\":\n",
    "            section=3\n",
    "\n",
    "\n",
    "        df_select = preselection_df[preselection_df['regex_sect_index']== section]       \n",
    "        df_select = df_select[df_select[v1_string] != -1]   ### i dont know why i have some -1 values!!! (7 occurrences total in df_merged)\n",
    "\n",
    "\n",
    "        values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))\n",
    "        print (v1_string, string_section, values_quantiles,  \" avg:\", df_select[v1_string].mean() ,\" N:\",len(df_select[v1_string]))\n",
    "        dict_group_subset_data[string_section]=list(df_select[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### ojo! calculo el valor para todas las referencias sin ninguna repeticion!!!\n",
    "    values_quantiles=list(preselection_df_ref[v1_string].quantile(list_quantiles_cell))      \n",
    "    print (v1_string,\" all sect:\", values_quantiles, preselection_df_ref[v1_string].median(), preselection_df_ref[v1_string].mean() ,len(preselection_df_ref[v1_string])     )\n",
    "\n",
    "    dict_group_subset_data[\"All\"]=list(preselection_df_ref[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"\\n\")\n",
    "\n",
    "    for pair in itertools.combinations(dict_group_subset_data.keys(), 2):    \n",
    "        set1=dict_group_subset_data[pair[0]]\n",
    "        set2=dict_group_subset_data[pair[1]]\n",
    "        print (\"comparison\",pair, \"\\t\\t\",stats.ks_2samp(set1, set2)  )\n",
    "\n",
    "    print (\"\\n\\n\")\n",
    "\n",
    "\n",
    "    \n",
    "elif   v1_string ==  'total_refs':  \n",
    " \n",
    "\n",
    "    ### ojo! calculo el valor para todos los papers sin ninguna repeticion!!!\n",
    "    values_quantiles=list(plos_df[v1_string].quantile(list_quantiles_cell))      \n",
    "    print (v1_string, plos_df.shape, \" all sect:\", values_quantiles, plos_df[v1_string].median(), plos_df[v1_string].mean() ,len(plos_df[v1_string])     )\n",
    "\n",
    "  \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2005]\n",
    "# cite_count intro [75.5, 163.0, 371.5]  avg: 712.105263158  N: 95\n",
    "# cite_count methods [92.0, 345.0, 2766.0]  avg: 5458.56962025  N: 79\n",
    "# cite_count results [176.0, 456.0, 1086.5]  avg: 1567.0  N: 12\n",
    "# cite_count discussion [84.5, 216.0, 455.0]  avg: 916.565217391  N: 23\n",
    "# cite_count  all sect: [61.75, 135.5, 428.0] 135.5 1252.28896104 308\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.25329780146568959, pvalue=0.0062545700038756698)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.3719298245614035, pvalue=0.078970623940412626)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.16521739130434782, pvalue=0.65245006785038939)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.081578947368421056, pvalue=0.70127968566273458)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.21308016877637134, pvalue=0.67676716325996555)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.24766097963676392, pvalue=0.19265408556072336)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.24239684366266645, pvalue=0.00096166253921665815)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.28260869565217395, pvalue=0.48281249767546308)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.34848484848484845, pvalue=0.093533247897836139)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.16586674195369849, pvalue=0.55855364648418981)\n",
    "\n",
    "\n",
    "\n",
    "# [2006]\n",
    "# cite_count intro [58.0, 134.0, 332.0]  avg: 394.05541811  N: 2021\n",
    "# cite_count methods [61.0, 177.0, 565.0]  avg: 1274.16686391  N: 845\n",
    "# cite_count results [60.0, 139.0, 389.5]  avg: 477.288117771  N: 951\n",
    "# cite_count discussion [48.0, 106.0, 260.5]  avg: 297.728525981  N: 1886\n",
    "# cite_count  all sect: [53.0, 123.0, 306.0] 123.0 407.0415032 4843\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.11202843515864491, pvalue=5.4830554842508837e-07)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.036075986578361419, pvalue=0.36320743961392332)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.081051661688012921, pvalue=4.8994831300630606e-06)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.030602992346621005, pvalue=0.1359449102992534)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.085469670667438158, pvalue=0.0026810043327635262)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.17324226470975801, pvalue=8.5372711703509143e-16)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.13442521201221314, pvalue=8.0143198063925495e-12)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.09669232476167855, pvalue=1.306320935198881e-05)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.053731544851122248, pvalue=0.01950266144950627)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.050749526653352195, pvalue=0.0017544817791915433)\n",
    "\n",
    "\n",
    "\n",
    "# [2007]\n",
    "# cite_count intro [52.0, 117.0, 283.0]  avg: 344.109310672  N: 18946\n",
    "# cite_count methods [53.0, 137.0, 500.0]  avg: 1345.10002826  N: 7078\n",
    "# cite_count results [50.0, 115.0, 271.0]  avg: 420.801821103  N: 7468\n",
    "# cite_count discussion [43.0, 96.0, 227.0]  avg: 311.673705722  N: 16148\n",
    "# cite_count  all sect: [45.0, 101.0, 240.0] 101.0 295.045347054 38481\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.11374438563616529, pvalue=1.5781894490324159e-58)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.016762890115175844, pvalue=0.097557654982484407)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.067864507606340885, pvalue=2.1694042977329587e-35)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.048960500873077795, pvalue=6.4683333608440171e-27)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.11321788448647729, pvalue=4.7674611391356954e-41)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.14048995645884499, pvalue=4.4042607380356084e-85)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.13591879587199573, pvalue=1.1725677008918019e-96)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.062006547894748076, pvalue=1.5479695358370105e-17)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.042746942623358264, pvalue=2.2078374129572309e-10)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.019314694562421131, pvalue=0.00040444475785580709)\n",
    "\n",
    "\n",
    "\n",
    "# [2008]\n",
    "# cite_count intro [50.0, 114.0, 291.0]  avg: 339.558649581  N: 44638\n",
    "# cite_count methods [49.0, 125.0, 453.25]  avg: 1419.93902011  N: 17104\n",
    "# cite_count results [49.0, 111.0, 286.75]  avg: 403.951819295  N: 17974\n",
    "# cite_count discussion [41.0, 91.0, 218.0]  avg: 250.268129934  N: 40913\n",
    "# cite_count  all sect: [41.0, 90.0, 210.0] 90.0 245.135458303 88315\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.093126900775560184, pvalue=8.8608669880096522e-94)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.0087537318429889988, pvalue=0.27869582492020051)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.073552232395137418, pvalue=6.6760137834109842e-101)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.079093042186489826, pvalue=9.1559286373686668e-162)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.093982936375304749, pvalue=7.7278248610139962e-68)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.13486374135932011, pvalue=2.143545634505759e-191)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.14143570044127796, pvalue=6.7383957696952838e-250)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.067404138515462619, pvalue=8.2013246685742346e-50)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.074031305833375138, pvalue=1.1635626889102283e-71)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.0091188807397771798, pvalue=0.018995385293811787)\n",
    "\n",
    "\n",
    "\n",
    "# [2009]\n",
    "# cite_count intro [46.0, 107.0, 270.0]  avg: 322.133103308  N: 73905\n",
    "# cite_count methods [44.0, 115.0, 428.0]  avg: 1384.63585649  N: 27426\n",
    "# cite_count results [44.0, 103.0, 261.0]  avg: 371.505167027  N: 29127\n",
    "# cite_count discussion [39.0, 85.0, 200.0]  avg: 221.86952106  N: 68923\n",
    "# cite_count  all sect: [38.0, 82.0, 191.0] 82.0 215.397065268 141478\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.095020060327733025, pvalue=1.4566737032524953e-157)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.012571472220341384, pvalue=0.0026804236497213708)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.074700065898250956, pvalue=1.6812363861329299e-173)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.086551688313025732, pvalue=1.200533670102295e-316)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.096901771094753175, pvalue=7.2291807146680422e-116)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.13944666636489167, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.14921474167869797, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.066337690951706718, pvalue=8.0819580991555063e-79)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.07811665076363139, pvalue=1.1944953344811408e-128)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.014265390614105256, pvalue=1.2586726812655362e-08)\n",
    "\n",
    "\n",
    "\n",
    "# [2010]\n",
    "# cite_count intro [43.0, 99.0, 253.0]  avg: 303.724509102  N: 117336\n",
    "# cite_count methods [42.0, 109.0, 414.0]  avg: 1325.07072553  N: 42672\n",
    "# cite_count results [41.0, 93.0, 238.0]  avg: 339.987886034  N: 44329\n",
    "# cite_count discussion [36.0, 78.0, 186.0]  avg: 212.628258675  N: 107981\n",
    "# cite_count  all sect: [34.0, 74.0, 170.0] 74.0 190.598084231 214744\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.098614461326410607, pvalue=4.1841729678629232e-265)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.020309913288783787, pvalue=5.728769430624991e-12)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.077352263056018056, pvalue=5.7600696347312613e-293)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.097740327261885973, pvalue=0.0)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.10302330501734724, pvalue=3.3816603863570733e-201)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.14190030439467494, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.16110139395178524, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.059948015862099191, pvalue=1.1670370163889574e-98)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.080628361895650702, pvalue=3.6279823993594878e-208)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.021547761339460569, pvalue=1.9857557007916606e-29)\n",
    "\n",
    "\n",
    "\n",
    "# [2011]\n",
    "# cite_count intro [39.0, 90.0, 233.0]  avg: 291.619384529  N: 249890\n",
    "# cite_count methods [38.0, 104.0, 430.0]  avg: 1501.28117795  N: 91515\n",
    "# cite_count results [38.0, 90.0, 235.0]  avg: 368.63789368  N: 81697\n",
    "# cite_count discussion [33.0, 71.0, 170.0]  avg: 196.804610986  N: 230146\n",
    "# cite_count  all sect: [30.0, 63.0, 143.0] 63.0 154.857112766 413025\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.11216867760775573, pvalue=0.0)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.006079717023175002, pvalue=0.021009716965126685)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.076373879358533681, pvalue=0.0)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.11922751856710057, pvalue=0.0)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.10790338419745449, pvalue=0.0)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.15533141615823209, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.19193679551610199, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.074866354835961157, pvalue=2.9997485052925934e-294)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.11823843131542855, pvalue=0.0)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.043627502300112586, pvalue=6.4806716497700419e-245)\n",
    "\n",
    "\n",
    "\n",
    "# [2012]\n",
    "# cite_count intro [34.0, 80.0, 209.0]  avg: 266.007153205  N: 444556\n",
    "# cite_count methods [34.0, 95.0, 422.0]  avg: 1442.76415788  N: 164449\n",
    "# cite_count results [34.0, 81.0, 215.0]  avg: 326.42070164  N: 138105\n",
    "# cite_count discussion [29.0, 64.0, 156.0]  avg: 186.368031284  N: 416576\n",
    "# cite_count  all sect: [25.0, 54.0, 121.0] 54.0 128.241665159 690985\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.12087256626957132, pvalue=0.0)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.0058257438803518413, pvalue=0.0015576581189053554)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.071072467213402657, pvalue=0.0)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.13473599814182269, pvalue=0.0)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.11614925159525091, pvalue=0.0)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.16366473388099945, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.21412046634148019, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.075864598763388025, pvalue=0.0)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.13956078365708346, pvalue=0.0)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.063767802246121219, pvalue=0.0)\n",
    "\n",
    "\n",
    "\n",
    "# [2013]\n",
    "# cite_count intro [30.0, 71.0, 192.0]  avg: 253.236809748  N: 557912\n",
    "# cite_count methods [30.0, 89.0, 416.0]  avg: 1463.92274269  N: 213119\n",
    "# cite_count results [30.0, 71.0, 192.0]  avg: 305.301532819  N: 165773\n",
    "# cite_count discussion [25.0, 57.0, 142.0]  avg: 174.604914135  N: 535028\n",
    "# cite_count  all sect: [22.0, 47.0, 108.0] 47.0 115.164844982 870897\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.12639324484936998, pvalue=0.0)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.0041141736470946544, pvalue=0.026353544226045581)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.071550663698717543, pvalue=0.0)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.13672258695265138, pvalue=0.0)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.12453689495163967, pvalue=0.0)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.16964808239571294, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.22246413811235144, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.070311844526598999, pvalue=0.0)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.13525481841551057, pvalue=0.0)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.065290066511303091, pvalue=0.0)\n",
    "\n",
    "\n",
    "\n",
    "# [2014]\n",
    "# cite_count intro [24.0, 61.0, 170.0]  avg: 235.963286617  N: 473642\n",
    "# cite_count methods [26.0, 79.0, 403.0]  avg: 1442.82032365  N: 178649\n",
    "# cite_count results [24.0, 60.0, 171.0]  avg: 292.88451101  N: 106495\n",
    "# cite_count discussion [21.0, 49.0, 125.0]  avg: 160.756162382  N: 454532\n",
    "# cite_count  all sect: [19.0, 43.0, 103.0] 43.0 113.86698398 790386\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.13373725995123775, pvalue=0.0)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.0070078539354760383, pvalue=0.00038833157570311035)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.069293993935837461, pvalue=0.0)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.11152442967683518, pvalue=0.0)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.1294469796804516, pvalue=0.0)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.17706574155961052, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.21397874692963914, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.066578102836356901, pvalue=0.0)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.1091939195683963, pvalue=0.0)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.043025998883527428, pvalue=0.0)\n",
    "\n",
    "\n",
    "\n",
    "# [2015]\n",
    "# cite_count intro [19.0, 51.0, 149.0]  avg: 211.261330775  N: 441938\n",
    "# cite_count methods [21.0, 74.0, 393.0]  avg: 1321.41146718  N: 171411\n",
    "# cite_count results [19.0, 53.0, 155.0]  avg: 285.532878384  N: 90120\n",
    "# cite_count discussion [16.0, 41.0, 110.0]  avg: 144.285564678  N: 422055\n",
    "# cite_count  all sect: [15.0, 38.0, 96.0] 38.0 108.83453719 761857\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.1434888293539871, pvalue=0.0)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.0086798658658674421, pvalue=2.5015485921793468e-05)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.065862977319555149, pvalue=0.0)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.094927366346560405, pvalue=0.0)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.13608148241763807, pvalue=0.0)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.18762189297058174, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.21673487424332216, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.072157604726515556, pvalue=0.0)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.10194418639016567, pvalue=0.0)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.030978121255135438, pvalue=6.4808496146642549e-227)\n",
    "\n",
    "\n",
    "\n",
    "# [2016]\n",
    "# cite_count intro [16.0, 46.0, 138.0]  avg: 192.706248355  N: 113950\n",
    "# cite_count methods [19.0, 67.0, 361.0]  avg: 1260.66683519  N: 45494\n",
    "# cite_count results [18.0, 51.0, 148.0]  avg: 269.426026856  N: 25320\n",
    "# cite_count discussion [14.0, 37.0, 102.0]  avg: 135.938681124  N: 110488\n",
    "# cite_count  all sect: [15.0, 40.0, 111.0] 40.0 147.606143736 230381\n",
    "\n",
    "\n",
    "# comparison ('intro', 'methods') \t\t Ks_2sampResult(statistic=0.1414163841634517, pvalue=0.0)\n",
    "# comparison ('intro', 'results') \t\t Ks_2sampResult(statistic=0.027602150135137227, pvalue=3.7029461113033712e-14)\n",
    "# comparison ('intro', 'discussion') \t\t Ks_2sampResult(statistic=0.061474851732005842, pvalue=9.4652044401845891e-185)\n",
    "# comparison ('intro', 'All') \t\t Ks_2sampResult(statistic=0.043537516526961029, pvalue=4.6518284702729584e-126)\n",
    "# comparison ('methods', 'results') \t\t Ks_2sampResult(statistic=0.13375240149370249, pvalue=1.1407348702461305e-253)\n",
    "# comparison ('methods', 'discussion') \t\t Ks_2sampResult(statistic=0.18621415494125704, pvalue=0.0)\n",
    "# comparison ('methods', 'All') \t\t Ks_2sampResult(statistic=0.17576104447631213, pvalue=0.0)\n",
    "# comparison ('results', 'discussion') \t\t Ks_2sampResult(statistic=0.086864950014086562, pvalue=1.1655747247625082e-135)\n",
    "# comparison ('results', 'All') \t\t Ks_2sampResult(statistic=0.068246636986181275, pvalue=7.3014693886050512e-93)\n",
    "# comparison ('discussion', 'All') \t\t Ks_2sampResult(statistic=0.019441614167417076, pvalue=5.7971987490500044e-25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in df_merged.categ_codes.unique():\n",
    "    print (item, len(item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)\n",
    "#len(preselection_df3.paper_UT.unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections or anything else)\n",
    "# list_q=[0.5,0.9,.99,1]\n",
    "\n",
    "# quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "\n",
    "# print (lista_bins_plos_citations)     # [[0, 5], [5, 25], [25, 80], [80, 1994]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.head()\n",
    "df_merged.rel_loc_in_sect.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_df[preselection_df['plos_j1']== \"PLOS GENET\" ].cite_count.value_counts()  \n",
    "df_merged.cite_count.value_counts()  \n",
    "df_merged[df_merged['cite_count']>=10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns#plos_field.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DONE  heatmap plot for median publication year (OR CITATIONS) of the references used in the different sections (AND RELATIVE LOCATION WITHIN SECTION: 1/3,2/3,3/3), and separating by citation category of the plos\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_median_value={}\n",
    "\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string =  'diff_year_plos_ref'#  log2_num_cit_ref'  ######  log2_num_cit_ref' # #  log_num_cit_ref   #'ref_pub_year'      cite_count   \n",
    "   \n",
    "    \n",
    "# string_filtering_x = 'paper_cite_count'   # bins by plos' citations  ###      \n",
    "\n",
    "# string_filtering_subsection = 'rel_loc_in_sect'\n",
    "\n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # neglected tropical diseases\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# string_references_age=\"old\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# string_isolated_ref=\"\" #\"\"#0  #\"\"   # 0  or 1 (or nothing, to include all ref)\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers and without including all the repetitions for ref. etc(so i calculate them FROM THE PLOS DF!!)\n",
    "# list_q=[0.5,0.9,.99,1]\n",
    "\n",
    "# quantiles=sorted(list(plos_df[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "\n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# # lista_bins_plos_citations[0][0]=2       \n",
    "# # lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "# ################################################3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# years=[2012]\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df0\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df0[preselection_df0['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "    \n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "# fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"\n",
    "        \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "    \n",
    "    \n",
    "#     fig_colorscale = \"Reds\"\n",
    "#     fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "  \n",
    "#     if  v1_string ==  'log_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     elif  v1_string ==  'log2_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# elif v1_string =='ref_pub_year':\n",
    "#     fig_colorscale = \"Viridis\"\n",
    "#     fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "   \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "         \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "# elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "#     fig_colorscale = [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "#     fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "           \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "    \n",
    "    \n",
    "        \n",
    "     \n",
    "\n",
    "# print (N_all)\n",
    "    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_section_subsect=[]\n",
    "\n",
    "\n",
    "# lista_sections = [\"intro\",\"methods\",\"results\",\"discussion\"]\n",
    "# lista_sub_section=[[0.,0.33],[0.33,0.66],[0.66,1.]]\n",
    "# #lista_sub_section=[[0.,0.5],[0.5,1.]]\n",
    "\n",
    "# for string_section in lista_sections:\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#     ##### preselection to include only occurences in the DISCUSSION section of the papers\n",
    "#     if  string_section == \"intro\":\n",
    "#         section=0\n",
    "#     elif  string_section == \"methods\":\n",
    "#         section=1\n",
    "#     elif  string_section == \"results\":\n",
    "#         section=2\n",
    "#     elif  string_section == \"discussion\":\n",
    "#         section=3\n",
    "\n",
    "   \n",
    "\n",
    "#     ### selection by section\n",
    "#     preselection_df4 = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "#     print (\"   size of preselection4 (by section):\",preselection_df4.shape, string_section)\n",
    "\n",
    "\n",
    "\n",
    "#     for sub_section in lista_sub_section:\n",
    "        \n",
    "#         min_sub_sect=sub_section[0]\n",
    "#         max_sub_sect=sub_section[1]                       \n",
    "       \n",
    "\n",
    "        \n",
    "#         preselection_df5 = preselection_df4[(preselection_df4[string_filtering_subsection] >= min_sub_sect)  &  (preselection_df4[string_filtering_subsection] < max_sub_sect)]\n",
    "#         print (\"    size of preselection5 (by sub-section):\",preselection_df5.shape, sub_section)\n",
    "                   \n",
    "\n",
    "#         x1_All = list(preselection_df5[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         cont=0\n",
    "#         for item in lista_bins_plos_citations:\n",
    "\n",
    "#             minimo = item[0]\n",
    "#             maximo = item[1]\n",
    "\n",
    "\n",
    "#             df_select = preselection_df5[(preselection_df5[string_filtering_x] >= minimo)  &  (preselection_df5[string_filtering_x] < maximo)]\n",
    "#             print (\"     size of final selection (by bin cit. plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "#             x1 = list(df_select[v1_string])\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#             if cont ==0:            \n",
    "#                 group=string_section+\"<br>\"+str(sub_section)+\" Bottom 50%\"            \n",
    "#             elif cont==1:\n",
    "#                  group=string_section+\"<br>\"+str(sub_section)+\" 51% to 90%\"            \n",
    "#             elif cont==2: \n",
    "#                 group=string_section+\"<br>\"+str(sub_section)+\" 91% to 99%\"            \n",
    "#             elif cont==3:\n",
    "#                  group=string_section+\"<br>\"+str(sub_section)+\" Top 1%\"            \n",
    "\n",
    "#             tupla=[np.nanmedian(x1), len(x1)]\n",
    "# #             if (len(x1)) ==0:\n",
    "# #                 input()\n",
    "#             dict_group_median_value[group]=tupla\n",
    "\n",
    "#             dict_group_subset_data[group]=x1\n",
    "#     #         print (\"group\",group)\n",
    "\n",
    "#             cont +=1\n",
    "\n",
    "#             name_sub=string_section+\"<br>\"+str(sub_section)\n",
    "#             if (name_sub) not in lista_section_subsect:\n",
    "#                 lista_section_subsect.append(name_sub)\n",
    "\n",
    "\n",
    "#         ### i also add the median values for the section across all data in the preselection\n",
    "#         tupla=[np.nanmedian(x1_All), len(x1_All)]\n",
    "#         dict_group_median_value[string_section+\"<br>\"+str(sub_section)+\" All\"]=tupla\n",
    "#         dict_group_subset_data[string_section+\"<br>\"+str(sub_section)+\" All\"]=x1_All    \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "# lista_y=lista_section_subsect#lista_sections\n",
    "# lista_x=[\" Bottom 50%\",\" 51% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" All\"]\n",
    "\n",
    "\n",
    "# lista_z=[]\n",
    "# lista_z_sizes=[]\n",
    "\n",
    "# for y_value in lista_y:\n",
    "#     aux_lista=[]\n",
    "#     aux_lista_sizes=[]\n",
    "    \n",
    "#     for x_value in lista_x:    \n",
    "# #         for llave in dict_group_median_value:\n",
    "# #             if (x_value in llave) and (y_value in llave):\n",
    "\n",
    "#         llave=y_value+x_value\n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_median_value[llave][0])\n",
    "#         except: # if nan\n",
    "#             value=dict_group_median_value[llave][0]\n",
    "#         aux_lista.append(value)\n",
    "        \n",
    "#         value_size=dict_group_median_value[llave][1]\n",
    "#         aux_lista_sizes.append(value_size)\n",
    "        \n",
    "        \n",
    "#         #print (y_value,\" \",x_value, value, value_size)\n",
    "#     lista_z.append(aux_lista)\n",
    "#     lista_z_sizes.append(aux_lista_sizes)\n",
    "    \n",
    "   \n",
    "\n",
    "# #print (lista_z)\n",
    "# #print (lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_text_z=[]\n",
    "# for i in range(len(lista_z)):\n",
    "#     aux=[]\n",
    "#     for j in range(len(lista_z[0])):\n",
    "#         value=\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "#         aux.append(value)\n",
    "#     lista_text_z.append(aux)\n",
    "# print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### using a different library\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=lista_z, x=lista_x, y=lista_y, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors, showscale=True)#, reversescale=True)\n",
    "# fig.layout.title =  fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# fig.layout.xaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    " \n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=\"labelled-heatmap\" ,image_width=2000, image_height=1400, \n",
    "#               filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for pair in itertools.combinations(dict_group_subset_data.keys(), 2):\n",
    "#     print (pair)\n",
    "#     set1=dict_group_subset_data[pair[0]]\n",
    "#     set2=dict_group_subset_data[pair[1]]\n",
    "#     print (\"comparison\",pair, stats.ks_2samp(set1, set2),\"\\n\"  )\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.ref_j1.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OLD   ########### i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data\n",
    "\n",
    "# Niter=100\n",
    "\n",
    "# years=[2011]\n",
    "\n",
    "# string_references_age=\"all\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "# string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "# string_self_ref=0    #\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "# string_journal=\"\"\n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     print (\" size of preselection1 (self-cit):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by isolated or group references:\n",
    "# preselection_df0 = preselection_df   \n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]        \n",
    "#     print (\"  size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df0\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df0[preselection_df0['plos_j1']== string_journal ]  \n",
    "#     print (\"   size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "#     print (\"    size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection only young/old references:       \n",
    "# preselection_df3 = preselection_df2\n",
    "# if string_references_age == \"young\":\n",
    "#     time_window_age = 1   \n",
    "#     preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_age) ]   \n",
    "    \n",
    "#     print (\"    size of preselection3 (only young references):\",preselection_df3.shape, string_references_age)\n",
    "\n",
    "# elif string_references_age == \"old\":\n",
    "#     time_window_age = 10    \n",
    "#     preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_age) ]   \n",
    "    \n",
    "#     print (\"    size of preselection3 (only old references):\",preselection_df3.shape,string_references_age )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "# print (\"     N plos:\", N_plos, \" N records:\", N_all)        \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "# ############## i define quantiles for plos papers (for that subselection, and based on their FINAL number of citations):\n",
    "# list_q_plos=[.2,.8,1]\n",
    "# #list_q_plos=[.1,.9,1]\n",
    "# #list_q_plos=[.05,.95,1]\n",
    "\n",
    "# df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])   # ojo!!! dont use preselection_df3 directly because there are REPETITIONS!!!!\n",
    "\n",
    "# quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos)#,quantiles, df_for_quantiles_plos.shape)   \n",
    "\n",
    "# lista_bins_plos=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "                           \n",
    "# #print (lista_bins_plos, min(preselection_df3['paper_cite_count']), max(preselection_df3['paper_cite_count']))\n",
    "\n",
    "\n",
    "\n",
    "# cont = 0\n",
    "# dict_bin_list_plos_UT={}\n",
    "# for item in lista_bins_plos:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]   \n",
    "\n",
    "#     df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "#     llave=str(minimo)+\"-\"+str(maximo)\n",
    "#     dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "#     print (llave, len(list(df_select.reference_UT.unique())))\n",
    "#     max_key_plos=llave\n",
    "\n",
    "    \n",
    "#     if cont ==0:\n",
    "#         min_key_plos = llave\n",
    "#     cont  +=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ########## i define quantiles for references (based on their FINAL number of citations)\n",
    "# list_q_ref=[.2,.8,1]\n",
    "# #list_q_ref=[.1,.9,1]\n",
    "\n",
    "# #list_q_ref=[.05,.95,1]\n",
    "# df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "# quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "# lista_bins=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "# cont = 0\n",
    "# dict_bin_list_ref_UT={}\n",
    "# for item in lista_bins:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]    \n",
    "     \n",
    "#     df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "#     llave=str(minimo)+\"-\"+str(maximo)\n",
    "#     dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "#     print (llave, len(list(df_select.reference_UT.unique())))\n",
    "#     max_key_ref=llave\n",
    "\n",
    "#     if cont ==0:\n",
    "#         min_key_ref = llave\n",
    "#     cont  +=1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ############### i create the list of top plos, top ref, bottom plos and bottom ref:\n",
    "# #########################\n",
    "\n",
    "# lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "# print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "# lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "# print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "# lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "# print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "# lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "# print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "# list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "# print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######  i look at the usage of the top ref\n",
    "# ################################################  \n",
    "\n",
    "# df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "# df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "# df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "# usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "# usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "# print (\"fraction of usage of top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######  i look at the usage of the non-top ref\n",
    "# ################################################      \n",
    "\n",
    "# df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "# df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "# df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "# usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "# usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "# print (\"fraction of usage of non-top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ####################\n",
    "# # I canculate the null model (usage of references by top and non top plos papers, from the randomized data)\n",
    "# #################################################  \n",
    "\n",
    "# lista_usage_top_ref_by_top_plos_rand = []\n",
    "# lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "# lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "# lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(Niter):\n",
    "    \n",
    "#     print (i)\n",
    "    \n",
    "#     lista_values = list(preselection_df3.paper_UT)   #[i for i in range (len(df_merged))]\n",
    "#     random.shuffle(lista_values)\n",
    "#     preselection_df3['randomized_paper_UT'] = lista_values\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#     ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "#     df_top_ref_rand = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "    \n",
    "#     df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['randomized_paper_UT'].isin(lista_top_plos)]\n",
    "#     df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['randomized_paper_UT'].isin(lista_bottom_plos)]\n",
    "    \n",
    "    \n",
    "#     usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "#     usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "    \n",
    "    \n",
    "#     lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "#     lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "#     df_non_top_ref_rand = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]        \n",
    "    \n",
    "#     df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['randomized_paper_UT'].isin(lista_top_plos)]\n",
    "#     df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['randomized_paper_UT'].isin(lista_bottom_plos)]\n",
    "    \n",
    "    \n",
    "#     usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "#     usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "    \n",
    "    \n",
    "#     lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "#     lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "# print(\"fraction of usage of top ref by\")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\navg randomized\")\n",
    "# print (\"fraction of usage of non-top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ##NEEWWWWWW for double checking!!  figure 4a\n",
    "\n",
    "# ########### i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data\n",
    "\n",
    "# Niter=1000\n",
    "\n",
    "# years=[2008]\n",
    "\n",
    "# string_references_age=\"all\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "# string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "# string_self_ref=0    #\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "# string_journal=\"\"\n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     print (\" size of preselection1 (self-cit):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by isolated or group references:\n",
    "# preselection_df0 = preselection_df   \n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]        \n",
    "#     print (\"  size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df0\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df0[preselection_df0['plos_j1']== string_journal ]  \n",
    "#     print (\"   size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "#     print (\"    size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection only young/old references:       \n",
    "# preselection_df3 = preselection_df2\n",
    "# if string_references_age == \"young\":\n",
    "#     time_window_age = 1   \n",
    "#     preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_age) ]   \n",
    "    \n",
    "#     print (\"    size of preselection3 (only young references):\",preselection_df3.shape, string_references_age)\n",
    "\n",
    "# elif string_references_age == \"old\":\n",
    "#     time_window_age = 10    \n",
    "#     preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_age) ]   \n",
    "    \n",
    "#     print (\"    size of preselection3 (only old references):\",preselection_df3.shape,string_references_age )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "# N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "# N_all = len(preselection_df3)\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "# print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    " \n",
    "  \n",
    "\n",
    "# preselection_df3 = preselection_df3.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "# print (\"OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\")\n",
    "  \n",
    "# N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "# N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "# N_all = len(preselection_df3)\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "# print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ############## i define quantiles for plos papers (for that subselection, and based on their FINAL number of citations):\n",
    "# #list_q_plos=[.2,.8,1]\n",
    "# list_q_plos=[.1,.9,1]\n",
    "# #list_q_plos=[.05,.95,1]\n",
    "\n",
    "# df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])   # ojo!!! dont use preselection_df3 directly because there are REPETITIONS!!!!\n",
    "\n",
    "# quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos)#,quantiles, df_for_quantiles_plos.shape)   \n",
    "\n",
    "# lista_bins_plos=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "                           \n",
    "# #print (lista_bins_plos, min(preselection_df3['paper_cite_count']), max(preselection_df3['paper_cite_count']))\n",
    "\n",
    "# print (\"\\nbins for PLOS papers:\")\n",
    "\n",
    "# cont = 0\n",
    "# dict_bin_list_plos_UT={}\n",
    "# for item in lista_bins_plos:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]   \n",
    "\n",
    "#     df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "#     llave=str(minimo)+\"-\"+str(maximo)\n",
    "#     dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "#     print (\" \",llave, \"  N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['paper_UT']).total_refs.mean())\n",
    "#     max_key_plos=llave\n",
    "\n",
    "    \n",
    "#     if cont ==0:\n",
    "#         min_key_plos = llave\n",
    "#     cont  +=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ########## i define quantiles for references (based on their FINAL number of citations)\n",
    "# #list_q_ref=[.2,.8,1]\n",
    "# list_q_ref=[.1,.9,1]\n",
    "\n",
    "# #list_q_ref=[.05,.95,1]\n",
    "# df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "# quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "# lista_bins=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\nbins for refrences:\")\n",
    "        \n",
    "\n",
    "# cont = 0\n",
    "# dict_bin_list_ref_UT={}\n",
    "# for item in lista_bins:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]    \n",
    "     \n",
    "#     df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "#     llave=str(minimo)+\"-\"+str(maximo)\n",
    "#     dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "#     print (\" \",llave, \"N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['reference_UT']).total_refs.mean())\n",
    "#     max_key_ref=llave\n",
    "\n",
    "#     if cont ==0:\n",
    "#         min_key_ref = llave\n",
    "#     cont  +=1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ############### i create the list of top plos, top ref, bottom plos and bottom ref:\n",
    "# #########################\n",
    "\n",
    "# lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "# print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "# lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "# print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "# lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "# print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "# lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "# print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "# list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "# print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######  i look at the usage of the top ref\n",
    "# ################################################  \n",
    "\n",
    "# df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "# df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "# df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "# usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "# usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "# print (\"fraction of usage of top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######  i look at the usage of the non-top ref\n",
    "# ################################################      \n",
    "\n",
    "# df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "# df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "# df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "# usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "# usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "# print (\"fraction of usage of non-top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ####################\n",
    "# # I canculate the null model (usage of references by top and non top plos papers, from the randomized data)\n",
    "# #################################################  \n",
    "\n",
    "# lista_usage_top_ref_by_top_plos_rand = []\n",
    "# lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "# lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "# lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(Niter):\n",
    "    \n",
    "#     print (i)\n",
    "    \n",
    "#     lista_values = list(preselection_df3.paper_UT)   #[i for i in range (len(df_merged))]\n",
    "#     random.shuffle(lista_values)\n",
    "#     preselection_df3['randomized_paper_UT'] = lista_values\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#     ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "#     df_top_ref_rand = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "    \n",
    "#     df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['randomized_paper_UT'].isin(lista_top_plos)]\n",
    "#     df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['randomized_paper_UT'].isin(lista_bottom_plos)]\n",
    "    \n",
    "    \n",
    "#     usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "#     usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "    \n",
    "    \n",
    "#     lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "#     lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "#     df_non_top_ref_rand = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]        \n",
    "    \n",
    "#     df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['randomized_paper_UT'].isin(lista_top_plos)]\n",
    "#     df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['randomized_paper_UT'].isin(lista_bottom_plos)]\n",
    "    \n",
    "    \n",
    "#     usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "#     usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "    \n",
    "    \n",
    "#     lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "#     lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "# print(\"fraction of usage of top ref by\")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\navg randomized\")\n",
    "# print (\"fraction of usage of non-top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_df3.total_refs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###  FIGURE 4A   but bottom papers first, then top papers\n",
    "# ######   FIGURE 4a FOR THE PAPER  (RUN PREVIOUS CELL FIRST, TO GET THE BOOT-STRAPPING DATA)\n",
    "\n",
    "\n",
    "# ### group by Top PApers  Bottom Papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #########################################\n",
    "\n",
    "# #lista_bin_names=['Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers', 'Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers']\n",
    "# lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers', 'Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_for_top_ref = [ usage_top_ref_top_plos, usage_top_ref_bottom_plos]\n",
    "# lista_for_bottom_ref = [usage_non_top_ref_top_plos, usage_non_top_ref_bottom_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_for_top_ref = [  usage_top_ref_bottom_plos, usage_top_ref_top_plos]\n",
    "# lista_for_bottom_ref = [usage_non_top_ref_bottom_plos, usage_non_top_ref_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###### this is the null model \n",
    "\n",
    "\n",
    "# # lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_top_plos_rand),np.mean(lista_usage_top_ref_by_bottom_plos_rand)]  \n",
    "# # lista_expectations_bottom_ref = [np.mean(lista_usage_nontop_ref_by_top_plos_rand), np.mean(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "# lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_bottom_plos_rand), np.mean(lista_usage_top_ref_by_top_plos_rand)]  \n",
    "# lista_expectations_bottom_ref = [ np.mean(lista_usage_nontop_ref_by_bottom_plos_rand),np.mean(lista_usage_nontop_ref_by_top_plos_rand)] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_top_plos_rand), 2.*np.std(lista_usage_top_ref_by_bottom_plos_rand)] \n",
    "# # list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) , 2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "# list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_top_ref_by_top_plos_rand) ] \n",
    "# list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# z_score_top_ref_by_top_plos = (usage_top_ref_top_plos - np.mean(lista_usage_top_ref_by_top_plos_rand))/np.std(lista_usage_top_ref_by_top_plos_rand)\n",
    "# z_score_nontop_ref_by_top_plos = (usage_non_top_ref_top_plos - np.mean(lista_usage_nontop_ref_by_top_plos_rand))/np.std(lista_usage_nontop_ref_by_top_plos_rand)\n",
    "\n",
    "# z_score_top_ref_by_bottom_plos = (usage_top_ref_bottom_plos - np.mean(lista_usage_top_ref_by_bottom_plos_rand))/np.std(lista_usage_top_ref_by_bottom_plos_rand)\n",
    "# z_score_nontop_ref_by_bottom_plos = (usage_non_top_ref_bottom_plos - np.mean(lista_usage_nontop_ref_by_bottom_plos_rand))/np.std(lista_usage_nontop_ref_by_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print ('zscore top ref by top plos:', z_score_top_ref_by_top_plos)\n",
    "# print ('zscore nontop ref by top plos:', z_score_nontop_ref_by_top_plos)\n",
    "\n",
    "# print ('zscore top ref by bottom plos:', z_score_top_ref_by_bottom_plos)\n",
    "# print ('zscore nontop ref by bottom plos:', z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "# title_string=''#'s top-ref by top plos: '+str(z_score_top_ref_by_top_plos)+';  zs top-ref by bottom plos: '+str(z_score_top_ref_by_bottom_plos)+\\\n",
    "# #'<br>zs nontop-ref by top plos: '+str(z_score_nontop_ref_by_top_plos)+';  zs nontop-ref by bottom plos: '+str(z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "# size_bar_name = 30#45\n",
    "# y_pos_bar_names = -.039\n",
    "# angle = -70\n",
    "\n",
    "# trace1 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_for_top_ref,\n",
    "# #     text=['by top '+str(int(100-100*list_q_plos[-2]))+'% papers', 'by top '+str(int(100-100*list_q_plos[-2]))+'% papers'],    \n",
    "# #     name='by top '+str(int(100-100*list_q_plos[-2]))+'% papers',   \n",
    "#     marker=dict(\n",
    "#         color='#88419d',           \n",
    "#     ),\n",
    "    \n",
    "    \n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# trace2 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_expectations_top_ref,\n",
    "# #     name='   expected value',\n",
    "# #     text=['Expected value for top', 'Expected value for top'],  \n",
    "#     error_y=dict(\n",
    "#        # type='data',\n",
    "#         array=list_errors_top_ref,\n",
    "#         thickness=5,\n",
    "#         visible=True\n",
    "#     ),\n",
    "#     marker=dict(\n",
    "#         color='#c994c7',         \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# trace3 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_for_bottom_ref,\n",
    "# #     name='by bottom '+str(int(list_q_plos[0]*100))+'% papers', \n",
    "# #     text=['by bottom '+str(int(list_q_plos[0]*100))+'% papers','by bottom '+str(int(list_q_plos[0]*100))+'% papers'],\n",
    "#     marker=dict(\n",
    "#         color='#225ea8',   \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# trace4 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_expectations_bottom_ref,\n",
    "#   #  name='   expected value',\n",
    "  \n",
    "#     error_y=dict(       \n",
    "#         array=list_errors_bottom_ref,#[0.5, 1, 2],\n",
    "#         thickness=5,\n",
    "#         visible=True\n",
    "#     ),\n",
    "#     marker=dict(\n",
    "#         color='#a6bddb',     \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = [trace1, trace2, trace3, trace4]\n",
    "# layout = go.Layout(   \n",
    "#     title=title_string,\n",
    "#     xaxis = dict(\n",
    "#         side= 'top',\n",
    "#         range = [-.5,1.5],\n",
    "#        # showline =  True,\n",
    "#         #title= 'Plos Citation percentile'),\n",
    "#     ),\n",
    "#     yaxis = dict(\n",
    "#         title= 'Fraction of references cited',\n",
    "#         range = [-.07,0.17],\n",
    "#         tickvals=[0.0,0.05,0.1,0.15],\n",
    "#         #showline =  True,\n",
    "#          ),\n",
    "    \n",
    "#     showlegend=False,\n",
    "# #     legend=dict(x=0.75, y=1.05,                 \n",
    "# #                font=dict(\n",
    "# #                     #family='sans-serif',\n",
    "# #                     size=40,\n",
    "# #                     #color='#000'\n",
    "# #                     ),\n",
    "# #                 ),\n",
    "    \n",
    "    \n",
    "# #     barmode='stacked',#group',\n",
    "#    # bargap=0.2,\n",
    "#     bargroupgap=0.15,\n",
    "    \n",
    "#     annotations = [  \n",
    "#         # the four bars on the left\n",
    "#         dict(\n",
    "#           x = -.34,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),    \n",
    "#         dict(\n",
    "#           x = -0.14,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Null Model',\n",
    "#            textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .08,\n",
    "#           y = y_pos_bar_names,\n",
    "#           showarrow = False,\n",
    "#           text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .28,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text =  'Null Model',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # the four bars on the right\n",
    "#         dict(  \n",
    "#           x = .68,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .88,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Null Model',\n",
    "#            textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = 1.08,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = 1.28,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text =  'Null Model',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "        \n",
    "       \n",
    "#         ],    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=30#55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# # fig['layout']['yaxis']['tickangle'] = -90\n",
    "# # fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "# # fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=100,\n",
    "#         t=200,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "# fig_filename='fract_usage_top_bottom_ref_'+str(years[0])\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ####################### MORE SOPHISTICATED RANDOMIZATION SCHEME: ALSO CONTROLING FOR PLOS FIELD (AS WELL AS PLOS YEAR):   \n",
    "\n",
    "\n",
    "\n",
    "# ##### ADD ALSO LATER CONTROLING FOR REF. FIELD, MAYBE????\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### (i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data)\n",
    "\n",
    "# Niter=1000\n",
    "\n",
    "# years=[2013]\n",
    "\n",
    "# string_references_age=\"all\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "# string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "# string_self_ref=0    #\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"2\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    " \n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "    \n",
    "    \n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection only young/old references:       \n",
    "# preselection_df3 = preselection_df2\n",
    "# if string_references_age == \"young\":\n",
    "#     time_window_age = 1   \n",
    "#     preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_age) ]   \n",
    "    \n",
    "#     print (\"    size of preselection3 (only young references):\",preselection_df3.shape, string_references_age)\n",
    "\n",
    "# elif string_references_age == \"old\":\n",
    "#     time_window_age = 10    \n",
    "#     preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_age) ]   \n",
    "    \n",
    "#     print (\"    size of preselection3 (only old references):\",preselection_df3.shape,string_references_age )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "# N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "# N_all = len(preselection_df3)\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "# print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    " \n",
    "  \n",
    "\n",
    "# preselection_df3 = preselection_df3.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "# print (\"OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\")\n",
    "  \n",
    "# N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "# N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "# N_all = len(preselection_df3)\n",
    "\n",
    "    \n",
    "    \n",
    "         \n",
    "# print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ############## i define quantiles for plos papers (for that subselection, and based on their FINAL number of citations):\n",
    "# #list_q_plos=[.2,.8,1]\n",
    "# list_q_plos=[.1,.9,1]\n",
    "# #list_q_plos=[.05,.95,1]\n",
    "\n",
    "# df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])   # ojo!!! dont use preselection_df3 directly because there are REPETITIONS!!!!\n",
    "\n",
    "# quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos)#,quantiles, df_for_quantiles_plos.shape)   \n",
    "\n",
    "# lista_bins_plos=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "                           \n",
    "# #print (lista_bins_plos, min(preselection_df3['paper_cite_count']), max(preselection_df3['paper_cite_count']))\n",
    "\n",
    "# print (\"\\nbins for PLOS papers:\")\n",
    "\n",
    "# cont = 0\n",
    "# dict_bin_list_plos_UT={}\n",
    "# for item in lista_bins_plos:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]   \n",
    "\n",
    "#     df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "#     llave=str(minimo)+\"-\"+str(maximo)\n",
    "#     dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "#     print (\" \",llave, \"  N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['paper_UT']).total_refs.mean())\n",
    "#     max_key_plos=llave\n",
    "\n",
    "    \n",
    "#     if cont ==0:\n",
    "#         min_key_plos = llave\n",
    "#     cont  +=1\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ########## i define quantiles for references (based on their FINAL number of citations)\n",
    "# #list_q_ref=[.2,.8,1]\n",
    "# list_q_ref=[.1,.9,1]\n",
    "\n",
    "# #list_q_ref=[.05,.95,1]\n",
    "# df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "# quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    " \n",
    "# print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "# lista_bins=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "                           \n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\nbins for refrences:\")\n",
    "        \n",
    "\n",
    "# cont = 0\n",
    "# dict_bin_list_ref_UT={}\n",
    "# for item in lista_bins:\n",
    "    \n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]    \n",
    "     \n",
    "#     df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "#     llave=str(minimo)+\"-\"+str(maximo)\n",
    "#     dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "#     print (\" \",llave, \"N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['reference_UT']).total_refs.mean())\n",
    "#     max_key_ref=llave\n",
    "\n",
    "#     if cont ==0:\n",
    "#         min_key_ref = llave\n",
    "#     cont  +=1\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ############### i create the list of top plos, top ref, bottom plos and bottom ref:\n",
    "# #########################\n",
    "\n",
    "# lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "# print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "# lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "# print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "# lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "# print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "# lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "# print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "# list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "# print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######  i look at the usage of the top ref\n",
    "# ################################################  \n",
    "\n",
    "# df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "# df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "# df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "# usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "# usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "# print (\"fraction of usage of top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######  i look at the usage of the non-top ref\n",
    "# ################################################      \n",
    "\n",
    "# df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "# df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "# df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "# usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "# usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "# print (\"fraction of usage of non-top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ####################\n",
    "# # I canculate the null model (usage of references by top and non top plos papers, from the randomized data)\n",
    "# #################################################  \n",
    "\n",
    "# lista_usage_top_ref_by_top_plos_rand = []\n",
    "# lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "# lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "# lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "# for i in range(Niter):\n",
    "    \n",
    "#     print (i)\n",
    "    \n",
    "#     lista_values = list(preselection_df3.paper_UT)   #[i for i in range (len(df_merged))]\n",
    "#     random.shuffle(lista_values)\n",
    "#     preselection_df3['randomized_paper_UT'] = lista_values\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#     ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "#     df_top_ref_rand = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "    \n",
    "#     df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['randomized_paper_UT'].isin(lista_top_plos)]\n",
    "#     df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['randomized_paper_UT'].isin(lista_bottom_plos)]\n",
    "    \n",
    "    \n",
    "#     usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "#     usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "    \n",
    "    \n",
    "#     lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "#     lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "#     #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "#     df_non_top_ref_rand = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]        \n",
    "    \n",
    "#     df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['randomized_paper_UT'].isin(lista_top_plos)]\n",
    "#     df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['randomized_paper_UT'].isin(lista_bottom_plos)]\n",
    "    \n",
    "    \n",
    "#     usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "#     usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "    \n",
    "    \n",
    "#     lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "#     lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# print(\"plos category:\", string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "# print(\"fraction of usage of top ref by\")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\navg randomized\")\n",
    "# print (\"fraction of usage of non-top ref by \")\n",
    "# print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "# print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #############################  second part\n",
    "\n",
    "\n",
    "# ###  PLOT FIGURE FOR MORE SOPHISTICATED VERSION OF THE RANDOMIZATION SCHEME\n",
    "# ######     (RUN PREVIOUS CELL FIRST, TO GET THE BOOT-STRAPPING DATA)\n",
    "\n",
    "\n",
    "# ### group by Top PApers  Bottom Papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #########################################\n",
    "\n",
    "# #lista_bin_names=['Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers', 'Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers']\n",
    "# lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers', 'Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_for_top_ref = [ usage_top_ref_top_plos, usage_top_ref_bottom_plos]\n",
    "# lista_for_bottom_ref = [usage_non_top_ref_top_plos, usage_non_top_ref_bottom_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_for_top_ref = [  usage_top_ref_bottom_plos, usage_top_ref_top_plos]\n",
    "# lista_for_bottom_ref = [usage_non_top_ref_bottom_plos, usage_non_top_ref_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ###### this is the null model \n",
    "\n",
    "\n",
    "# # lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_top_plos_rand),np.mean(lista_usage_top_ref_by_bottom_plos_rand)]  \n",
    "# # lista_expectations_bottom_ref = [np.mean(lista_usage_nontop_ref_by_top_plos_rand), np.mean(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "# lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_bottom_plos_rand), np.mean(lista_usage_top_ref_by_top_plos_rand)]  \n",
    "# lista_expectations_bottom_ref = [ np.mean(lista_usage_nontop_ref_by_bottom_plos_rand),np.mean(lista_usage_nontop_ref_by_top_plos_rand)] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_top_plos_rand), 2.*np.std(lista_usage_top_ref_by_bottom_plos_rand)] \n",
    "# # list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) , 2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "# list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_top_ref_by_top_plos_rand) ] \n",
    "# list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# z_score_top_ref_by_top_plos = (usage_top_ref_top_plos - np.mean(lista_usage_top_ref_by_top_plos_rand))/np.std(lista_usage_top_ref_by_top_plos_rand)\n",
    "# z_score_nontop_ref_by_top_plos = (usage_non_top_ref_top_plos - np.mean(lista_usage_nontop_ref_by_top_plos_rand))/np.std(lista_usage_nontop_ref_by_top_plos_rand)\n",
    "\n",
    "# z_score_top_ref_by_bottom_plos = (usage_top_ref_bottom_plos - np.mean(lista_usage_top_ref_by_bottom_plos_rand))/np.std(lista_usage_top_ref_by_bottom_plos_rand)\n",
    "# z_score_nontop_ref_by_bottom_plos = (usage_non_top_ref_bottom_plos - np.mean(lista_usage_nontop_ref_by_bottom_plos_rand))/np.std(lista_usage_nontop_ref_by_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print ('zscore top ref cited  by top plos:', z_score_top_ref_by_top_plos)\n",
    "# print ('zscore bottom ref cited  by top plos:', z_score_nontop_ref_by_top_plos)\n",
    "\n",
    "# print ('zscore top ref cited  by bottom plos:', z_score_top_ref_by_bottom_plos)\n",
    "# print ('zscore bottom ref cited  by bottom plos:', z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "# title_string=''#'s top-ref by top plos: '+str(z_score_top_ref_by_top_plos)+';  zs top-ref by bottom plos: '+str(z_score_top_ref_by_bottom_plos)+\\\n",
    "# #'<br>zs nontop-ref by top plos: '+str(z_score_nontop_ref_by_top_plos)+';  zs nontop-ref by bottom plos: '+str(z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "# size_bar_name = 30#45\n",
    "# y_pos_bar_names = -.039\n",
    "# angle = -70\n",
    "\n",
    "# trace1 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_for_top_ref,\n",
    "# #     text=['by top '+str(int(100-100*list_q_plos[-2]))+'% papers', 'by top '+str(int(100-100*list_q_plos[-2]))+'% papers'],    \n",
    "# #     name='by top '+str(int(100-100*list_q_plos[-2]))+'% papers',   \n",
    "#     marker=dict(\n",
    "#         color='#88419d',           \n",
    "#     ),\n",
    "    \n",
    "    \n",
    "    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# trace2 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_expectations_top_ref,\n",
    "# #     name='   expected value',\n",
    "# #     text=['Expected value for top', 'Expected value for top'],  \n",
    "#     error_y=dict(\n",
    "#        # type='data',\n",
    "#         array=list_errors_top_ref,\n",
    "#         thickness=5,\n",
    "#         visible=True\n",
    "#     ),\n",
    "#     marker=dict(\n",
    "#         color='#c994c7',         \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# trace3 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_for_bottom_ref,\n",
    "# #     name='by bottom '+str(int(list_q_plos[0]*100))+'% papers', \n",
    "# #     text=['by bottom '+str(int(list_q_plos[0]*100))+'% papers','by bottom '+str(int(list_q_plos[0]*100))+'% papers'],\n",
    "#     marker=dict(\n",
    "#         color='#225ea8',   \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "# trace4 = go.Bar(\n",
    "#     x=lista_bin_names,\n",
    "#     y=lista_expectations_bottom_ref,\n",
    "#   #  name='   expected value',\n",
    "  \n",
    "#     error_y=dict(       \n",
    "#         array=list_errors_bottom_ref,#[0.5, 1, 2],\n",
    "#         thickness=5,\n",
    "#         visible=True\n",
    "#     ),\n",
    "#     marker=dict(\n",
    "#         color='#a6bddb',     \n",
    "#     ),\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = [trace1, trace2, trace3, trace4]\n",
    "# layout = go.Layout(   \n",
    "#     title=title_string,\n",
    "#     xaxis = dict(\n",
    "#         side= 'top',\n",
    "#         range = [-.5,1.5],\n",
    "#        # showline =  True,\n",
    "#         #title= 'Plos Citation percentile'),\n",
    "#     ),\n",
    "#     yaxis = dict(\n",
    "#         title= 'Fraction of references cited',\n",
    "#         range = [-.07,0.17],\n",
    "#         tickvals=[0.0,0.05,0.1,0.15],\n",
    "#         #showline =  True,\n",
    "#          ),\n",
    "    \n",
    "#     showlegend=False,\n",
    "# #     legend=dict(x=0.75, y=1.05,                 \n",
    "# #                font=dict(\n",
    "# #                     #family='sans-serif',\n",
    "# #                     size=40,\n",
    "# #                     #color='#000'\n",
    "# #                     ),\n",
    "# #                 ),\n",
    "    \n",
    "    \n",
    "# #     barmode='stacked',#group',\n",
    "#    # bargap=0.2,\n",
    "#     bargroupgap=0.15,\n",
    "    \n",
    "#     annotations = [  \n",
    "#         # the four bars on the left\n",
    "#         dict(\n",
    "#           x = -.34,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),    \n",
    "#         dict(\n",
    "#           x = -0.14,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Null Model',\n",
    "#            textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .08,\n",
    "#           y = y_pos_bar_names,\n",
    "#           showarrow = False,\n",
    "#           text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .28,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text =  'Null Model',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "        \n",
    "        \n",
    "        \n",
    "#         # the four bars on the right\n",
    "#         dict(  \n",
    "#           x = .68,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .88,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Null Model',\n",
    "#            textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = 1.08,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = 1.28,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text =  'Null Model',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "        \n",
    "       \n",
    "#         ],    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=30#55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# # fig['layout']['yaxis']['tickangle'] = -90\n",
    "# # fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "# # fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=100,\n",
    "#         t=200,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "# fig_filename='fract_usage_top_bottom_ref_'+str(years[0])\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### i need this function to do the randomization scheme that preserves groups of references cited together in  a paper\n",
    "\n",
    "def get_list_lists_references(preselection_df3):\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    ####  NOTA: preselection_df3 only includes one instance of paper_UT-ref_UT  (the first occurrence in each paper), and i sort it too:\n",
    "    preselection_df3.sort_values(by=['paper_UT','reference_UT'], inplace=True)\n",
    "\n",
    "\n",
    "    distance_threshold = 5  # characters tops to separate group ref\n",
    "\n",
    "    list_lists_all_ref = []  # WITH STRUCTURE\n",
    "    lista_ref = []           # WITHOUT STRUCTURE\n",
    "    cont = 0\n",
    "    for paper_UT, group_df in preselection_df3.groupby(['paper_UT']):  #### OJO!!!! THIS LOOP IS WAY FASTER THAN DOING:  for   paper_UT in list_paper_UT    !!!!    \n",
    "\n",
    "        group_df.sort_values(by=['regex_sect_index','sect_char_pos','reference_UT'],inplace = True)  # i sort the reference of a paper by section first, then by location within the section\n",
    "\n",
    "\n",
    "\n",
    "        ### first i take care of the isolated references: \n",
    "        group_df1= group_df[group_df['isolated_citation'] ==1]        \n",
    "        for index, row in group_df1.iterrows():  \n",
    "\n",
    "            ref_UT = row['reference_UT']\n",
    "            aux = [ref_UT]\n",
    "            lista_ref.append(ref_UT)   \n",
    "            list_lists_all_ref.append(aux)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #### then i take care of the group references:\n",
    "        group_df0= group_df[group_df['isolated_citation'] == 0]  \n",
    "        previous_position = 0\n",
    "        list_group_ref = []\n",
    "        for index, row in group_df0.iterrows():\n",
    "\n",
    "            ref_UT = row['reference_UT']\n",
    "            lista_ref.append(ref_UT)   # list without structure (for comparison reasons)\n",
    "\n",
    "            position = row['sect_char_pos']\n",
    "\n",
    "            if previous_position == 0:  # for the very first entry                \n",
    "                list_group_ref.append(ref_UT)\n",
    "\n",
    "\n",
    "            else:  # for all other entries\n",
    "                if (position - previous_position) <= distance_threshold  :   # if the current ref is close to the previous one\n",
    "                     list_group_ref.append(ref_UT)\n",
    "                else: \n",
    "                    list_lists_all_ref.append(list_group_ref)\n",
    "                    list_group_ref = []\n",
    "                    list_group_ref.append(ref_UT)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            previous_position = position\n",
    "\n",
    "        list_lists_all_ref.append(list_group_ref)  # i need this for the final group/isolated one!!!\n",
    "\n",
    "        cont +=1   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### i flatten out the list of lists for comparison purposes\n",
    "\n",
    "    flat_list = []    \n",
    "    for sublist in list_lists_all_ref:\n",
    "        for item in sublist:\n",
    "            flat_list.append(item)\n",
    "\n",
    "    print (\"list_lists created:\", len(list_lists_all_ref), ' without structure:', len(lista_ref), \"   flat_list:\", len(flat_list))\n",
    "\n",
    "\n",
    "    # for i in range(len(flat_list)):\n",
    "    #     print (lista_ref[i], flat_list[i])\n",
    "    \n",
    "\n",
    "    return list_lists_all_ref\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ####################### RANDOMIZATION SCHEME ALSO CONTROLING FOR PLOS FIELD (AS WELL AS PLOS YEAR):   \n",
    "\n",
    "\n",
    "\n",
    "# ##### ADD ALSO LATER CONTROLING FOR REF. FIELD, MAYBE????\n",
    "\n",
    "\n",
    "\n",
    "# Niter=1000\n",
    "\n",
    "# years=[2009]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# #string_code_categ=\"4\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "# lista_categ = ['0']#,'1','2','3','4','5','6','7','8','9','10']\n",
    "\n",
    "\n",
    "# for string_code_categ in lista_categ:\n",
    "\n",
    "#     #### (i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data)\n",
    "\n",
    "   \n",
    "\n",
    "#     string_references_age=\"all\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "#     string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#     string_self_ref=0    #\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### plos journals \n",
    "#     string_journal=\"\"\n",
    "\n",
    "#         # PLOS ONE       6,367,070\n",
    "#         # PLOS GENET      149,923\n",
    "#         # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#         # PLOS PATHOG     109,803\n",
    "#         # PLOS COMPUT      77,924\n",
    "#         # PLOS BIOL        56,754\n",
    "#         # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#     ######### WoS subject categories. \n",
    "#     string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "#     # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "#     # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "#     # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "#     # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "#     # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "#     # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "#     # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "#     # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "#     # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "#     # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection by plos year\n",
    "#     print (years)\n",
    "#     preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "#     print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #### i remove self-citations\n",
    "#     if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#         preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#         if string_self_ref ==0:\n",
    "#             string_self_ref = \", no self-cit\"\n",
    "#         elif string_self_ref ==1:\n",
    "#             string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by isolated or group references:\n",
    "#     if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#         preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "#         if string_isolated_ref ==0:\n",
    "#             string_isolated_ref = \", group ref\"\n",
    "#         elif string_isolated_ref ==1:\n",
    "#             string_isolated_ref = \", isolated ref\"\n",
    "#     else:    \n",
    "#         preselection_df0 = preselection_df   \n",
    "#         print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by plos ONE subject category:\n",
    "#     if string_code_categ==\"\": \n",
    "#         preselection_df111 = preselection_df0\n",
    "#     else:    \n",
    "#         if \" \" not in string_code_categ:  # to include one single category\n",
    "#             preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#             string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "#         else:  # if multiple codes-categories\n",
    "#             list_codes = string_code_categ.split(\" \")\n",
    "#             print (list_codes)\n",
    "\n",
    "#             if len(list_codes) >= 2:              \n",
    "#                 preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "#             string_code_categ = \"\" \n",
    "#             for code in list_codes:\n",
    "#                 string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "#         print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by plos journal:\n",
    "#     if string_journal==\"\": \n",
    "#         preselection_df1 = preselection_df111\n",
    "#     else:    \n",
    "#         preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "#     print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by plos field:\n",
    "#     if string_plos_field==\"\": \n",
    "#         preselection_df2 = preselection_df1\n",
    "#     else:    \n",
    "#         preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "#     print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection only young/old references:       \n",
    "#     preselection_df3 = preselection_df2\n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window_age = 1   \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_age) ]   \n",
    "\n",
    "#         print (\"    size of preselection3 (only young references):\",preselection_df3.shape, string_references_age)\n",
    "\n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window_age = 10    \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_age) ]   \n",
    "\n",
    "#         print (\"    size of preselection3 (only old references):\",preselection_df3.shape,string_references_age )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "#     N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "#     N_all = len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "#     preselection_df3 = preselection_df3.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "#     print (\"OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\")\n",
    "\n",
    "#     N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "#     N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "#     N_all = len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ############## i define quantiles for plos papers (for that subselection, and based on their FINAL number of citations):\n",
    "#     #list_q_plos=[.2,.8,1]\n",
    "#     list_q_plos=[.1,.9,1]\n",
    "#     #list_q_plos=[.05,.95,1]\n",
    "\n",
    "#     df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])   # ojo!!! dont use preselection_df3 directly because there are REPETITIONS!!!!\n",
    "\n",
    "#     quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) #mean 10.68 \n",
    "\n",
    "#     print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos)#,quantiles, df_for_quantiles_plos.shape)   \n",
    "\n",
    "#     lista_bins_plos=[]\n",
    "#     old_value=0\n",
    "#     for item in quantiles:\n",
    "#         pair=[old_value, int(item[1])]\n",
    "#         lista_bins_plos.append(pair)\n",
    "#         old_value = int(item[1])\n",
    "\n",
    "#     #print (lista_bins_plos, min(preselection_df3['paper_cite_count']), max(preselection_df3['paper_cite_count']))\n",
    "\n",
    "#     print (\"\\nbins for PLOS papers:\")\n",
    "\n",
    "#     cont = 0\n",
    "#     dict_bin_list_plos_UT={}\n",
    "#     for item in lista_bins_plos:\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]   \n",
    "\n",
    "#         df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "#         llave=str(minimo)+\"-\"+str(maximo)\n",
    "#         dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "#         print (\" \",llave, \"  N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['paper_UT']).total_refs.mean())\n",
    "#         max_key_plos=llave\n",
    "\n",
    "\n",
    "#         if cont ==0:\n",
    "#             min_key_plos = llave\n",
    "#         cont  +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ########## i define quantiles for references (based on their FINAL number of citations)\n",
    "#     #list_q_ref=[.2,.8,1]\n",
    "#     list_q_ref=[.1,.9,1]\n",
    "\n",
    "#     #list_q_ref=[.05,.95,1]\n",
    "#     df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "#     quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    "\n",
    "#     print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "#     lista_bins=[]\n",
    "#     old_value=0\n",
    "#     for item in quantiles:\n",
    "#         pair=[old_value, int(item[1])]\n",
    "#         lista_bins.append(pair)\n",
    "#         old_value = int(item[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"\\nbins for refrences:\")\n",
    "\n",
    "\n",
    "#     cont = 0\n",
    "#     dict_bin_list_ref_UT={}\n",
    "#     for item in lista_bins:\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]    \n",
    "\n",
    "#         df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "#         llave=str(minimo)+\"-\"+str(maximo)\n",
    "#         dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "#         print (\" \",llave, \"N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['reference_UT']).total_refs.mean())\n",
    "#         max_key_ref=llave\n",
    "\n",
    "#         if cont ==0:\n",
    "#             min_key_ref = llave\n",
    "#         cont  +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ############### i create the list of top plos, top ref, bottom plos and bottom ref:\n",
    "#     #########################\n",
    "\n",
    "#     lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "#     print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "#     lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "#     print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "#     lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "#     print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "#     lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "#     print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "#     list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "#     print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######  i look at the usage of the top ref\n",
    "#     ################################################  \n",
    "\n",
    "#     df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "#     df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "#     df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "#     usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "#     usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "#     print (\"fraction of usage of top ref by \")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######  i look at the usage of the non-top ref\n",
    "#     ################################################      \n",
    "\n",
    "#     df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "#     df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "#     df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "#     usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "#     usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "#     print (\"fraction of usage of non-top ref by \")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ####################\n",
    "#     # I canculate the null model (usage of references by top and non top plos papers, from the randomized data)\n",
    "#     #################################################  \n",
    "\n",
    "#     lista_usage_top_ref_by_top_plos_rand = []\n",
    "#     lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "#     lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "#     lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "#     for i in range(Niter):\n",
    "\n",
    "#         print (i)\n",
    "\n",
    "#         lista_values = list(preselection_df3.reference_UT)   #[i for i in range (len(df_merged))]\n",
    "#         random.shuffle(lista_values)\n",
    "#         preselection_df3['randomized_ref_UT'] = lista_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "#         df_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_top_ref)]\n",
    "\n",
    "#         df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "#         df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "#         usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "#         usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "\n",
    "\n",
    "#         lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "#         lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "#         df_non_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_bottom_ref)]        \n",
    "\n",
    "#         df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "#         df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "#         usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "#         usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "\n",
    "\n",
    "#         lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "#         lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"plos category:\", string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "#     print(\"fraction of usage of top ref by\")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"\\n\\navg randomized\")\n",
    "#     print (\"fraction of usage of non-top ref by \")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #############################  second part\n",
    "\n",
    "\n",
    "#     ###  PLOT FIGURE FOR MORE SOPHISTICATED VERSION OF THE RANDOMIZATION SCHEME\n",
    "#     ######     (RUN PREVIOUS CELL FIRST, TO GET THE BOOT-STRAPPING DATA)\n",
    "\n",
    "\n",
    "#     ### group by Top PApers  Bottom Papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #########################################\n",
    "\n",
    "#     #lista_bin_names=['Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers', 'Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers']\n",
    "#     lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers', 'Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     lista_for_top_ref = [ usage_top_ref_top_plos, usage_top_ref_bottom_plos]\n",
    "#     lista_for_bottom_ref = [usage_non_top_ref_top_plos, usage_non_top_ref_bottom_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     lista_for_top_ref = [  usage_top_ref_bottom_plos, usage_top_ref_top_plos]\n",
    "#     lista_for_bottom_ref = [usage_non_top_ref_bottom_plos, usage_non_top_ref_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ###### this is the null model \n",
    "\n",
    "\n",
    "#     # lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_top_plos_rand),np.mean(lista_usage_top_ref_by_bottom_plos_rand)]  \n",
    "#     # lista_expectations_bottom_ref = [np.mean(lista_usage_nontop_ref_by_top_plos_rand), np.mean(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "#     lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_bottom_plos_rand), np.mean(lista_usage_top_ref_by_top_plos_rand)]  \n",
    "#     lista_expectations_bottom_ref = [ np.mean(lista_usage_nontop_ref_by_bottom_plos_rand),np.mean(lista_usage_nontop_ref_by_top_plos_rand)] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_top_plos_rand), 2.*np.std(lista_usage_top_ref_by_bottom_plos_rand)] \n",
    "#     # list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) , 2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "#     list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_top_ref_by_top_plos_rand) ] \n",
    "#     list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     z_score_top_ref_by_top_plos = (usage_top_ref_top_plos - np.mean(lista_usage_top_ref_by_top_plos_rand))/np.std(lista_usage_top_ref_by_top_plos_rand)\n",
    "#     z_score_nontop_ref_by_top_plos = (usage_non_top_ref_top_plos - np.mean(lista_usage_nontop_ref_by_top_plos_rand))/np.std(lista_usage_nontop_ref_by_top_plos_rand)\n",
    "\n",
    "#     z_score_top_ref_by_bottom_plos = (usage_top_ref_bottom_plos - np.mean(lista_usage_top_ref_by_bottom_plos_rand))/np.std(lista_usage_top_ref_by_bottom_plos_rand)\n",
    "#     z_score_nontop_ref_by_bottom_plos = (usage_non_top_ref_bottom_plos - np.mean(lista_usage_nontop_ref_by_bottom_plos_rand))/np.std(lista_usage_nontop_ref_by_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print ('zscore top ref cited  by top plos:', z_score_top_ref_by_top_plos)\n",
    "#     print ('zscore bottom ref cited  by top plos:', z_score_nontop_ref_by_top_plos)\n",
    "\n",
    "#     print ('zscore top ref cited  by bottom plos:', z_score_top_ref_by_bottom_plos)\n",
    "#     print ('zscore bottom ref cited  by bottom plos:', z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "#     title_string=''#'s top-ref by top plos: '+str(z_score_top_ref_by_top_plos)+';  zs top-ref by bottom plos: '+str(z_score_top_ref_by_bottom_plos)+\\\n",
    "#     #'<br>zs nontop-ref by top plos: '+str(z_score_nontop_ref_by_top_plos)+';  zs nontop-ref by bottom plos: '+str(z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "#     size_bar_name = 30#45\n",
    "#     y_pos_bar_names = -.039\n",
    "#     angle = -70\n",
    "\n",
    "#     trace1 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_for_top_ref,\n",
    "#     #     text=['by top '+str(int(100-100*list_q_plos[-2]))+'% papers', 'by top '+str(int(100-100*list_q_plos[-2]))+'% papers'],    \n",
    "#     #     name='by top '+str(int(100-100*list_q_plos[-2]))+'% papers',   \n",
    "#         marker=dict(\n",
    "#             color='#88419d',           \n",
    "#         ),\n",
    "\n",
    "\n",
    "\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "#     trace2 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_expectations_top_ref,\n",
    "#     #     name='   expected value',\n",
    "#     #     text=['Expected value for top', 'Expected value for top'],  \n",
    "#         error_y=dict(\n",
    "#            # type='data',\n",
    "#             array=list_errors_top_ref,\n",
    "#             thickness=5,\n",
    "#             visible=True\n",
    "#         ),\n",
    "#         marker=dict(\n",
    "#             color='#c994c7',         \n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trace3 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_for_bottom_ref,\n",
    "#     #     name='by bottom '+str(int(list_q_plos[0]*100))+'% papers', \n",
    "#     #     text=['by bottom '+str(int(list_q_plos[0]*100))+'% papers','by bottom '+str(int(list_q_plos[0]*100))+'% papers'],\n",
    "#         marker=dict(\n",
    "#             color='#225ea8',   \n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trace4 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_expectations_bottom_ref,\n",
    "#       #  name='   expected value',\n",
    "\n",
    "#         error_y=dict(       \n",
    "#             array=list_errors_bottom_ref,#[0.5, 1, 2],\n",
    "#             thickness=5,\n",
    "#             visible=True\n",
    "#         ),\n",
    "#         marker=dict(\n",
    "#             color='#a6bddb',     \n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     data = [trace1, trace2, trace3, trace4]\n",
    "#     layout = go.Layout(   \n",
    "#         title=title_string,\n",
    "#         xaxis = dict(\n",
    "#             side= 'top',\n",
    "#             range = [-.5,1.5],\n",
    "#            # showline =  True,\n",
    "#             #title= 'Plos Citation percentile'),\n",
    "#         ),\n",
    "#         yaxis = dict(\n",
    "#             title= 'Fraction of references cited',\n",
    "#             range = [-.07,0.17],\n",
    "#             tickvals=[0.0,0.05,0.1,0.15],\n",
    "#             #showline =  True,\n",
    "#              ),\n",
    "\n",
    "#         showlegend=False,\n",
    "#     #     legend=dict(x=0.75, y=1.05,                 \n",
    "#     #                font=dict(\n",
    "#     #                     #family='sans-serif',\n",
    "#     #                     size=40,\n",
    "#     #                     #color='#000'\n",
    "#     #                     ),\n",
    "#     #                 ),\n",
    "\n",
    "\n",
    "#     #     barmode='stacked',#group',\n",
    "#        # bargap=0.2,\n",
    "#         bargroupgap=0.15,\n",
    "\n",
    "#         annotations = [  \n",
    "#             # the four bars on the left\n",
    "#             dict(\n",
    "#               x = -.34,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),    \n",
    "#             dict(\n",
    "#               x = -0.14,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Null Model',\n",
    "#                textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = .08,\n",
    "#               y = y_pos_bar_names,\n",
    "#               showarrow = False,\n",
    "#               text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = .28,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text =  'Null Model',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "\n",
    "\n",
    "\n",
    "#             # the four bars on the right\n",
    "#             dict(  \n",
    "#               x = .68,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = .88,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Null Model',\n",
    "#                textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = 1.08,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = 1.28,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text =  'Null Model',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "\n",
    "\n",
    "#             ],    \n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     font_gral=30#55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "#     fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "#     # Altering x axis\n",
    "#     #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "#     fig['layout']['xaxis']['tickangle'] = 0\n",
    "#     # fig['layout']['yaxis']['tickangle'] = -90\n",
    "#     # fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "#     # fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "#     fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "#     fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     fig['layout']['margin']=dict(\n",
    "#             l=200,\n",
    "#            # r=50,\n",
    "#             b=100,\n",
    "#             t=200,\n",
    "#             pad=15\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "#     fig_filename='null_model_fract_usage_top_bottom_ref_'+str(years[0])+string_code_categ.replace(\" \",\"_\")+\"_\"+str(Niter)+\"iter\"\n",
    "#     offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ####################### RANDOMIZATION SCHEME ALSO CONTROLING FOR PLOS FIELD (AS WELL AS PLOS YEAR):   \n",
    "\n",
    "\n",
    "\n",
    "# ##### ADD ALSO LATER CONTROLING FOR REF. FIELD, MAYBE????\n",
    "\n",
    "\n",
    "\n",
    "# Niter=10\n",
    "\n",
    "# years=[2011]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# #string_code_categ=\"4\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "# lista_categ = ['0']#,'1','2','3','4','5','6','7','8','9','10']\n",
    "\n",
    "\n",
    "# for string_code_categ in lista_categ:\n",
    "\n",
    "#     #### (i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data)\n",
    "\n",
    "   \n",
    "\n",
    "#     string_references_age=\"all\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "#     string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "#     string_self_ref=0    #\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### plos journals \n",
    "#     string_journal=\"\"\n",
    "\n",
    "#         # PLOS ONE       6,367,070\n",
    "#         # PLOS GENET      149,923\n",
    "#         # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#         # PLOS PATHOG     109,803\n",
    "#         # PLOS COMPUT      77,924\n",
    "#         # PLOS BIOL        56,754\n",
    "#         # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "#     ######### WoS subject categories. \n",
    "#     string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "#     # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "#     # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "#     # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "#     # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "#     # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "#     # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "#     # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "#     # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "#     # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "#     # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection by plos year\n",
    "#     print (years)\n",
    "#     preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "#     print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #### i remove self-citations\n",
    "#     if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#         preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#         if string_self_ref ==0:\n",
    "#             string_self_ref = \", no self-cit\"\n",
    "#         elif string_self_ref ==1:\n",
    "#             string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by isolated or group references:\n",
    "#     if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#         preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "#         if string_isolated_ref ==0:\n",
    "#             string_isolated_ref = \", group ref\"\n",
    "#         elif string_isolated_ref ==1:\n",
    "#             string_isolated_ref = \", isolated ref\"\n",
    "#     else:    \n",
    "#         preselection_df0 = preselection_df   \n",
    "#         print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by plos ONE subject category:\n",
    "#     if string_code_categ==\"\": \n",
    "#         preselection_df111 = preselection_df0\n",
    "#     else:    \n",
    "#         if \" \" not in string_code_categ:  # to include one single category\n",
    "#             preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#             string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "#         else:  # if multiple codes-categories\n",
    "#             list_codes = string_code_categ.split(\" \")\n",
    "#             print (list_codes)\n",
    "\n",
    "#             if len(list_codes) >= 2:              \n",
    "#                 preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "#             string_code_categ = \"\" \n",
    "#             for code in list_codes:\n",
    "#                 string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "#         print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by plos journal:\n",
    "#     if string_journal==\"\": \n",
    "#         preselection_df1 = preselection_df111\n",
    "#     else:    \n",
    "#         preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "#     print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######### preselection by plos field:\n",
    "#     if string_plos_field==\"\": \n",
    "#         preselection_df2 = preselection_df1\n",
    "#     else:    \n",
    "#         preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "#     print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection only young/old references:       \n",
    "#     preselection_df3 = preselection_df2\n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window_age = 1   \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_age) ]   \n",
    "\n",
    "#         print (\"    size of preselection3 (only young references):\",preselection_df3.shape, string_references_age)\n",
    "\n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window_age = 10    \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_age) ]   \n",
    "\n",
    "#         print (\"    size of preselection3 (only old references):\",preselection_df3.shape,string_references_age )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "#     N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "#     N_all = len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "#     preselection_df3 = preselection_df3.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "#     print (\"OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\")\n",
    "\n",
    "#     N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "#     N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "#     N_all = len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ############## i define quantiles for plos papers (for that subselection, and based on their FINAL number of citations):\n",
    "#     #list_q_plos=[.2,.8,1]\n",
    "#     list_q_plos=[.1,.9,1]\n",
    "#     #list_q_plos=[.05,.95,1]\n",
    "\n",
    "#     df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])   # ojo!!! dont use preselection_df3 directly because there are REPETITIONS!!!!\n",
    "\n",
    "#     quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) #mean 10.68 \n",
    "\n",
    "#     print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos)#,quantiles, df_for_quantiles_plos.shape)   \n",
    "\n",
    "#     lista_bins_plos=[]\n",
    "#     old_value=0\n",
    "#     for item in quantiles:\n",
    "#         pair=[old_value, int(item[1])]\n",
    "#         lista_bins_plos.append(pair)\n",
    "#         old_value = int(item[1])\n",
    "\n",
    "#     #print (lista_bins_plos, min(preselection_df3['paper_cite_count']), max(preselection_df3['paper_cite_count']))\n",
    "\n",
    "#     print (\"\\nbins for PLOS papers:\")\n",
    "\n",
    "#     cont = 0\n",
    "#     dict_bin_list_plos_UT={}\n",
    "#     for item in lista_bins_plos:\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]   \n",
    "\n",
    "#         df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "#         llave=str(minimo)+\"-\"+str(maximo)\n",
    "#         dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "#         print (\" \",llave, \"  N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['paper_UT']).total_refs.mean())\n",
    "#         max_key_plos=llave\n",
    "\n",
    "\n",
    "#         if cont ==0:\n",
    "#             min_key_plos = llave\n",
    "#         cont  +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ########## i define quantiles for references (based on their FINAL number of citations)\n",
    "#     #list_q_ref=[.2,.8,1]\n",
    "#     list_q_ref=[.1,.9,1]\n",
    "\n",
    "#     #list_q_ref=[.05,.95,1]\n",
    "#     df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "#     quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    "\n",
    "#     print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "#     lista_bins=[]\n",
    "#     old_value=0\n",
    "#     for item in quantiles:\n",
    "#         pair=[old_value, int(item[1])]\n",
    "#         lista_bins.append(pair)\n",
    "#         old_value = int(item[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"\\nbins for refrences:\")\n",
    "\n",
    "\n",
    "#     cont = 0\n",
    "#     dict_bin_list_ref_UT={}\n",
    "#     for item in lista_bins:\n",
    "\n",
    "#         minimo = item[0]\n",
    "#         maximo = item[1]    \n",
    "\n",
    "#         df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "#         llave=str(minimo)+\"-\"+str(maximo)\n",
    "#         dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "#         print (\" \",llave, \"N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['reference_UT']).total_refs.mean())\n",
    "#         max_key_ref=llave\n",
    "\n",
    "#         if cont ==0:\n",
    "#             min_key_ref = llave\n",
    "#         cont  +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ############### i create the list of top plos, top ref, bottom plos and bottom ref:\n",
    "#     #########################\n",
    "\n",
    "#     lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "#     print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "#     lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "#     print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "#     lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "#     print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "#     lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "#     print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "#     list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "#     print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######  i look at the usage of the top ref\n",
    "#     ################################################  \n",
    "\n",
    "#     df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "#     df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "#     df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "#     usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "#     usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "#     print (\"fraction of usage of top ref by \")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ######  i look at the usage of the non-top ref\n",
    "#     ################################################      \n",
    "\n",
    "#     df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "#     df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "#     df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "#     usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "#     usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "#     print (\"fraction of usage of non-top ref by \")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ####################\n",
    "#     # I canculate the null model (usage of references by top and non top plos papers, from the randomized data)\n",
    "#     #################################################  \n",
    "\n",
    "#     lista_usage_top_ref_by_top_plos_rand = []\n",
    "#     lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "#     lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "#     lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "#     for i in range(Niter):\n",
    "\n",
    "#         print (i)\n",
    "\n",
    "#         lista_values = list(preselection_df3.reference_UT)   #[i for i in range (len(df_merged))]\n",
    "#         random.shuffle(lista_values)\n",
    "#         preselection_df3['randomized_ref_UT'] = lista_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "#         df_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_top_ref)]\n",
    "\n",
    "#         df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "#         df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "#         usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "#         usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "\n",
    "\n",
    "#         lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "#         lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "#         df_non_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_bottom_ref)]        \n",
    "\n",
    "#         df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "#         df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "#         usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "#         usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "\n",
    "\n",
    "#         lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "#         lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print(\"plos category:\", string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "#     print(\"fraction of usage of top ref by\")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print (\"\\n\\navg randomized\")\n",
    "#     print (\"fraction of usage of non-top ref by \")\n",
    "#     print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "#     print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #############################  second part\n",
    "\n",
    "\n",
    "#     ###  PLOT FIGURE FOR MORE SOPHISTICATED VERSION OF THE RANDOMIZATION SCHEME\n",
    "#     ######     (RUN PREVIOUS CELL FIRST, TO GET THE BOOT-STRAPPING DATA)\n",
    "\n",
    "\n",
    "#     ### group by Top PApers  Bottom Papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     #########################################\n",
    "\n",
    "#     #lista_bin_names=['Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers', 'Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers']\n",
    "#     lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers', 'Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     lista_for_top_ref = [ usage_top_ref_top_plos, usage_top_ref_bottom_plos]\n",
    "#     lista_for_bottom_ref = [usage_non_top_ref_top_plos, usage_non_top_ref_bottom_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     lista_for_top_ref = [  usage_top_ref_bottom_plos, usage_top_ref_top_plos]\n",
    "#     lista_for_bottom_ref = [usage_non_top_ref_bottom_plos, usage_non_top_ref_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     ###### this is the null model \n",
    "\n",
    "\n",
    "#     # lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_top_plos_rand),np.mean(lista_usage_top_ref_by_bottom_plos_rand)]  \n",
    "#     # lista_expectations_bottom_ref = [np.mean(lista_usage_nontop_ref_by_top_plos_rand), np.mean(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "#     lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_bottom_plos_rand), np.mean(lista_usage_top_ref_by_top_plos_rand)]  \n",
    "#     lista_expectations_bottom_ref = [ np.mean(lista_usage_nontop_ref_by_bottom_plos_rand),np.mean(lista_usage_nontop_ref_by_top_plos_rand)] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     # list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_top_plos_rand), 2.*np.std(lista_usage_top_ref_by_bottom_plos_rand)] \n",
    "#     # list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) , 2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "#     list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_top_ref_by_top_plos_rand) ] \n",
    "#     list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     z_score_top_ref_by_top_plos = (usage_top_ref_top_plos - np.mean(lista_usage_top_ref_by_top_plos_rand))/np.std(lista_usage_top_ref_by_top_plos_rand)\n",
    "#     z_score_nontop_ref_by_top_plos = (usage_non_top_ref_top_plos - np.mean(lista_usage_nontop_ref_by_top_plos_rand))/np.std(lista_usage_nontop_ref_by_top_plos_rand)\n",
    "\n",
    "#     z_score_top_ref_by_bottom_plos = (usage_top_ref_bottom_plos - np.mean(lista_usage_top_ref_by_bottom_plos_rand))/np.std(lista_usage_top_ref_by_bottom_plos_rand)\n",
    "#     z_score_nontop_ref_by_bottom_plos = (usage_non_top_ref_bottom_plos - np.mean(lista_usage_nontop_ref_by_bottom_plos_rand))/np.std(lista_usage_nontop_ref_by_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     print ('zscore top ref cited  by top plos:', z_score_top_ref_by_top_plos)\n",
    "#     print ('zscore bottom ref cited  by top plos:', z_score_nontop_ref_by_top_plos)\n",
    "\n",
    "#     print ('zscore top ref cited  by bottom plos:', z_score_top_ref_by_bottom_plos)\n",
    "#     print ('zscore bottom ref cited  by bottom plos:', z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "#     title_string=''#'s top-ref by top plos: '+str(z_score_top_ref_by_top_plos)+';  zs top-ref by bottom plos: '+str(z_score_top_ref_by_bottom_plos)+\\\n",
    "#     #'<br>zs nontop-ref by top plos: '+str(z_score_nontop_ref_by_top_plos)+';  zs nontop-ref by bottom plos: '+str(z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "#     size_bar_name = 30#45\n",
    "#     y_pos_bar_names = -.039\n",
    "#     angle = -70\n",
    "\n",
    "#     trace1 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_for_top_ref,\n",
    "#     #     text=['by top '+str(int(100-100*list_q_plos[-2]))+'% papers', 'by top '+str(int(100-100*list_q_plos[-2]))+'% papers'],    \n",
    "#     #     name='by top '+str(int(100-100*list_q_plos[-2]))+'% papers',   \n",
    "#         marker=dict(\n",
    "#             color='#88419d',           \n",
    "#         ),\n",
    "\n",
    "\n",
    "\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "#     trace2 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_expectations_top_ref,\n",
    "#     #     name='   expected value',\n",
    "#     #     text=['Expected value for top', 'Expected value for top'],  \n",
    "#         error_y=dict(\n",
    "#            # type='data',\n",
    "#             array=list_errors_top_ref,\n",
    "#             thickness=5,\n",
    "#             visible=True\n",
    "#         ),\n",
    "#         marker=dict(\n",
    "#             color='#c994c7',         \n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trace3 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_for_bottom_ref,\n",
    "#     #     name='by bottom '+str(int(list_q_plos[0]*100))+'% papers', \n",
    "#     #     text=['by bottom '+str(int(list_q_plos[0]*100))+'% papers','by bottom '+str(int(list_q_plos[0]*100))+'% papers'],\n",
    "#         marker=dict(\n",
    "#             color='#225ea8',   \n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "#     trace4 = go.Bar(\n",
    "#         x=lista_bin_names,\n",
    "#         y=lista_expectations_bottom_ref,\n",
    "#       #  name='   expected value',\n",
    "\n",
    "#         error_y=dict(       \n",
    "#             array=list_errors_bottom_ref,#[0.5, 1, 2],\n",
    "#             thickness=5,\n",
    "#             visible=True\n",
    "#         ),\n",
    "#         marker=dict(\n",
    "#             color='#a6bddb',     \n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     data = [trace1, trace2, trace3, trace4]\n",
    "#     layout = go.Layout(   \n",
    "#         title=title_string,\n",
    "#         xaxis = dict(\n",
    "#             side= 'top',\n",
    "#             range = [-.5,1.5],\n",
    "#            # showline =  True,\n",
    "#             #title= 'Plos Citation percentile'),\n",
    "#         ),\n",
    "#         yaxis = dict(\n",
    "#             title= 'Fraction of references cited',\n",
    "#             range = [-.07,0.17],\n",
    "#             tickvals=[0.0,0.05,0.1,0.15],\n",
    "#             #showline =  True,\n",
    "#              ),\n",
    "\n",
    "#         showlegend=False,\n",
    "#     #     legend=dict(x=0.75, y=1.05,                 \n",
    "#     #                font=dict(\n",
    "#     #                     #family='sans-serif',\n",
    "#     #                     size=40,\n",
    "#     #                     #color='#000'\n",
    "#     #                     ),\n",
    "#     #                 ),\n",
    "\n",
    "\n",
    "#     #     barmode='stacked',#group',\n",
    "#        # bargap=0.2,\n",
    "#         bargroupgap=0.15,\n",
    "\n",
    "#         annotations = [  \n",
    "#             # the four bars on the left\n",
    "#             dict(\n",
    "#               x = -.34,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),    \n",
    "#             dict(\n",
    "#               x = -0.14,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Null Model',\n",
    "#                textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = .08,\n",
    "#               y = y_pos_bar_names,\n",
    "#               showarrow = False,\n",
    "#               text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = .28,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text =  'Null Model',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "\n",
    "\n",
    "\n",
    "#             # the four bars on the right\n",
    "#             dict(  \n",
    "#               x = .68,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = .88,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Null Model',\n",
    "#                textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = 1.08,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "#             dict(\n",
    "#               x = 1.28,\n",
    "#               y = y_pos_bar_names,\n",
    "#               text =  'Null Model',\n",
    "#               textangle=angle,\n",
    "#                 font = dict(size = size_bar_name ),\n",
    "#                ),\n",
    "\n",
    "\n",
    "#             ],    \n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     font_gral=30#55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "#     fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "#     # Altering x axis\n",
    "#     #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "#     fig['layout']['xaxis']['tickangle'] = 0\n",
    "#     # fig['layout']['yaxis']['tickangle'] = -90\n",
    "#     # fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "#     # fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "#     fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "#     fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     fig['layout']['margin']=dict(\n",
    "#             l=200,\n",
    "#            # r=50,\n",
    "#             b=100,\n",
    "#             t=200,\n",
    "#             pad=15\n",
    "#         )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "#     fig_filename='null_model_fract_usage_top_bottom_ref_'+str(years[0])+string_code_categ.replace(\" \",\"_\")+\"_\"+str(Niter)+\"iter\"\n",
    "#     offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original size: (5787630, 34)\n",
      "[2008]\n",
      "size of preselection1 (by plos years): (104056, 34)\n",
      "size of preselection1 (by isolated/group ref): (104056, 34) \n",
      " size of preselection (by plos ONE subject category): (95977, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (95977, 34) \n",
      " size of preselection2 (by plos field): (95977, 34) \n",
      "     N plos: 2687   N  ref: 69604  N records: 95977\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 2687   N  ref: 69604  N records: 79000\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-7   N: 6060   avg # ref: 38.58\n",
      "  7-76   N: 57481   avg # ref: 45.280369515\n",
      "  76-787   N: 8569   avg # ref: 60.3948339483\n",
      "string for bottom: [0-7] citations  string for top: [76-787] citations\n",
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 22.0), (0.9, 549.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-22 N: 6664   avg # ref: 53.6332533013\n",
      "  22-549 N: 55966   avg # ref: 54.1429439302\n",
      "  549-326393 N: 6973   avg # ref: 54.1751039725\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 271\n",
      "# UTs top 10.0 % ref: 6973\n",
      "# UTs bottom  10.0 % plos: 250\n",
      "# UTs bottom  10.0 % ref: 6664\n",
      "Tot # records: 79000 , # plos: 2687\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.154388320187951\n",
      "  bottom 10.0 % plos: 0.06981037086759523\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.07648809523809524\n",
      "  bottom 10.0 % plos: 0.10416666666666667\n",
      "list_lists created: 59282  without structure: 79000    flat_list: 79000\n",
      "len list_lists_all_ref: 59282 (79000, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.115149102198\n",
      "  bottom 10.0 % plos: 0.0785280248364\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.115208333333\n",
      "  bottom 10.0 % plos: 0.0785160714286 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 13.8313325765\n",
      "zscore bottom ref cited  by top plos: -10.5225361207\n",
      "zscore top ref cited  by bottom plos: -3.65586113121\n",
      "zscore bottom ref cited  by bottom plos: 7.92632979384\n",
      "original size: (5787630, 34)\n",
      "[2009]\n",
      "size of preselection1 (by plos years): (171018, 34)\n",
      "size of preselection1 (by isolated/group ref): (171018, 34) \n",
      " size of preselection (by plos ONE subject category): (160543, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (160543, 34) \n",
      " size of preselection2 (by plos field): (160543, 34) \n",
      "     N plos: 4338   N  ref: 114269  N records: 160543\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 4338   N  ref: 114269  N records: 133149\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-6   N: 10095   avg # ref: 39.2780612245\n",
      "  6-62   N: 94987   avg # ref: 46.2429022082\n",
      "  62-421   N: 14080   avg # ref: 49.8406113537\n",
      "string for bottom: [0-6] citations  string for top: [62-421] citations\n",
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 20.0), (0.9, 486.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-20 N: 10627   avg # ref: 54.4327655971\n",
      "  20-486 N: 92181   avg # ref: 54.6185114069\n",
      "  486-326393 N: 11460   avg # ref: 52.7073298429\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 458\n",
      "# UTs top 10.0 % ref: 11460\n",
      "# UTs bottom  10.0 % plos: 392\n",
      "# UTs bottom  10.0 % ref: 10627\n",
      "Tot # records: 133149 , # plos: 4338\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.1453225198038476\n",
      "  bottom 10.0 % plos: 0.06931346661637118\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.0723531055033057\n",
      "  bottom 10.0 % plos: 0.10503771300866002\n",
      "list_lists created: 97647  without structure: 133149    flat_list: 133149\n",
      "len list_lists_all_ref: 97647 (133149, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.112810401735\n",
      "  bottom 10.0 % plos: 0.0773081855903\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.112972157557\n",
      "  bottom 10.0 % plos: 0.0773504981842 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 14.8180201563\n",
      "zscore bottom ref cited  by top plos: -13.1521855726\n",
      "zscore top ref cited  by bottom plos: -4.49222429217\n",
      "zscore bottom ref cited  by bottom plos: 10.9654572164\n",
      "original size: (5787630, 34)\n",
      "[2010]\n",
      "size of preselection1 (by plos years): (269213, 34)\n",
      "size of preselection1 (by isolated/group ref): (269213, 34) \n",
      " size of preselection (by plos ONE subject category): (256358, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (256358, 34) \n",
      " size of preselection2 (by plos field): (256358, 34) \n",
      "     N plos: 6617   N  ref: 177852  N records: 256358\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 6617   N  ref: 177852  N records: 212655\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-5   N: 16930   avg # ref: 40.9208074534\n",
      "  5-50   N: 148691   avg # ref: 47.5238274628\n",
      "  50-1173   N: 20971   avg # ref: 50.1719457014\n",
      "string for bottom: [0-5] citations  string for top: [50-1173] citations\n",
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 18.0), (0.9, 426.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-18 N: 16447   avg # ref: 67.7423846294\n",
      "  18-426 N: 143615   avg # ref: 59.9671273892\n",
      "  426-326393 N: 17789   avg # ref: 54.4957558041\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 663\n",
      "# UTs top 10.0 % ref: 17789\n",
      "# UTs bottom  10.0 % plos: 644\n",
      "# UTs bottom  10.0 % ref: 16447\n",
      "Tot # records: 212655 , # plos: 6617\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.1384816899838359\n",
      "  bottom 10.0 % plos: 0.07912045036508555\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.06856628674265147\n",
      "  bottom 10.0 % plos: 0.10767846430713857\n",
      "list_lists created: 151086  without structure: 212655    flat_list: 212655\n",
      "len list_lists_all_ref: 151086 (212655, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.107422356613\n",
      "  bottom 10.0 % plos: 0.082161613065\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.107416256749\n",
      "  bottom 10.0 % plos: 0.0821946610678 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 18.8626511475\n",
      "zscore bottom ref cited  by top plos: -15.6911966507\n",
      "zscore top ref cited  by bottom plos: -2.0425680791\n",
      "zscore bottom ref cited  by bottom plos: 11.3460640909\n",
      "original size: (5787630, 34)\n",
      "[2011]\n",
      "size of preselection1 (by plos years): (564251, 34)\n",
      "size of preselection1 (by isolated/group ref): (564251, 34) \n",
      " size of preselection (by plos ONE subject category): (528594, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (528594, 34) \n",
      " size of preselection2 (by plos field): (528594, 34) \n",
      "     N plos: 13477   N  ref: 339696  N records: 528594\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 13477   N  ref: 339696  N records: 438207\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-4   N: 32083   avg # ref: 41.2131687243\n",
      "  4-37   N: 287206   avg # ref: 47.5549228508\n",
      "  37-856   N: 42845   avg # ref: 51.2716678806\n",
      "string for bottom: [0-4] citations  string for top: [37-856] citations\n",
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 16.0), (0.9, 351.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-16 N: 30829   avg # ref: 54.9616594765\n",
      "  16-351 N: 274796   avg # ref: 55.2204362509\n",
      "  351-326393 N: 34070   avg # ref: 53.6065453478\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 1373\n",
      "# UTs top 10.0 % ref: 34070\n",
      "# UTs bottom  10.0 % plos: 1215\n",
      "# UTs bottom  10.0 % ref: 30829\n",
      "Tot # records: 438207 , # plos: 13477\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.1307634164777022\n",
      "  bottom 10.0 % plos: 0.07369614512471655\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.07484619553090434\n",
      "  bottom 10.0 % plos: 0.09881737910809346\n",
      "list_lists created: 311469  without structure: 438207    flat_list: 438207\n",
      "len list_lists_all_ref: 311469 (438207, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.109464982521\n",
      "  bottom 10.0 % plos: 0.0767942649282\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.109457492589\n",
      "  bottom 10.0 % plos: 0.0767133020943 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 20.298660416\n",
      "zscore bottom ref cited  by top plos: -19.7330087534\n",
      "zscore top ref cited  by bottom plos: -3.39751668872\n",
      "zscore bottom ref cited  by bottom plos: 14.4961206692\n",
      "original size: (5787630, 34)\n",
      "[2012]\n",
      "size of preselection1 (by plos years): (1008811, 34)\n",
      "size of preselection1 (by isolated/group ref): (1008811, 34) \n",
      " size of preselection (by plos ONE subject category): (885007, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (885007, 34) \n",
      " size of preselection2 (by plos field): (885007, 34) \n",
      "     N plos: 22873   N  ref: 546992  N records: 885007\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 22873   N  ref: 546992  N records: 738448\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-2   N: 33388   avg # ref: 41.5019275251\n",
      "  2-26   N: 477497   avg # ref: 47.7307210031\n",
      "  26-404   N: 73248   avg # ref: 51.5716632444\n",
      "string for bottom: [0-2] citations  string for top: [26-404] citations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 14.0), (0.9, 297.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-14 N: 51794   avg # ref: 54.9897864618\n",
      "  14-297 N: 440366   avg # ref: 55.6722499012\n",
      "  297-326393 N: 54831   avg # ref: 54.3393883022\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 2435\n",
      "# UTs top 10.0 % ref: 54831\n",
      "# UTs bottom  10.0 % plos: 1297\n",
      "# UTs bottom  10.0 % ref: 51794\n",
      "Tot # records: 738448 , # plos: 22873\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.1360779610587919\n",
      "  bottom 10.0 % plos: 0.0458486522619586\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.07743054246616654\n",
      "  bottom 10.0 % plos: 0.06355788441964791\n",
      "list_lists created: 534674  without structure: 738448    flat_list: 738448\n",
      "len list_lists_all_ref: 534674 (738448, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.114518667794\n",
      "  bottom 10.0 % plos: 0.0470473035409\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.114547988842\n",
      "  bottom 10.0 % plos: 0.0470806348249 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 28.2155839268\n",
      "zscore bottom ref cited  by top plos: -26.1915524664\n",
      "zscore top ref cited  by bottom plos: -2.28483970224\n",
      "zscore bottom ref cited  by bottom plos: 17.7668293305\n",
      "original size: (5787630, 34)\n",
      "[2013]\n",
      "size of preselection1 (by plos years): (1280020, 34)\n",
      "size of preselection1 (by isolated/group ref): (1280020, 34) \n",
      " size of preselection (by plos ONE subject category): (1043096, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (1043096, 34) \n",
      " size of preselection2 (by plos field): (1043096, 34) \n",
      "     N plos: 28391   N  ref: 650376  N records: 1043096\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 28391   N  ref: 650376  N records: 876962\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-1   N: 33228   avg # ref: 42.5361305361\n",
      "  1-18   N: 571191   avg # ref: 47.5194023656\n",
      "  18-269   N: 88640   avg # ref: 51.65625\n",
      "string for bottom: [0-1] citations  string for top: [18-269] citations\n",
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 12.0), (0.9, 273.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-12 N: 60599   avg # ref: 54.3966567105\n",
      "  12-273 N: 524488   avg # ref: 55.449707143\n",
      "  273-326393 N: 65288   avg # ref: 54.5998498958\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 3008\n",
      "# UTs top 10.0 % ref: 65288\n",
      "# UTs bottom  10.0 % plos: 1287\n",
      "# UTs bottom  10.0 % ref: 60599\n",
      "Tot # records: 876962 , # plos: 28391\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.1326942730507113\n",
      "  bottom 10.0 % plos: 0.03745279461353066\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.08086505301833886\n",
      "  bottom 10.0 % plos: 0.049537499597125084\n",
      "list_lists created: 643989  without structure: 876962    flat_list: 876962\n",
      "len list_lists_all_ref: 643989 (876962, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.115852713729\n",
      "  bottom 10.0 % plos: 0.0391522595736\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.115793695813\n",
      "  bottom 10.0 % plos: 0.0391512875882 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 23.4660327295\n",
      "zscore bottom ref cited  by top plos: -27.4203396469\n",
      "zscore top ref cited  by bottom plos: -3.79122775382\n",
      "zscore bottom ref cited  by bottom plos: 12.9985010868\n",
      "original size: (5787630, 34)\n",
      "[2014]\n",
      "size of preselection1 (by plos years): (1072594, 34)\n",
      "size of preselection1 (by isolated/group ref): (1072594, 34) \n",
      " size of preselection (by plos ONE subject category): (991762, 34)  Biology and life sciences\n",
      " size of preselection2 (by plos journal): (991762, 34) \n",
      " size of preselection2 (by plos field): (991762, 34) \n",
      "     N plos: 27595   N  ref: 643363  N records: 991762\n",
      "OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\n",
      "     N plos: 27595   N  ref: 643363  N records: 836432\n",
      "\n",
      "\n",
      "citation bins for the selected plos: [0.1, 0.9, 1]\n",
      "\n",
      "bins for PLOS papers:\n",
      "  0-1   N: 62193   avg # ref: 41.9829268293\n",
      "  1-11   N: 538613   avg # ref: 47.6956737269\n",
      "  11-162   N: 86726   avg # ref: 52.066236413\n",
      "string for bottom: [0-1] citations  string for top: [11-162] citations\n",
      "\n",
      "\n",
      "citation bins for the references in the selected plos: [0.1, 0.9, 1] [(0.1, 10.0), (0.9, 264.0), (1.0, 326393.0)]\n",
      "\n",
      "bins for refrences:\n",
      "  0-10 N: 58246   avg # ref: 54.2088212066\n",
      "  10-264 N: 520561   avg # ref: 55.5768737958\n",
      "  264-326393 N: 64555   avg # ref: 54.7609635195\n",
      "\n",
      "\n",
      "# UTs top 10.0 % plos: 2944\n",
      "# UTs top 10.0 % ref: 64555\n",
      "# UTs bottom  10.0 % plos: 2460\n",
      "# UTs bottom  10.0 % ref: 58246\n",
      "Tot # records: 836432 , # plos: 27595\n",
      "fraction of usage of top ref by \n",
      "  top 10.0 % plos: 0.12576351944399664\n",
      "  bottom 10.0 % plos: 0.0746114484628107\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.08497105936195988\n",
      "  bottom 10.0 % plos: 0.10011441647597254\n",
      "list_lists created: 629164  without structure: 836432    flat_list: 836432\n",
      "len list_lists_all_ref: 629164 (836432, 34)\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "plos category:  Biology and life sciences\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "avg randomized!!\n",
      "fraction of usage of top ref by\n",
      "  top 10.0 % plos: 0.115432646091\n",
      "  bottom 10.0 % plos: 0.0777205414346\n",
      "\n",
      "\n",
      "avg randomized\n",
      "fraction of usage of non-top ref by \n",
      "  top 10.0 % plos: 0.115498569794\n",
      "  bottom 10.0 % plos: 0.0776325043747 \n",
      "\n",
      "\n",
      "\n",
      "zscore top ref cited  by top plos: 13.2454164705\n",
      "zscore bottom ref cited  by top plos: -23.8618192974\n",
      "zscore top ref cited  by bottom plos: -4.82790344429\n",
      "zscore bottom ref cited  by bottom plos: 20.5661361629\n"
     ]
    }
   ],
   "source": [
    "#######################  MORE SOPHISTICATED RANDOMIZATION SCHEME: ALSO CONTROLING FOR PLOS FIELD  (AS WELL AS PLOS YEAR)  AND PRESERVING REFERENCES THAT ARE CITED TOGETHER IN A GROUP\n",
    "############################################## ####################### ####################### ####################### ####################### ####################### \n",
    "\n",
    "\n",
    "### Figure 4A\n",
    "\n",
    "\n",
    "\n",
    "#### (i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### (i compare the usafe of top/nontop references by top/nontop plos papers with a null model that comes from randomizing the data)\n",
    "\n",
    "Niter=1000\n",
    "\n",
    "list_list_years = [[2008],[2009],[2010],[2011],[2012],[2013],[2014]]\n",
    "\n",
    "#years=[2008]\n",
    "    \n",
    "for years in list_list_years:\n",
    "   \n",
    "\n",
    "    string_references_age=\"all\"   #young\"#old\"  # young # all   for the selection of what references i include\n",
    "    string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "    string_self_ref=0    #\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### plos ONE categories. \n",
    "    string_code_categ=\"0\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "    #  '0': 'Biology and life sciences'             6,032,537\n",
    "    #  '1': 'Computer and information sciences'     1,207,799\n",
    "    #  '10': 'Social sciences'                      755,899\n",
    "    #  '2': 'Earth sciences'                        533,155\n",
    "    #  '3': 'Ecology and environmental sciences'    624,142\n",
    "    #  '4': 'Engineering and technology'            382,247 \n",
    "    #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "    #  '6': 'People and places'                     691,523\n",
    "    #  '7': 'Physical sciences'                     2,100,827\n",
    "    #  '8': 'Research and analysis methods'         3,871,470\n",
    "    #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # for string_code_categ in range(11):\n",
    "    #     string_code_categ = str(string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### plos journals \n",
    "    string_journal=\"\"\n",
    "\n",
    "        # PLOS ONE       6,367,070\n",
    "        # PLOS GENET      149,923\n",
    "        # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "        # PLOS PATHOG     109,803\n",
    "        # PLOS COMPUT      77,924\n",
    "        # PLOS BIOL        56,754\n",
    "        # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "    ######### WoS subject categories. \n",
    "    string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "    # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "    # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "    # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "    # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "    # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "    # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "    # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "    # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "    # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "    # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### preselection by plos year\n",
    "    print (years)\n",
    "    preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "    print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### i remove self-citations\n",
    "    if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "        preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "        if string_self_ref ==0:\n",
    "            string_self_ref = \", no self-cit\"\n",
    "        elif string_self_ref ==1:\n",
    "            string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by isolated or group references:\n",
    "    if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "        preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "        if string_isolated_ref ==0:\n",
    "            string_isolated_ref = \", group ref\"\n",
    "        elif string_isolated_ref ==1:\n",
    "            string_isolated_ref = \", isolated ref\"\n",
    "    else:    \n",
    "        preselection_df0 = preselection_df   \n",
    "        print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos ONE subject category:\n",
    "    if string_code_categ==\"\": \n",
    "        preselection_df111 = preselection_df0\n",
    "    else:    \n",
    "        if \" \" not in string_code_categ:  # to include one single category\n",
    "            preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "            string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "        else:  # if multiple codes-categories\n",
    "            list_codes = string_code_categ.split(\" \")\n",
    "            print (list_codes)\n",
    "\n",
    "            if len(list_codes) >= 2:              \n",
    "                preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "            string_code_categ = \"\" \n",
    "            for code in list_codes:\n",
    "                string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "        print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos journal:\n",
    "    if string_journal==\"\": \n",
    "        preselection_df1 = preselection_df111\n",
    "    else:    \n",
    "        preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "    print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######### preselection by plos field:\n",
    "    if string_plos_field==\"\": \n",
    "        preselection_df2 = preselection_df1\n",
    "    else:    \n",
    "        preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "    print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### preselection only young/old references:       \n",
    "    preselection_df3 = preselection_df2\n",
    "    if string_references_age == \"young\":\n",
    "        time_window_age = 1   \n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window_age) ]   \n",
    "\n",
    "        print (\"    size of preselection3 (only young references):\",preselection_df3.shape, string_references_age)\n",
    "\n",
    "    elif string_references_age == \"old\":\n",
    "        time_window_age = 10    \n",
    "        preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window_age) ]   \n",
    "\n",
    "        print (\"    size of preselection3 (only old references):\",preselection_df3.shape,string_references_age )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "    N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "    N_all = len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "    preselection_df3 = preselection_df3.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "    print (\"OJO!!! EACH REFERENCE ONLY COUNTED ONCE PER PAPER:\")\n",
    "\n",
    "    N_plos = len(preselection_df3.paper_UT.unique())         \n",
    "    N_ref = len(preselection_df3.reference_UT.unique()) \n",
    "    N_all = len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"     N plos:\", N_plos,\"  N  ref:\",N_ref, \" N records:\", N_all)        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############## i define quantiles for plos papers (for that subselection, and based on their FINAL number of citations):\n",
    "    list_q_plos=[.1,.9,1]\n",
    "    #list_q_plos=[.05,.95,1]\n",
    "\n",
    "    df_for_quantiles_plos = preselection_df3.drop_duplicates(subset=['paper_UT'])   # ojo!!! dont use preselection_df3 directly because there are REPETITIONS!!!!\n",
    "\n",
    "    quantiles=sorted(list(df_for_quantiles_plos['paper_cite_count'].quantile(list_q_plos).to_dict().items())) #mean 10.68 \n",
    "\n",
    "    print (\"\\n\\ncitation bins for the selected plos:\", list_q_plos)#,quantiles, df_for_quantiles_plos.shape)   \n",
    "\n",
    "    lista_bins_plos=[]\n",
    "    old_value=0\n",
    "    for item in quantiles:\n",
    "        pair=[old_value, int(item[1])]\n",
    "        lista_bins_plos.append(pair)\n",
    "        old_value = int(item[1])\n",
    "\n",
    "    #print (lista_bins_plos, min(preselection_df3['paper_cite_count']), max(preselection_df3['paper_cite_count']))\n",
    "\n",
    "    print (\"\\nbins for PLOS papers:\")\n",
    "\n",
    "    cont = 0\n",
    "    dict_bin_list_plos_UT={}\n",
    "    for item in lista_bins_plos:\n",
    "\n",
    "        minimo = item[0]\n",
    "        maximo = item[1]   \n",
    "\n",
    "        df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]\n",
    "        llave=str(minimo)+\"-\"+str(maximo)\n",
    "        dict_bin_list_plos_UT[llave]= list(df_select.paper_UT.unique())\n",
    "        print (\" \",llave, \"  N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['paper_UT']).total_refs.mean())\n",
    "        max_key_plos=llave\n",
    "\n",
    "\n",
    "        if cont ==0:\n",
    "            min_key_plos = llave\n",
    "            string_range_bottom_plos =   \"[\"+llave+\"] citations\"\n",
    "\n",
    "\n",
    "        cont  +=1\n",
    "\n",
    "\n",
    "\n",
    "        string_range_top_plos =  \"[\"+llave+\"] citations\"  \n",
    "\n",
    "    print (\"string for bottom:\", string_range_bottom_plos, \" string for top:\", string_range_top_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ########## i define quantiles for references (based on their FINAL number of citations)\n",
    "    #list_q_ref=[.2,.8,1]\n",
    "    list_q_ref=[.1,.9,1]\n",
    "\n",
    "    #list_q_ref=[.05,.95,1]\n",
    "    df_for_quantiles_ref = preselection_df3.drop_duplicates(subset=['reference_UT'])   # ojo!!! remember to remove REPETITIONS!!!!\n",
    "    quantiles=sorted(list(df_for_quantiles_ref['cite_count'].quantile(list_q_ref).to_dict().items())) #mean 10.68 \n",
    "\n",
    "    print (\"\\n\\ncitation bins for the references in the selected plos:\", list_q_ref,quantiles)    \n",
    "\n",
    "    lista_bins=[]\n",
    "    old_value=0\n",
    "    for item in quantiles:\n",
    "        pair=[old_value, int(item[1])]\n",
    "        lista_bins.append(pair)\n",
    "        old_value = int(item[1])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"\\nbins for refrences:\")\n",
    "\n",
    "\n",
    "    cont = 0\n",
    "    dict_bin_list_ref_UT={}\n",
    "    for item in lista_bins:\n",
    "\n",
    "        minimo = item[0]\n",
    "        maximo = item[1]    \n",
    "\n",
    "        df_select = preselection_df3[(preselection_df3['cite_count'] >= minimo)  &  (preselection_df3['cite_count'] < maximo)]\n",
    "        llave=str(minimo)+\"-\"+str(maximo)\n",
    "        dict_bin_list_ref_UT[llave]=list(df_select.reference_UT.unique())\n",
    "        print (\" \",llave, \"N:\",len(list(df_select.reference_UT.unique())), \"  avg # ref:\",df_select.drop_duplicates(subset=['reference_UT']).total_refs.mean())\n",
    "        max_key_ref=llave\n",
    "\n",
    "        if cont ==0:\n",
    "            min_key_ref = llave\n",
    "        cont  +=1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ############### i create the list of top plos, top ref, bottom plos and bottom ref:\n",
    "    #########################\n",
    "\n",
    "    lista_top_plos = dict_bin_list_plos_UT[max_key_plos]\n",
    "    print (\"\\n\\n# UTs top\",(100-100*list_q_plos[-2]),\"% plos:\",len(lista_top_plos))\n",
    "\n",
    "    lista_top_ref=dict_bin_list_ref_UT[max_key_ref]\n",
    "    print (\"# UTs top\",(100-100*list_q_ref[-2]),\"% ref:\", len(lista_top_ref))\n",
    "\n",
    "\n",
    "    lista_bottom_plos = dict_bin_list_plos_UT[min_key_plos]\n",
    "    print (\"# UTs bottom \",(100*list_q_plos[0]),\"% plos:\",len(lista_bottom_plos))\n",
    "\n",
    "    lista_bottom_ref=dict_bin_list_ref_UT[min_key_ref]\n",
    "    print (\"# UTs bottom \",(100*list_q_ref[0]),\"% ref:\", len(lista_bottom_ref))\n",
    "\n",
    "    list_plos_in_year= list(preselection_df3.paper_UT.unique())\n",
    "    print (\"Tot # records:\",len(preselection_df3),\", # plos:\",len(list_plos_in_year))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######  i look at the usage of the top ref\n",
    "    ################################################  \n",
    "\n",
    "    df_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "    df_top_ref_top_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "    df_top_ref_bottom_plos = df_top_ref[df_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "    usage_top_ref_top_plos = len(df_top_ref_top_plos)/float(len(df_top_ref))\n",
    "    usage_top_ref_bottom_plos = len(df_top_ref_bottom_plos)/float(len(df_top_ref))\n",
    "\n",
    "\n",
    "    print (\"fraction of usage of top ref by \")\n",
    "    print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  usage_top_ref_top_plos)\n",
    "    print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",usage_top_ref_bottom_plos  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ######  i look at the usage of the non-top ref\n",
    "    ################################################      \n",
    "\n",
    "    df_non_top_ref = preselection_df3[preselection_df3['reference_UT'].isin(lista_bottom_ref)]\n",
    "\n",
    "\n",
    "    df_non_top_ref_top_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_top_plos)]\n",
    "    df_non_top_ref_bottom_plos = df_non_top_ref[df_non_top_ref['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "    usage_non_top_ref_top_plos = len(df_non_top_ref_top_plos)/float(len(df_non_top_ref))\n",
    "    usage_non_top_ref_bottom_plos = len(df_non_top_ref_bottom_plos)/float(len(df_non_top_ref))\n",
    "\n",
    "\n",
    "    print (\"fraction of usage of non-top ref by \")\n",
    "    print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", usage_non_top_ref_top_plos )\n",
    "    print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", usage_non_top_ref_bottom_plos )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ####################\n",
    "    # I canculate the null model (usage of references by top and non top plos papers, from the randomized data)\n",
    "    #################################################  \n",
    "\n",
    "    lista_usage_top_ref_by_top_plos_rand = []\n",
    "    lista_usage_top_ref_by_bottom_plos_rand = []\n",
    "\n",
    "    lista_usage_nontop_ref_by_top_plos_rand = []\n",
    "    lista_usage_nontop_ref_by_bottom_plos_rand = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #### first i get the list of lists corresponding to the references used in the selected df, preserving the reference grouping or isolation:  \n",
    "    lista_lists_values = get_list_lists_references(preselection_df3)\n",
    "\n",
    "    print (\"len list_lists_all_ref:\",len(lista_lists_values), preselection_df3.shape)\n",
    "\n",
    "\n",
    "    for i in range(Niter):\n",
    "\n",
    "        print (i)\n",
    "\n",
    "\n",
    "\n",
    "        #### old, simple randomization scheme (only controling for year)\n",
    "    #     lista_values = list(preselection_df3.reference_UT)   #[i for i in range (len(df_merged))]\n",
    "    #     random.shuffle(lista_values)\n",
    "    #     preselection_df3['randomized_ref_UT'] = lista_values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ########   new randomization scheme (controling for year, but also preserving groups of references cited together in a paper):   \n",
    "        #### lista_values is created outside the Niter loop   (i only need to do it once per selected df)    \n",
    "\n",
    "\n",
    "\n",
    "        random.shuffle(lista_lists_values)      \n",
    "        ### to flat out a list of lists:   \n",
    "        flat_list = []    \n",
    "        for sublist in lista_lists_values:\n",
    "            for item in sublist:\n",
    "                flat_list.append(item)\n",
    "\n",
    "\n",
    "       # print (\"len flat list:\", len(flat_list), preselection_df3.shape)\n",
    "\n",
    "        preselection_df3['randomized_ref_UT'] = flat_list   ### ojo!!! esto randomiza paper_UT, no reference_UT  !!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ####### (RANDOMIZED)  i look at the usage of the top ref\n",
    "        df_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_top_ref)]\n",
    "\n",
    "        df_top_ref_top_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "        df_top_ref_bottom_plos_rand = df_top_ref_rand[df_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "        usage_top_ref_top_plos_rand = len(df_top_ref_top_plos_rand)/float(len(df_top_ref_rand))\n",
    "        usage_top_ref_bottom_plos_rand = len(df_top_ref_bottom_plos_rand)/float(len(df_top_ref_rand))\n",
    "\n",
    "\n",
    "        lista_usage_top_ref_by_top_plos_rand.append(usage_top_ref_top_plos_rand)\n",
    "        lista_usage_top_ref_by_bottom_plos_rand.append(usage_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #######  (RANDOMIZED) i look at the usage of the non-top ref            \n",
    "        df_non_top_ref_rand = preselection_df3[preselection_df3['randomized_ref_UT'].isin(lista_bottom_ref)]        \n",
    "\n",
    "        df_non_top_ref_top_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_top_plos)]\n",
    "        df_non_top_ref_bottom_plos_rand = df_non_top_ref_rand[df_non_top_ref_rand['paper_UT'].isin(lista_bottom_plos)]\n",
    "\n",
    "\n",
    "        usage_non_top_ref_top_plos_rand = len(df_non_top_ref_top_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "        usage_non_top_ref_bottom_plos_rand = len(df_non_top_ref_bottom_plos_rand)/float(len(df_non_top_ref_rand))\n",
    "\n",
    "\n",
    "        lista_usage_nontop_ref_by_top_plos_rand.append(usage_non_top_ref_top_plos_rand)\n",
    "        lista_usage_nontop_ref_by_bottom_plos_rand.append(usage_non_top_ref_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(\"plos category:\", string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"\\n\\n\\n\\navg randomized!!\")\n",
    "    print(\"fraction of usage of top ref by\")\n",
    "    print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\",  np.mean(lista_usage_top_ref_by_top_plos_rand) )   \n",
    "    print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\",np.mean(lista_usage_top_ref_by_bottom_plos_rand)  )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print (\"\\n\\navg randomized\")\n",
    "    print (\"fraction of usage of non-top ref by \")\n",
    "    print (\"  top\",(100-100*list_q_plos[-2]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_top_plos_rand) )   \n",
    "    print (\"  bottom\",(100*list_q_plos[0]),\"% plos:\", np.mean(lista_usage_nontop_ref_by_bottom_plos_rand) ,\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #############################  second part\n",
    "\n",
    "\n",
    "    ###  PLOT FIGURE FOR MORE SOPHISTICATED VERSION OF THE RANDOMIZATION SCHEME\n",
    "    ######     (RUN PREVIOUS CELL FIRST, TO GET THE BOOT-STRAPPING DATA)\n",
    "\n",
    "\n",
    "    ### group by Top PApers  Bottom Papers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #########################################\n",
    "\n",
    "    #lista_bin_names=['Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers', 'Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers']\n",
    "    #lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'%<br>papers', 'Top '+str(int(100-100*list_q_plos[-2]))+'%<br>papers']\n",
    "\n",
    "    lista_bin_names = ['Bottom '+str(int(list_q_plos[0]*100))+'% papers<br>'+string_range_bottom_plos, 'Top '+str(int(100-100*list_q_plos[-2]))+'% papers<br>'+string_range_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_for_top_ref = [ usage_top_ref_top_plos, usage_top_ref_bottom_plos]\n",
    "    lista_for_bottom_ref = [usage_non_top_ref_top_plos, usage_non_top_ref_bottom_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    lista_for_top_ref = [  usage_top_ref_bottom_plos, usage_top_ref_top_plos]\n",
    "    lista_for_bottom_ref = [usage_non_top_ref_bottom_plos, usage_non_top_ref_top_plos]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ###### this is the null model \n",
    "\n",
    "\n",
    "    # lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_top_plos_rand),np.mean(lista_usage_top_ref_by_bottom_plos_rand)]  \n",
    "    # lista_expectations_bottom_ref = [np.mean(lista_usage_nontop_ref_by_top_plos_rand), np.mean(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "    lista_expectations_top_ref = [np.mean(lista_usage_top_ref_by_bottom_plos_rand), np.mean(lista_usage_top_ref_by_top_plos_rand)]  \n",
    "    lista_expectations_bottom_ref = [ np.mean(lista_usage_nontop_ref_by_bottom_plos_rand),np.mean(lista_usage_nontop_ref_by_top_plos_rand)] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_top_plos_rand), 2.*np.std(lista_usage_top_ref_by_bottom_plos_rand)] \n",
    "    # list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) , 2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand)] \n",
    "\n",
    "\n",
    "    list_errors_top_ref = [2.*np.std(lista_usage_top_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_top_ref_by_top_plos_rand) ] \n",
    "    list_errors_bottom_ref = [2.*np.std(lista_usage_nontop_ref_by_bottom_plos_rand), 2.*np.std(lista_usage_nontop_ref_by_top_plos_rand) ] \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    z_score_top_ref_by_top_plos = (usage_top_ref_top_plos - np.mean(lista_usage_top_ref_by_top_plos_rand))/np.std(lista_usage_top_ref_by_top_plos_rand)\n",
    "    z_score_nontop_ref_by_top_plos = (usage_non_top_ref_top_plos - np.mean(lista_usage_nontop_ref_by_top_plos_rand))/np.std(lista_usage_nontop_ref_by_top_plos_rand)\n",
    "\n",
    "    z_score_top_ref_by_bottom_plos = (usage_top_ref_bottom_plos - np.mean(lista_usage_top_ref_by_bottom_plos_rand))/np.std(lista_usage_top_ref_by_bottom_plos_rand)\n",
    "    z_score_nontop_ref_by_bottom_plos = (usage_non_top_ref_bottom_plos - np.mean(lista_usage_nontop_ref_by_bottom_plos_rand))/np.std(lista_usage_nontop_ref_by_bottom_plos_rand)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print ('zscore top ref cited  by top plos:', z_score_top_ref_by_top_plos)\n",
    "    print ('zscore bottom ref cited  by top plos:', z_score_nontop_ref_by_top_plos)\n",
    "\n",
    "    print ('zscore top ref cited  by bottom plos:', z_score_top_ref_by_bottom_plos)\n",
    "    print ('zscore bottom ref cited  by bottom plos:', z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "    title_string=''#'s top-ref by top plos: '+str(z_score_top_ref_by_top_plos)+';  zs top-ref by bottom plos: '+str(z_score_top_ref_by_bottom_plos)+\\\n",
    "    #'<br>zs nontop-ref by top plos: '+str(z_score_nontop_ref_by_top_plos)+';  zs nontop-ref by bottom plos: '+str(z_score_nontop_ref_by_bottom_plos)\n",
    "\n",
    "\n",
    "\n",
    "    size_bar_name = 30#45\n",
    "    y_pos_bar_names = -.039\n",
    "    angle = -70\n",
    "\n",
    "    trace1 = go.Bar(\n",
    "        x=lista_bin_names,\n",
    "        y=lista_for_top_ref,\n",
    "    #     text=['by top '+str(int(100-100*list_q_plos[-2]))+'% papers', 'by top '+str(int(100-100*list_q_plos[-2]))+'% papers'],    \n",
    "    #     name='by top '+str(int(100-100*list_q_plos[-2]))+'% papers',   \n",
    "        marker=dict(\n",
    "            color='#88419d',           \n",
    "        ),\n",
    "\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    trace2 = go.Bar(\n",
    "        x=lista_bin_names,\n",
    "        y=lista_expectations_top_ref,\n",
    "    #     name='   expected value',\n",
    "    #     text=['Expected value for top', 'Expected value for top'],  \n",
    "        error_y=dict(\n",
    "           # type='data',\n",
    "            array=list_errors_top_ref,\n",
    "            thickness=5,\n",
    "            visible=True\n",
    "        ),\n",
    "        marker=dict(\n",
    "            color='#c994c7',         \n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    trace3 = go.Bar(\n",
    "        x=lista_bin_names,\n",
    "        y=lista_for_bottom_ref,\n",
    "    #     name='by bottom '+str(int(list_q_plos[0]*100))+'% papers', \n",
    "    #     text=['by bottom '+str(int(list_q_plos[0]*100))+'% papers','by bottom '+str(int(list_q_plos[0]*100))+'% papers'],\n",
    "        marker=dict(\n",
    "            color='#225ea8',   \n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    trace4 = go.Bar(\n",
    "        x=lista_bin_names,\n",
    "        y=lista_expectations_bottom_ref,\n",
    "      #  name='   expected value',\n",
    "\n",
    "        error_y=dict(       \n",
    "            array=list_errors_bottom_ref,#[0.5, 1, 2],\n",
    "            thickness=5,\n",
    "            visible=True\n",
    "        ),\n",
    "        marker=dict(\n",
    "            color='#a6bddb',     \n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    data = [trace1, trace2, trace3, trace4]\n",
    "    layout = go.Layout(   \n",
    "        title=title_string,\n",
    "        xaxis = dict(\n",
    "            side= 'top',\n",
    "            range = [-.5,1.5],\n",
    "           # showline =  True,\n",
    "            #title= 'Plos Citation percentile'),\n",
    "        ),\n",
    "        yaxis = dict(\n",
    "            title= 'Fraction references cited',\n",
    "            range = [-.07,0.17],\n",
    "            tickvals=[0.0,0.05,0.1,0.15],\n",
    "            #showline =  True,\n",
    "             ),\n",
    "\n",
    "        showlegend=False,\n",
    "    #     legend=dict(x=0.75, y=1.05,                 \n",
    "    #                font=dict(\n",
    "    #                     #family='sans-serif',\n",
    "    #                     size=40,\n",
    "    #                     #color='#000'\n",
    "    #                     ),\n",
    "    #                 ),\n",
    "\n",
    "\n",
    "    #     barmode='stacked',#group',\n",
    "       # bargap=0.2,\n",
    "        bargroupgap=0.15,\n",
    "\n",
    "        annotations = [  \n",
    "            # the four bars on the left\n",
    "            dict(\n",
    "              x = -.34,\n",
    "              y = y_pos_bar_names,\n",
    "              text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "              textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),    \n",
    "            dict(\n",
    "              x = -0.14,\n",
    "              y = y_pos_bar_names,\n",
    "              text = 'Null Model',\n",
    "               textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "            dict(\n",
    "              x = .08,\n",
    "              y = y_pos_bar_names,\n",
    "              showarrow = False,\n",
    "              text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "              textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "            dict(\n",
    "              x = .28,\n",
    "              y = y_pos_bar_names,\n",
    "              text =  'Null Model',\n",
    "              textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "\n",
    "\n",
    "\n",
    "            # the four bars on the right\n",
    "            dict(  \n",
    "              x = .68,\n",
    "              y = y_pos_bar_names,\n",
    "              text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "              textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "            dict(\n",
    "              x = .88,\n",
    "              y = y_pos_bar_names,\n",
    "              text = 'Null Model',\n",
    "               textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "            dict(\n",
    "              x = 1.08,\n",
    "              y = y_pos_bar_names,\n",
    "              text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "              textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "            dict(\n",
    "              x = 1.28,\n",
    "              y = y_pos_bar_names,\n",
    "              text =  'Null Model',\n",
    "              textangle=angle,\n",
    "                font = dict(size = size_bar_name ),\n",
    "               ),\n",
    "\n",
    "\n",
    "            ],    \n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "    fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "    # Altering x axis\n",
    "    #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "    fig['layout']['xaxis']['tickangle'] = 0\n",
    "    # fig['layout']['yaxis']['tickangle'] = -90\n",
    "    # fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "    # fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "    fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "    fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    fig['layout']['margin']=dict(\n",
    "            l=300,\n",
    "           # r=50,\n",
    "            b=100,\n",
    "            t=200,\n",
    "            pad=15\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "    fig_filename='fract_usage_top_bottom_ref_'+str(years[0])+string_code_categ.replace(\" \",\"_\")\n",
    "    offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename='../plots/'+ffig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # earth science, 2013, 1000 iter, simple randomization scheme:\n",
    "\n",
    "    # zscore top ref cited  by top plos: 1.8633822051\n",
    "    # zscore bottom ref cited  by top plos: -8.29803601221\n",
    "    # zscore top ref cited  by bottom plos: -4.18320566755\n",
    "    # zscore bottom ref cited  by bottom plos: 2.22458205576\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## earth science, 2013, 1000 iter, more sophisticated randomization scheme (preserving reference groups/isolated references):\n",
    "\n",
    "    # zscore top ref cited  by top plos: 1.61069883113\n",
    "    # zscore bottom ref cited  by top plos: -8.02168733553\n",
    "    # zscore top ref cited  by bottom plos: -3.84738913187\n",
    "    # zscore bottom ref cited  by bottom plos: 2.16556097748\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'file:///home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/notebooks/fract_usage_top_bottom_ref_2014_Biology_and_life_sciences.html'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #######  TEMPORARY, JUST FOR PLOTTING\n",
    "\n",
    "\n",
    "\n",
    "# data = [trace1, trace2, trace3, trace4]\n",
    "# layout = go.Layout(   \n",
    "#     title=title_string,\n",
    "#     xaxis = dict(\n",
    "#         side= 'top',\n",
    "#         range = [-.5,1.5],\n",
    "#        # showline =  True,\n",
    "#         #title= 'Plos Citation percentile'),\n",
    "#     ),\n",
    "#     yaxis = dict(\n",
    "#         title= 'Fraction references cited',\n",
    "#         range = [-.07,0.19],\n",
    "#         tickvals=[0.0,0.05,0.1,0.15],\n",
    "#         #showline =  True,\n",
    "#          ),\n",
    "\n",
    "#     showlegend=False,\n",
    "# #     legend=dict(x=0.75, y=1.05,                 \n",
    "# #                font=dict(\n",
    "# #                     #family='sans-serif',\n",
    "# #                     size=40,\n",
    "# #                     #color='#000'\n",
    "# #                     ),\n",
    "# #                 ),\n",
    "\n",
    "\n",
    "# #     barmode='stacked',#group',\n",
    "#    # bargap=0.2,\n",
    "#     bargroupgap=0.15,\n",
    "\n",
    "#     annotations = [  \n",
    "#         # the four bars on the left\n",
    "#         dict(\n",
    "#           x = -.34,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),    \n",
    "#         dict(\n",
    "#           x = -0.14,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Null Model',\n",
    "#            textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .08,\n",
    "#           y = y_pos_bar_names,\n",
    "#           showarrow = False,\n",
    "#           text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .28,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text =  'Null Model',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "\n",
    "\n",
    "\n",
    "#         # the four bars on the right\n",
    "#         dict(  \n",
    "#           x = .68,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Top '+str(int(100-100*list_q_ref[-2]))+'%<br>references',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = .88,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Null Model',\n",
    "#            textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = 1.08,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text = 'Bottom '+str(int(list_q_ref[0]*100))+'%<br>references', \n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "#         dict(\n",
    "#           x = 1.28,\n",
    "#           y = y_pos_bar_names,\n",
    "#           text =  'Null Model',\n",
    "#           textangle=angle,\n",
    "#             font = dict(size = size_bar_name ),\n",
    "#            ),\n",
    "\n",
    "\n",
    "#         ],    \n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# # fig['layout']['yaxis']['tickangle'] = -90\n",
    "# # fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "# # fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=300,\n",
    "#        # r=50,\n",
    "#         b=100,\n",
    "#         t=200,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "# fig_filename='fract_usage_top_bottom_ref_'+str(years[0])+string_code_categ.replace(\" \",\"_\")\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # earth science, 2013, 1000 iter, simple randomization scheme:\n",
    "\n",
    "# # zscore top ref cited  by top plos: 1.8633822051\n",
    "# # zscore bottom ref cited  by top plos: -8.29803601221\n",
    "# # zscore top ref cited  by bottom plos: -4.18320566755\n",
    "# # zscore bottom ref cited  by bottom plos: 2.22458205576\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## earth science, 2013, 1000 iter, more sophisticated randomization scheme (preserving reference groups/isolated references):\n",
    "\n",
    "# # zscore top ref cited  by top plos: 1.61069883113\n",
    "# # zscore bottom ref cited  by top plos: -8.02168733553\n",
    "# # zscore top ref cited  by bottom plos: -3.84738913187\n",
    "# # zscore bottom ref cited  by bottom plos: 2.16556097748"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "fig['layout']['font']['size'] = font_gral-5\n",
    "\n",
    "\n",
    "# Altering x axis\n",
    "#fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral -10\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral -10\n",
    "\n",
    "fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "fig['layout']['yaxis']['tickfont']['size'] = font_gral -10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fig['layout']['margin']=dict(\n",
    "        l=200,\n",
    "       # r=50,\n",
    "        b=100,\n",
    "        t=200,\n",
    "        pad=15\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "py.iplot(fig, filename='grouped-bar')\n",
    "\n",
    "\n",
    "fig_filename='fract_usage_top_bottom_ref_'+str(years[0])\n",
    "offline.plot(fig, auto_open=True, image = 'png', image_filename='../plots/'+fig_filename ,image_width=2000, image_height=1200, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### simple randomization:\n",
    "# avg randomized!!\n",
    "# fraction of usage of top ref by\n",
    "#   top 10.0 % plos: 0.115860955202\n",
    "#   bottom 10.0 % plos: 0.0391509861895\n",
    "\n",
    "\n",
    "# avg randomized\n",
    "# fraction of usage of non-top ref by \n",
    "#   top 10.0 % plos: 0.115921294357\n",
    "#   bottom 10.0 % plos: 0.038985722113 \n",
    "\n",
    "\n",
    "\n",
    "# zscore top ref cited  by top plos: 24.7123940702\n",
    "# zscore bottom ref cited  by top plos: -31.4785040033\n",
    "# zscore top ref cited  by bottom plos: -4.30264317563\n",
    "# zscore bottom ref cited  by bottom plos: 13.9133250876\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### more sophisticated randomization:\n",
    "\n",
    "# avg randomized!!\n",
    "# fraction of usage of top ref by\n",
    "#   top 10.0 % plos: 0.115839914306\n",
    "#   bottom 10.0 % plos: 0.0392069495073\n",
    "\n",
    "\n",
    "# avg randomized\n",
    "# fraction of usage of non-top ref by \n",
    "#   top 10.0 % plos: 0.11576207819\n",
    "#   bottom 10.0 % plos: 0.0391660489251 \n",
    "\n",
    "\n",
    "\n",
    "# zscore top ref cited  by top plos: 20.9556909104\n",
    "# zscore bottom ref cited  by top plos: -24.9962596815\n",
    "# zscore top ref cited  by bottom plos: -3.61423619718\n",
    "# zscore bottom ref cited  by bottom plos: 13.7449530258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)\n",
    "\n",
    "df_merged[['paper_UT','plos_pub_year','paper_cite_count','reference_UT','ref_pub_year','num_cit_young_ref_by2009','num_cit_young_ref_by2010','num_cit_young_ref_by2011','num_cit_young_ref_by2012','num_cit_young_ref_by2013','cite_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### TESTING DIFFERENT SORTING ALGORITHMS:\n",
    "# ############################\n",
    "# ############################\n",
    "\n",
    "# def bubblesort(list):\n",
    "\n",
    "#     #### Swap the elements to arrange in order\n",
    "#     for i in range(len(list)-1,0,-1):  #### range(START, STOP, STEP)\n",
    "        \n",
    "#         for idx in range(i):\n",
    "#             print (idx, i)\n",
    "#             if list[idx]>list[idx+1]:\n",
    "#                 temp = list[idx]\n",
    "#                 list[idx] = list[idx+1]\n",
    "#                 list[idx+1] = temp\n",
    "#                 print (\"  \", list)\n",
    "\n",
    "\n",
    "                \n",
    "# list = [19,2,31,45,6,11,121,27]\n",
    "# print (list, \"\\n\\n\")\n",
    "# bubblesort(list)\n",
    "# print(\"\\n\\n\",list)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ############################\n",
    "\n",
    "# def merge_sort(unsorted_list):\n",
    "#     if len(unsorted_list) <= 1:\n",
    "#         return unsorted_list\n",
    "# # Find the middle point and devide it\n",
    "#     middle = len(unsorted_list) // 2\n",
    "#     left_list = unsorted_list[:middle]\n",
    "#     right_list = unsorted_list[middle:]\n",
    "\n",
    "#     left_list = merge_sort(left_list)\n",
    "#     right_list = merge_sort(right_list)\n",
    "#     return list(merge(left_list, right_list))\n",
    "\n",
    "# # Merge the sorted halves\n",
    "\n",
    "# def merge(left_half,right_half):\n",
    "\n",
    "#     res = []\n",
    "#     while len(left_half) != 0 and len(right_half) != 0:\n",
    "#         if left_half[0] < right_half[0]:\n",
    "#             res.append(left_half[0])\n",
    "#             left_half.remove(left_half[0])\n",
    "#         else:\n",
    "#             res.append(right_half[0])\n",
    "#             right_half.remove(right_half[0])\n",
    "#     if len(left_half) == 0:\n",
    "#         res = res + right_half\n",
    "#     else:\n",
    "#         res = res + left_half\n",
    "#     return res\n",
    "\n",
    "# unsorted_list = [64, 34, 25, 12, 22, 11, 90]\n",
    "\n",
    "# print(merge_sort(unsorted_list))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #####################################\n",
    "\n",
    "\n",
    "\n",
    "# def insertion_sort(InputList):\n",
    "#     for i in range(1, len(InputList)):\n",
    "#         j = i-1\n",
    "#         nxt_element = InputList[i]\n",
    "# # Compare the current element with next one\n",
    "\n",
    "#         while (InputList[j] > nxt_element) and (j >= 0):\n",
    "#             InputList[j+1] = InputList[j]\n",
    "#             j=j-1\n",
    "#         InputList[j+1] = nxt_element\n",
    "\n",
    "# list = [19,2,31,45,30,11,121,27]\n",
    "# insertion_sort(list)\n",
    "# print(list)\n",
    "\n",
    "\n",
    "# #####################################\n",
    "\n",
    "\n",
    "# def shellSort(input_list):\n",
    "    \n",
    "#     gap = len(input_list) / 2\n",
    "#     while gap > 0:\n",
    "\n",
    "#         for i in range(gap, len(input_list)):\n",
    "#             temp = input_list[i]\n",
    "#             j = i\n",
    "# # Sort the sub list for this gap\n",
    "\n",
    "#             while j >= gap and input_list[j - gap] > temp:\n",
    "#                 input_list[j] = input_list[j - gap]\n",
    "#                 j = j-gap\n",
    "#             input_list[j] = temp\n",
    "\n",
    "# # Reduce the gap for the next element\n",
    "\n",
    "#         gap = gap/2\n",
    "\n",
    "# list = [19,2,31,45,30,11,121,27]\n",
    "\n",
    "# shellSort(list)\n",
    "# print(list)\n",
    "\n",
    "\n",
    "# ##############################\n",
    "\n",
    "\n",
    "# def selection_sort(input_list):\n",
    "\n",
    "#     for idx in range(len(input_list)):\n",
    "\n",
    "#         min_idx = idx\n",
    "#         for j in range( idx +1, len(input_list)):\n",
    "#             if input_list[min_idx] > input_list[j]:\n",
    "#                 min_idx = j\n",
    "# # Swap the minimum value with the compared value\n",
    "\n",
    "#         input_list[idx], input_list[min_idx] = input_list[min_idx], input_list[idx]\n",
    "\n",
    "\n",
    "# l = [19,2,31,45,30,11,121,27]\n",
    "# selection_sort(l)\n",
    "# print(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(preselection_df3.columns)\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3.head(200)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preselection_df3[['paper_UT','reference_UT','occurence',  'regex_sect_index','isolated_citation','rel_loc_in_sect','sect_char_pos','sect_char_total']]\n",
    "preselection_df3.shape   #16021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_df3.occurence.unique()  # only first occurrences of references for a given paper!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "group_df[['paper_UT','reference_UT','regex_sect_index', 'sect_char_pos','isolated_citation']].sort_values(by=['regex_sect_index','sect_char_pos','reference_UT'])\n",
    "\n",
    "#sorted(group_df.columns)\n",
    "\n",
    "#list(sub_group_df['isolated_citation'])[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_merged.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 123212./357866.=.344 tot fract of old ref\n",
    "\n",
    "\n",
    "\n",
    "#34436./357866.  #=.0.09  tot fract of young ref\n",
    "\n",
    "string_self_ref=0\n",
    "\n",
    "tot_list_old_ref_included = []\n",
    "tot_list_young_ref_included = []\n",
    "list_all_papers_included = []\n",
    "\n",
    "for year in [2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] :\n",
    "\n",
    "    print (\"\\n\", year)\n",
    "    \n",
    "    print (\"tot:\",df_merged.shape)\n",
    "\n",
    "    \n",
    "    preselection_df = df_merged[df_merged['plos_pub_year'] == year]         \n",
    "    preselection_df1 = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "    print (\"  size of preselection1 (by plos years and NO self-cit):\",preselection_df1.shape, \" # papers\",len(preselection_df1.paper_UT.unique()), \" # ref\",len(preselection_df1.reference_UT.unique()))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    time_window = 1\n",
    "    df_selection_old = preselection_df1[preselection_df1['ref_pub_year'] >= (year-time_window) ]   \n",
    "    print (\"   old:\",df_selection_old.shape, \" # papers\",len(df_selection_old.paper_UT.unique()), \" # ref\",len(df_selection_old.reference_UT.unique()))\n",
    "    tot_list_old_ref_included +=list(df_selection_old.reference_UT.unique())\n",
    "    list_all_papers_included +=list(df_selection_old.paper_UT.unique())\n",
    "    \n",
    "\n",
    "    time_window = 10\n",
    "    df_selection_young = preselection_df1[preselection_df1['ref_pub_year'] <= (year-time_window) ]   \n",
    "    print (\"   young:\",df_selection_young.shape, \" # papers\",len(df_selection_young.paper_UT.unique()), \" # ref\",len(df_selection_young.reference_UT.unique()))\n",
    "    tot_list_young_ref_included +=list(df_selection_young.reference_UT.unique())\n",
    "    list_all_papers_included +=list(df_selection_young.paper_UT.unique())\n",
    "    \n",
    "    \n",
    "    ##### \"2011:  (N = 14,351, that cite 357,866 unique references yielding 564,251 records)\"\n",
    "\n",
    "    \n",
    "    \n",
    "print (\"--->>  TOT number old ref included (year after years):\", len(set(tot_list_old_ref_included)))\n",
    "print (\"--->>  TOT number young ref included (year after years):\", len(set(tot_list_young_ref_included)))\n",
    "print (\"--->>  TOT number papers included (year after years):\", len(set(list_all_papers_included)))\n",
    "print (\"--->>  TOT ref in df_merged:\", len(df_merged.reference_UT.unique()), \"  # papers in df_merged\",len(df_merged.paper_UT.unique())  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################  FIGURE FOR THE EXPERTISE OF TEAMS, BY PLOS IMPACT GROUP\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string = 'cite_count'   #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "  \n",
    "# string_references_age = \"all\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "# years=[2011] \n",
    "# #years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "   \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "# #list_strings = [1,0]\n",
    "# #for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "# #list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "# #for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "# #   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "# #  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537 --\n",
    "# #  '1': 'Computer and information sciences'     1,207,799 --\n",
    "# #  '10': 'Social sciences'                      755,899 --\n",
    "# #  '2': 'Earth sciences'                        533,155 --\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "# #  '4': 'Engineering and technology'            382,247 --\n",
    "# #  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "# #  '6': 'People and places'                     691,523 --\n",
    "# #  '7': 'Physical sciences'                     2,100,827 --\n",
    "# #  '8': 'Research and analysis methods'         3,871,470 --\n",
    "# #  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "# #list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "# #for string_journal in list_strings:\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "# #PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "# fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "\n",
    "\n",
    "#     fig_colorscale = \"Reds\"\n",
    "#     fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "\n",
    "#     if  v1_string ==  'log_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     elif  v1_string ==  'log2_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# elif v1_string =='ref_pub_year':\n",
    "#     fig_colorscale = \"Viridis\"\n",
    "#     fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "#     fig_colorscale = [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "#     fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "\n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "# ###### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1] \n",
    "# df_plos_unique_in_select = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "\n",
    "# quantiles=sorted(list(df_plos_unique_in_select['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     try:\n",
    "#         pair=[old_value, int(item[1])]    \n",
    "#     except:  # if it is a nan:\n",
    "#         pair=[old_value, item[1]]\n",
    "    \n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "#     try:\n",
    "#         old_value = int(item[1])\n",
    "#     except:\n",
    "#         old_value = item[1]\n",
    "\n",
    "# print (lista_bins_plos_citations, df_plos.shape)\n",
    "\n",
    "\n",
    "\n",
    "# ######\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # z = [[.1],  \n",
    "# #      [1.0],\n",
    "# #      [1.0],\n",
    "# #      [.6]]\n",
    "\n",
    "# tot_recors_included = 0\n",
    "\n",
    "# lista_text_z = []\n",
    "# list_of_lists_z =[]\n",
    "# for pair_values in lista_bins_plos_citations:       \n",
    "    \n",
    "#     minimo = pair_values[0]\n",
    "#     maximo = pair_values[1]   \n",
    "\n",
    "#     #df_select = preselection_df3[(preselection_df3['paper_cite_count'] >= minimo)  &  (preselection_df3['paper_cite_count'] < maximo)]            \n",
    "#     df_select = df_plos_unique_in_select[(df_plos_unique_in_select['paper_cite_count'] >= minimo)  &  (df_plos_unique_in_select['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "    \n",
    "    \n",
    "#     list_q = [0,.25,.5,.75,1]\n",
    "#     quantiles_cell_aux  = sorted(list(df_select['number_authors'].quantile(list_q).to_dict().items()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# #     list_quantiles = []   # for [intro, methods, results, discussion]\n",
    "# #     for column in ['num_ref_section0', 'num_ref_section1', 'num_ref_section2', 'num_ref_section3']:\n",
    "# #         quantiles_cell_aux  = sorted(list(df_select[column].quantile(list_q_cell).to_dict().items())) #mean 10.68 \n",
    "# #         list_quantiles.append(quantiles_cell_aux)\n",
    "# #     print (pair_values, preselection_df3.shape, df_select.shape, list_quantiles)\n",
    "   \n",
    "\n",
    "#     aux_list = [int(df_select.team_expertise.median())]\n",
    "#     list_of_lists_z.append(aux_list)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     list_q_cell = [.25,.5,.75]\n",
    "#     quantiles_cell_aux  = sorted(list(df_select['team_expertise'].quantile(list_q_cell).to_dict().items()))\n",
    "#    # print (pair_values, quantiles_cell_aux)  ### [(0.25, 49.0), (0.5, 118.0), (0.75, 252.0)]\n",
    "#     aux_lista_text_z = [int(item[1]) for item in quantiles_cell_aux]\n",
    "#     aux_lista_text_z = [str(aux_lista_text_z).replace('[','').replace(']','').replace(', ', '-')+\" <br>\"+str(format(len(df_select), ',d'))]    \n",
    "#    # print (aux_lista_text_z)\n",
    "    \n",
    "#     lista_text_z.append(aux_lista_text_z)\n",
    "    \n",
    "    \n",
    "#     tot_recors_included  += len(df_select)\n",
    "    \n",
    "# # print (\"\\n\\n\")\n",
    "# # for lista in list_of_lists_z:\n",
    "# print (list_of_lists_z)\n",
    "\n",
    "# # print (\"\\n\\n\")\n",
    "# # for lista in lista_text_z:\n",
    "# #     print (lista)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # z = [[.1],  \n",
    "# #      [1.0],\n",
    "# #      [1.0],\n",
    "# #      [3.0],\n",
    "# #      [.6]]\n",
    "\n",
    "# x = [' ']\n",
    "\n",
    "# lista_bin_names=[\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z = list_of_lists_z, x=x, y=lista_bin_names, annotation_text=lista_text_z, colorscale='Blues', reversescale=True)\n",
    "\n",
    "\n",
    "\n",
    "# fig.layout.title = int(years[0])  #\"\"# fig_title_plot\n",
    "\n",
    "# #fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# #fig.layout.xaxis.update({'title': 'Section'})\n",
    "\n",
    "\n",
    "# fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=25  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "      \n",
    "# #       fig['layout']['title'] = \"Young references\"\n",
    "\n",
    "# #     fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "# # fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral + 20\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -7 \n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         t=top_space,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='stacked_bar_plot' ,image_width=1000, image_height=2000,\n",
    "#               filename='stacked_bar_plot.html', validate=True)\n",
    "                                \n",
    "                                \n",
    "# print (years, \"  # records included\",tot_recors_included)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorted(df_merged.number_authors.unique())\n",
    "df_plos_unique_in_select.columns\n",
    "\n",
    "\n",
    "# df_merged['norm_expertise_by_size'] = df_merged['team_expertise']/df_merged['number_authors']\n",
    "\n",
    "\n",
    "# df_merged[['paper_UT', 'reference_UT','team_expertise','number_authors','norm_expertise_by_size']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################  FIGURE FOR THE EXPERTISE OF TEAMS, BY PLOS IMPACT GROUP and by team size\n",
    "\n",
    "\n",
    "# df_merged['norm_expertise_by_size'] = df_merged['team_expertise']/df_merged['number_authors']\n",
    "\n",
    "# flag_norm = 1  # for either normalized team expertise (number of papers authored / number team members) or raw count of papers\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string = 'cite_count'   #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "  \n",
    "# string_references_age = \"all\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "# years=[2011] \n",
    "# #years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "   \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "# #list_strings = [1,0]\n",
    "# #for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "# #list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "# #for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "# #   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "# #  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537 --\n",
    "# #  '1': 'Computer and information sciences'     1,207,799 --\n",
    "# #  '10': 'Social sciences'                      755,899 --\n",
    "# #  '2': 'Earth sciences'                        533,155 --\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "# #  '4': 'Engineering and technology'            382,247 --\n",
    "# #  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "# #  '6': 'People and places'                     691,523 --\n",
    "# #  '7': 'Physical sciences'                     2,100,827 --\n",
    "# #  '8': 'Research and analysis methods'         3,871,470 --\n",
    "# #  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "# #list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "# #for string_journal in list_strings:\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "# #PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "       \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "        \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_plos_unique_in_select = preselection_df3.drop_duplicates(subset=['paper_UT'])\n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "# list_q = [0.,.25,.5,.75,1]\n",
    "# quantiles_auth  = sorted(list(df_plos_unique_in_select['number_authors'].quantile(list_q).to_dict().items()))   # [(0.25, 143.0), (0.5, 291.0), (0.75, 565.0)]\n",
    "# print (\"quantiles team size for all data this year:\",quantiles_auth)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "# ###### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1] \n",
    "\n",
    "# print (len(df_plos_unique_in_select))\n",
    "\n",
    "# quantiles_impact=sorted(list(df_plos_unique_in_select['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles_impact:\n",
    "#     try:\n",
    "#         pair=[old_value, int(item[1])]    \n",
    "#     except:  # if it is a nan:\n",
    "#         pair=[old_value, item[1]]\n",
    "    \n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "#     try:\n",
    "#         old_value = int(item[1])\n",
    "#     except:\n",
    "#         old_value = item[1]\n",
    "\n",
    "# print (\"bins for # cit:\",lista_bins_plos_citations, df_plos_unique_in_select.shape)\n",
    "# print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### ojo!! this plot is about plos records, not reference occurrences!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tot_recors_included = 0\n",
    "\n",
    "# lista_text_z = []\n",
    "# list_of_lists_z =[]\n",
    "# for pair_values in lista_bins_plos_citations:       ### loop over impact groups for plos papers\n",
    "    \n",
    "#     minimo = pair_values[0]\n",
    "#     maximo = pair_values[1]   \n",
    "\n",
    "           \n",
    "#     df_select = df_plos_unique_in_select[(df_plos_unique_in_select['paper_cite_count'] >= minimo)  &  (df_plos_unique_in_select['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ###### i get the bins number of authors of the plos papers\n",
    "\n",
    "#     ###OJO!!!@ ESTO AQUI O DENTRO DEL LOOP DE IMPAC T GROUP???\n",
    "\n",
    "#     list_q = [0.,.25,.5,.75,1]\n",
    "#     quantiles_auth  = sorted(list(df_select['number_authors'].quantile(list_q).to_dict().items()))   # [(0.25, 143.0), (0.5, 291.0), (0.75, 565.0)]\n",
    "\n",
    "#     lista_bins_plos_num_auth=[]\n",
    "#     old_value=0\n",
    "#     for item in quantiles_auth:\n",
    "#         try:\n",
    "#             pair=[old_value, int(item[1])]    \n",
    "#         except:  # if it is a nan:\n",
    "#             pair=[old_value, item[1]]\n",
    "\n",
    "#         lista_bins_plos_num_auth.append(pair)\n",
    "\n",
    "#         try:\n",
    "#             old_value = int(item[1])\n",
    "#         except:\n",
    "#             old_value = item[1]\n",
    "\n",
    "#     print (\"  bins for # authors in the\",pair_values, \"-citation bin:   \",quantiles_auth, lista_bins_plos_num_auth, df_plos_unique_in_select.shape)\n",
    "   \n",
    "#     #input()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     aux_list =[]\n",
    "#     aux_lista_text_z = []\n",
    "#     for par_valores in lista_bins_plos_num_auth:   ### loop ofer the quantiles of plos papers by number of authors    \n",
    "    \n",
    "#         minimo_num_auth = par_valores[0]\n",
    "#         maximo_num_auth = par_valores[1]   \n",
    "\n",
    "#         df_select_by_num_auth = df_select[(df_select['number_authors'] > minimo_num_auth)  &  (df_select['number_authors'] <= maximo_num_auth)]        ### ojo!! no puedo tener num_authors = 0!!     \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         list_q_cell = [.25,.5,.75]\n",
    "\n",
    "#         if flag_norm == 0:\n",
    "#             aux_list.append(df_select_by_num_auth.team_expertise.median())      #   aux_list = [int(df_select.team_expertise.median())]\n",
    "#             quantiles_cell_aux  = sorted(list(df_select_by_num_auth['team_expertise'].quantile(list_q_cell).to_dict().items()))    # example  quantiles_cell_aux:    [(0.25, 49.0), (0.5, 118.0), (0.75, 252.0)]\n",
    "#             title_string = \"Team expertise\"\n",
    "        \n",
    "#         elif flag_norm == 1:\n",
    "         \n",
    "#             aux_list.append(df_select_by_num_auth.norm_expertise_by_size.median())      #   aux_list = [int(df_select.team_expertise.median())]\n",
    "#             quantiles_cell_aux  = sorted(list(df_select_by_num_auth['norm_expertise_by_size'].quantile(list_q_cell).to_dict().items()))    # example  quantiles_cell_aux:    [(0.25, 49.0), (0.5, 118.0), (0.75, 252.0)]\n",
    "#             title_string = \"Normalized team expertise\"\n",
    "        \n",
    " \n",
    "#         aux_text_cell = [int(item[1]) for item in quantiles_cell_aux]\n",
    "#         aux_text_cell = str(aux_text_cell).replace('[','').replace(']','').replace(', ', '-')+\" <br>\"+str(format(len(df_select_by_num_auth), ',d')) \n",
    "#         aux_lista_text_z.append(aux_text_cell)\n",
    "       \n",
    "                                                      \n",
    "#         print (\"     q for team expertise labels:\",quantiles_cell_aux, len(df_select_by_num_auth))\n",
    "\n",
    "#         tot_recors_included  += len(df_select_by_num_auth)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     list_of_lists_z.append(aux_list)\n",
    "#     lista_text_z.append(aux_lista_text_z)\n",
    "\n",
    "    \n",
    "# print (list_of_lists_z)    \n",
    "# print (\"\\n\\n\")\n",
    "\n",
    "\n",
    "# list_x = [\"Solo\",\"Small\",\"Typical\",\"Large\",\"Very large\"]\n",
    "\n",
    "# lista_bin_names_y=[\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# ##### using a different library\n",
    "# path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=list_of_lists_z, x=list_x, y=lista_bin_names_y, annotation_text=lista_text_z, colorscale='Blues', showscale=True, colorbar=dict(title=title_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "# fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# fig.layout.xaxis.update({'title': 'Team Size'})\n",
    "# fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "\n",
    "# for i in range(len(fig.layout.annotations)):\n",
    "#     fig.layout.annotations[i].font.size = 30\n",
    "\n",
    "\n",
    "# font_gral=50 # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "# #fig['layout']['title'] = \"Team expertise\"    \n",
    "# # if v1_string ==  'cite_count'  :\n",
    "# #     if string_references_age == \"young\":  \n",
    "# #         #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "       \n",
    "# #     elif string_references_age == \"old\":  \n",
    "# #         fig.layout.update({'title': 'Old references'})\n",
    "\n",
    "# #     fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral \n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -15\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         t=50,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2200, image_height=1600, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='heatmap' ,image_width=2000, image_height=1200, filename=path+'heatmap.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preselection_df = df_merged.drop_duplicates(subset=['paper_UT', 'reference_UT'])  ### i care about unique reference-paper tuples\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_merged.shape,  preselection_df.shape\n",
    "\n",
    "# dict_paper_fract_recycled_ref = {}\n",
    "# for paper_UT,  group_df in preselection_df.groupby(['paper_UT']):  # i look at the usage of references (no repetitions of references in a paper for different sections)\n",
    "#     avg_ref_recycling_for_paper_UT = group_df.recycled_ref.mean()\n",
    "#     #print (paper_UT, len(group_df), len(group_df.reference_UT.unique()), avg_ref_recycling_for_paper_UT)\n",
    "#     dict_paper_fract_recycled_ref[paper_UT] = avg_ref_recycling_for_paper_UT\n",
    "#     #input()\n",
    "    \n",
    "  \n",
    "# print (\"done with the groupby-ing\", len(dict_paper_fract_recycled_ref))\n",
    "\n",
    "# df_plos = preselection_df.drop_duplicates(subset=['paper_UT']).sort_values(by=['paper_UT'])    ### i make sure the order by plos paper UT is fixed, so I add the new columns in their proper order \n",
    "\n",
    "# print (len(df_plos))\n",
    "\n",
    "# lista_plos_UT = list(df_plos.paper_UT.unique())\n",
    "\n",
    "# lista_recycled_ref = []\n",
    "# for paper_UT in lista_plos_UT:\n",
    "#     lista_recycled_ref.append( dict_paper_fract_recycled_ref[paper_UT])\n",
    "\n",
    "# print (\"done creating list of fract_recycl_ref\", len(lista_recycled_ref))    \n",
    "    \n",
    "# df_plos['fract_recycl_ref'] = lista_recycled_ref\n",
    "\n",
    "# df_plos = df_plos[['paper_UT','fract_recycl_ref']]   # i simplify the df as not to duplicate columns in the merging:\n",
    "\n",
    "\n",
    "# preselection_df  = pd.merge(preselection_df , df_plos, on='paper_UT', how='left')\n",
    "\n",
    "# df_for_recycled_ref = preselection_df\n",
    "\n",
    "\n",
    "\n",
    "# df_for_recycled_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################  FIGURE FOR THE FRACTION OF RECYCLED REDERENCES IN A PLOS PAPER,  BY PLOS IMPACT GROUP and by team size\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ### ojo!! this plot is about reference-plos records!!!\n",
    "# ######################################\n",
    "# ######################################\n",
    "\n",
    "# preselection_df = df_for_recycled_ref.drop_duplicates(subset=['paper_UT', 'reference_UT'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string = 'cite_count'   #      cite_count    diff_year_plos_ref \n",
    "       \n",
    "  \n",
    "# string_references_age = \"all\"   #\"#old\"  # young # all   for the selection of what references i include\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "# years=[2011] \n",
    "# #years=[2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016,2017] \n",
    "\n",
    "\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1]    # for the percentile sections for number of citations of the PLOS papers\n",
    " \n",
    "   \n",
    "      \n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# string_isolated_ref = \"\"   #\"\"   #\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "# #list_strings = [1,0]\n",
    "# #for  string_isolated_ref  in list_strings:\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# string_self_ref =0         #\"\"      # \"\"   #1   # 0  or 1 (or empty string, to include all ref)   OJO!!! THIS NEW FILE DOES NOT INCLUDE SELF-CITATIONS TO BEGING WITH\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. \n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "\n",
    "# #list_strings=['0', '1', '4', '5', '7', '8', '2 3','10 6 9']\n",
    "# #for string_code_categ in list_strings:\n",
    "\n",
    "\n",
    "# #   Biology and Live Sciences;   Computational Sciences;   Engineering;   Medicine;   Physical Sciences;   Research and Analysis; \\\n",
    "# #  Earth Sciences and Ecology;   Social Sciences, Political Sciences and People & Places\n",
    "\n",
    "\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537 --\n",
    "# #  '1': 'Computer and information sciences'     1,207,799 --\n",
    "# #  '10': 'Social sciences'                      755,899 --\n",
    "# #  '2': 'Earth sciences'                        533,155 --\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142 --\n",
    "# #  '4': 'Engineering and technology'            382,247 --\n",
    "# #  '5': 'Medicine and health sciences'          4,535,926  -- \n",
    "# #  '6': 'People and places'                     691,523 --\n",
    "# #  '7': 'Physical sciences'                     2,100,827 --\n",
    "# #  '8': 'Research and analysis methods'         3,871,470 --\n",
    "# #  '9': 'Science policy'                        43,360 --\n",
    "\n",
    "\n",
    "# ######### plos journals \n",
    "# string_journal=\"\"#   PLOS ONE\"\n",
    "\n",
    "# #list_strings=['PLOS MED', 'PLOS BIOL', 'PLOS COMPUT', 'PLOS PATHOG', 'PLO NE TR D', 'PLOS GENET', 'PLOS ONE']\n",
    "\n",
    "\n",
    "# #for string_journal in list_strings:\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    "\n",
    "\n",
    "# #PLOS Medicine, PLOS Biol-ogy, PLOS Computational Biology, PLOS Pathology, PLOS Neglected Tropical Diseases, PLOSGenetics, and PLOS ONE\n",
    "\n",
    "\n",
    "# ######### WoS subject categories. \n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = preselection_df[preselection_df['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "\n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "\n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "\n",
    "\n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "\n",
    "\n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "\n",
    "\n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "\n",
    "\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "       \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "        \n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "\n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    "\n",
    "# # list_q = [0.,.25,.5,.75,1]\n",
    "# # quantiles_auth  = sorted(list(df_plos_unique_in_select['number_authors'].quantile(list_q).to_dict().items()))   # [(0.25, 143.0), (0.5, 291.0), (0.75, 565.0)]\n",
    "# # print (\"quantiles team size for all data this year:\",quantiles_auth)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "# df_plos_unique_in_select =   preselection_df3 \n",
    "    \n",
    "    \n",
    "    \n",
    "# ###### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "\n",
    "# list_q=[0.3,0.6,.9,.99,1] \n",
    "\n",
    "# print (len(df_plos_unique_in_select))\n",
    "\n",
    "# quantiles_impact=sorted(list(df_plos_unique_in_select['paper_cite_count'].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles_impact:\n",
    "#     try:\n",
    "#         pair=[old_value, int(item[1])]    \n",
    "#     except:  # if it is a nan:\n",
    "#         pair=[old_value, item[1]]\n",
    "    \n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "    \n",
    "#     try:\n",
    "#         old_value = int(item[1])\n",
    "#     except:\n",
    "#         old_value = item[1]\n",
    "\n",
    "# print (\"bins for # cit:\",lista_bins_plos_citations, df_plos_unique_in_select.shape)\n",
    "# print (\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tot_recors_included = 0\n",
    "\n",
    "# lista_text_z = []\n",
    "# list_of_lists_z =[]\n",
    "# for pair_values in lista_bins_plos_citations:       ### loop over impact groups for plos papers\n",
    "    \n",
    "#     minimo = pair_values[0]\n",
    "#     maximo = pair_values[1]   \n",
    "\n",
    "           \n",
    "#     df_select = df_plos_unique_in_select[(df_plos_unique_in_select['paper_cite_count'] >= minimo)  &  (df_plos_unique_in_select['paper_cite_count'] < maximo)]            \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     ###### i get the bins number of authors of the plos papers\n",
    "\n",
    "#     ###OJO!!!@ ESTO AQUI O DENTRO DEL LOOP DE IMPAC T GROUP???\n",
    "\n",
    "#     list_q = [0.,.25,.5,.75,1]\n",
    "#     #quantiles_auth  = sorted(list(df_select['number_authors'].quantile(list_q).to_dict().items()))   # [(0.25, 143.0), (0.5, 291.0), (0.75, 565.0)]\n",
    "#     quantiles_expertise  = sorted(list(df_select['norm_expertise_by_size'].quantile(list_q).to_dict().items()))   # [(0.25, 143.0), (0.5, 291.0), (0.75, 565.0)]\n",
    "\n",
    "#     lista_bins_plos_expertise = []\n",
    "#     old_value=0\n",
    "#     for item in quantiles_expertise:\n",
    "#         try:\n",
    "#             pair=[old_value, int(item[1])]    \n",
    "#         except:  # if it is a nan:\n",
    "#             pair=[old_value, item[1]]\n",
    "\n",
    "#         lista_bins_plos_expertise.append(pair)\n",
    "\n",
    "#         try:\n",
    "#             old_value = int(item[1])\n",
    "#         except:\n",
    "#             old_value = item[1]\n",
    "\n",
    "#     print (\"  bins for expertise the\",pair_values, \"-citation bin:   \",quantiles_expertise, lista_bins_plos_expertise, df_plos_unique_in_select.shape)\n",
    "   \n",
    "#     #input()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "#     aux_list =[]\n",
    "#     aux_lista_text_z = []\n",
    "#     #for par_valores in lista_bins_plos_num_auth:   ### loop over the quantiles of plos papers by number of authors    \n",
    "#     for par_valores in lista_bins_plos_expertise:   ### loop over the quantiles of plos papers by number of authors    \n",
    "#         minimo_expertise = par_valores[0]\n",
    "#         maximo_expertise = par_valores[1]   \n",
    "\n",
    "#         #df_select_by_num_auth = df_select[(df_select['number_authors'] > minimo_num_auth)  &  (df_select['number_authors'] <= maximo_num_auth)]        ### ojo!! no puedo tener num_authors = 0!!     \n",
    "#         df_select_by_expertise = df_select[(df_select['norm_expertise_by_size'] > minimo_expertise)  &  (df_select['norm_expertise_by_size'] <= maximo_expertise)]        ### ojo!! no puedo tener num_authors = 0!!     \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         list_q_cell = [.25,.5,.75]\n",
    "\n",
    "       \n",
    "#         aux_list.append(df_select_by_expertise.fract_recycl_ref.median())      #   aux_list = [int(df_select.team_expertise.median())]\n",
    "#         quantiles_cell_aux  = sorted(list(df_select_by_expertise['fract_recycl_ref'].quantile(list_q_cell).to_dict().items()))    # example  quantiles_cell_aux:    [(0.25, 49.0), (0.5, 118.0), (0.75, 252.0)]\n",
    "#         title_string = \"Fraction recycled references in paper\"\n",
    "        \n",
    "       \n",
    "    \n",
    " \n",
    "#         aux_text_cell = [item[1] for item in quantiles_cell_aux]\n",
    "#         aux_text_cell = str(aux_text_cell).replace('[','').replace(']','').replace(', ', '-')+\" <br>\"+str(format(len(df_select_by_expertise), ',d')) \n",
    "#         aux_lista_text_z.append(aux_text_cell)\n",
    "       \n",
    "                                                      \n",
    "#         print (\"     q for team expertise labels:\",quantiles_cell_aux, len(df_select_by_expertise))\n",
    "\n",
    "#         tot_recors_included  += len(df_select_by_expertise)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "#     list_of_lists_z.append(aux_list)\n",
    "#     lista_text_z.append(aux_lista_text_z)\n",
    "\n",
    "    \n",
    "# print (list_of_lists_z)    \n",
    "# print (\"\\n\\n\")\n",
    "\n",
    "\n",
    "# list_x = [\"Low norm. experience\",\"Small\",\"Typical\",\"Large\",\"Very high norm. experience\"]\n",
    "\n",
    "# lista_bin_names_y=[\"Bottom\",\"Typical\",\"Good\",\"High\",\"Top\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# ##### using a different library\n",
    "# path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=list_of_lists_z, x=list_x, y=lista_bin_names_y, annotation_text=lista_text_z, colorscale='Blues', showscale=True, colorbar=dict(title=title_string, titleside='right' ),)#, reversescale=True)\n",
    "\n",
    "# fig.layout.title = \"\"# fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# fig.layout.xaxis.update({'title': 'Team Size'})\n",
    "# fig.layout.yaxis.update({'title': 'Impact Group'})\n",
    "\n",
    "# for i in range(len(fig.layout.annotations)):\n",
    "#     fig.layout.annotations[i].font.size = 30\n",
    "\n",
    "\n",
    "# font_gral=50 # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "# #fig['layout']['title'] = \"Team expertise\"    \n",
    "# # if v1_string ==  'cite_count'  :\n",
    "# #     if string_references_age == \"young\":  \n",
    "# #         #fig.layout.update({'title': '$d, r \\\\text{ (solar radius)}$'})\n",
    "       \n",
    "# #     elif string_references_age == \"old\":  \n",
    "# #         fig.layout.update({'title': 'Old references'})\n",
    "\n",
    "# #     fig.layout.update({'font': dict(size=25)})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=55  # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral \n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -15\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig['layout']['margin']=dict(\n",
    "#         l=200,\n",
    "#        # r=50,\n",
    "#         b=150,\n",
    "#         t=50,\n",
    "#         pad=15\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2200, image_height=1600, filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='heatmap' ,image_width=2000, image_height=1200, filename=path+'heatmap.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged.columns\n",
    "\n",
    "\n",
    "\n",
    "list_q = [0.,.25,.5,.75,1]\n",
    "\n",
    "quantiles_expertise  = sorted(list(df_preselection['norm_expertise_by_size'].quantile(list_q).to_dict().items()))   # [(0.25, 143.0), (0.5, 291.0), (0.75, 565.0)]\n",
    "\n",
    "quantiles_expertise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2011]\n",
    "# shape preselection: (564251, 25)  num_ref: 357866 num_plos 14351\n",
    "# shape one ref per plos: (467305, 25)  num_ref: 357866 num_plos 14351\n",
    "#     size of only young references: (51289, 25)\n",
    "#     size of only old references: (183640, 25)\n",
    "# [(0.3, 9.0), (0.99, 98.0), (1.0, 856.0)]\n",
    "# # young ref (unique) 34436   # old ref(unique) 123212    Tot ref (unique): 357866 357866     # plos in selection: 14351\n",
    "\n",
    "\n",
    "# citation bins for the selected plos: [0.3, 0.99, 1]\n",
    "# 0-9 114545\n",
    "# 9-98 266331\n",
    "# 98-856 5240\n",
    "\n",
    "\n",
    "# # UTs top 1.0 % plos: 146 (6541, 25)\n",
    "# # UTs bottom  30.0 % plos: 4289 (153224, 25)\n",
    "# new ref: [0.14493196758905366, 0.0743225604343967]\n",
    "# old ref: [0.2031799419049075, 0.3787265702500914]\n",
    "# rest of ref: [0.6518880905060388, 0.546950869315512]\n",
    "# all ref: [0.09622596167280491, 0.09622596167280491]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preselection_df2.drop_duplicates(subset=['reference_UT','paper_UT'])  #   467305\n",
    "\n",
    "\n",
    "#preselection_df2.shape   # 564251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### more checks: if the \"endorsement effect\" is real (as opposed to just better plos papers being better at spotting quality in young references), \n",
    "# #                   succesful plos would get more citations than the young references they cite --->> i get the (distributions of) ratios:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_paper_UT=[]\n",
    "# lista_ref_UT=[]\n",
    "# lista_num_ref=[]\n",
    "# lista_ratios_all=[]\n",
    "\n",
    "\n",
    "# for paper_UT, group in df_merged.groupby(['paper_UT']):  ### for df_merged: 158813 158813 5617059 2607457 43.600139787 0.0542472340106   # for preselection_df3: 6211 6211 30225 25508 6.34068587989 0.472185840259  # selected year: 7005 7005 260294 214744 46.4395431834 0.110949088539\n",
    "        \n",
    "#         lista_paper_UT.append(paper_UT)\n",
    "#         lista_ref_UT += list(group.reference_UT.unique())\n",
    "#         lista_num_ref.append(len(group))\n",
    "#         num_cit_paper = float(group.paper_cite_count.unique()) # the group refers to one single plos\n",
    "#         lista_num_cit_ref = list(group.cite_count.values)\n",
    "#         list_ratios = [num_cit_paper/item   if item !=0  else 0 for item in lista_num_cit_ref]  #num_cit_paper / avg_num_cit_ref\n",
    "#         lista_ratios_all += list_ratios #.append(ratio)        \n",
    "        \n",
    "# print (len(lista_paper_UT), len(set(lista_paper_UT)), len(lista_ref_UT), len(set(lista_ref_UT)), np.mean(lista_num_ref), np.mean(lista_ratios_all) )\n",
    "\n",
    "\n",
    "\n",
    "# lista_paper_UT=[]\n",
    "# lista_ref_UT=[]\n",
    "# lista_num_ref=[]\n",
    "\n",
    "# lista_ratios_selected_year=[]\n",
    "\n",
    "# for paper_UT, group in preselection_df.groupby(['paper_UT']):  ### for df_merged: 158813 158813 5617059 2607457 43.600139787 0.0542472340106   # for preselection_df3: 6211 6211 30225 25508 6.34068587989 0.472185840259  # selected year: 7005 7005 260294 214744 46.4395431834 0.110949088539\n",
    "        \n",
    "#         lista_paper_UT.append(paper_UT)\n",
    "#         lista_ref_UT += list(group.reference_UT.unique())\n",
    "#         lista_num_ref.append(len(group))\n",
    "#         num_cit_paper = float(group.paper_cite_count.unique()) # the group refers to one single plos\n",
    "#         lista_num_cit_ref = list(group.cite_count.values)\n",
    "#         list_ratios = [num_cit_paper/item   if item !=0  else 0 for item in lista_num_cit_ref] #num_cit_paper / avg_num_cit_ref\n",
    "#         lista_ratios_selected_year += list_ratios #.append(ratio)   \n",
    "        \n",
    "        \n",
    "# print (len(lista_paper_UT), len(set(lista_paper_UT)), len(lista_ref_UT), len(set(lista_ref_UT)), np.mean(lista_num_ref), np.mean(lista_ratios_selected_year) )\n",
    "\n",
    "\n",
    "\n",
    "# lista_paper_UT=[]\n",
    "# lista_ref_UT=[]\n",
    "# lista_num_ref=[]\n",
    "\n",
    "# lista_ratios_selected_year_young_ref=[]\n",
    "\n",
    "\n",
    "# for paper_UT, group in preselection_df3.groupby(['paper_UT']):  ### for df_merged: 158813 158813 5617059 2607457 43.600139787 0.0542472340106   # for preselection_df3: 6211 6211 30225 25508 6.34068587989 0.472185840259  # selected year: 7005 7005 260294 214744 46.4395431834 0.110949088539\n",
    "        \n",
    "#         lista_paper_UT.append(paper_UT)\n",
    "#         lista_ref_UT += list(group.reference_UT.unique())\n",
    "#         lista_num_ref.append(len(group))\n",
    "#         num_cit_paper = float(group.paper_cite_count.unique()) # the group refers to one single plos\n",
    "#         lista_num_cit_ref = list(group.cite_count.values)\n",
    "#         list_ratios = [num_cit_paper/item   if item !=0  else 0 for item in lista_num_cit_ref]  #num_cit_paper / avg_num_cit_ref\n",
    "#         lista_ratios_selected_year_young_ref += list_ratios #.append(ratio)   \n",
    "        \n",
    "        \n",
    "# print (len(lista_paper_UT), len(set(lista_paper_UT)), len(lista_ref_UT), len(set(lista_ref_UT)), np.mean(lista_num_ref), np.mean(lista_ratios_selected_year_young_ref) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_paper_UT=[]\n",
    "# lista_ref_UT=[]\n",
    "# lista_num_ref=[]\n",
    "\n",
    "# lista_ratios_selected_year_young_ref_top_plos=[]\n",
    "# df_select = preselection_df3[preselection_df3['paper_UT'].isin(lista_top_plos)]\n",
    "\n",
    "# for paper_UT, group in df_select.groupby(['paper_UT']):  ### for df_merged: 158813 158813 5617059 2607457 43.600139787 0.0542472340106   # for preselection_df3: 6211 6211 30225 25508 6.34068587989 0.472185840259  # selected year: 7005 7005 260294 214744 46.4395431834 0.110949088539\n",
    "        \n",
    "#         lista_paper_UT.append(paper_UT)\n",
    "#         lista_ref_UT += list(group.reference_UT.unique())\n",
    "#         lista_num_ref.append(len(group))\n",
    "#         num_cit_paper = float(group.paper_cite_count.unique()) # the group refers to one single plos\n",
    "        \n",
    "#         lista_num_cit_ref = list(group.cite_count.values)\n",
    "#         list_ratios = [num_cit_paper/item   if item !=0  else 0 for item in lista_num_cit_ref]  #num_cit_paper / avg_num_cit_ref\n",
    "#         lista_ratios_selected_year_young_ref_top_plos += list_ratios #.append(ratio)   \n",
    "        \n",
    "        \n",
    "# print (len(lista_paper_UT), len(set(lista_paper_UT)), len(lista_ref_UT), len(set(lista_ref_UT)), np.mean(lista_num_ref), np.mean(lista_ratios_selected_year_young_ref_top_plos) )\n",
    "\n",
    "\n",
    "\n",
    "# lista_paper_UT=[]\n",
    "# lista_ref_UT=[]\n",
    "# lista_num_ref=[]\n",
    "\n",
    "# lista_ratios_selected_year_young_ref_top_plos_top_ref=[]\n",
    "\n",
    "# df_select = preselection_df3[preselection_df3['paper_UT'].isin(lista_top_plos)]\n",
    "# df_select = df_select[df_select['reference_UT'].isin(lista_top_ref)]\n",
    "\n",
    "\n",
    "# for paper_UT, group in df_select.groupby(['paper_UT']):  ### for df_merged: 158813 158813 5617059 2607457 43.600139787 0.0542472340106   # for preselection_df3: 6211 6211 30225 25508 6.34068587989 0.472185840259  # selected year: 7005 7005 260294 214744 46.4395431834 0.110949088539\n",
    "        \n",
    "#         lista_paper_UT.append(paper_UT)\n",
    "#         lista_ref_UT += list(group.reference_UT.unique())\n",
    "#         lista_num_ref.append(len(group))\n",
    "#         num_cit_paper = float(group.paper_cite_count.unique()) # the group refers to one single plos\n",
    "#         lista_num_cit_ref = list(group.cite_count.values)\n",
    "#         list_ratios = [num_cit_paper/item   if item !=0  else 0 for item in lista_num_cit_ref]  #num_cit_paper / avg_num_cit_ref\n",
    "#         lista_ratios_selected_year_young_ref_top_plos_top_ref += list_ratios #.append(ratio)   \n",
    "        \n",
    "# print (len(lista_paper_UT), len(set(lista_paper_UT)), len(lista_ref_UT), len(set(lista_ref_UT)), np.mean(lista_num_ref), np.mean(lista_ratios_selected_year_young_ref_top_plos_top_ref) )\n",
    "\n",
    "\n",
    "# # 158813 158813 5617059 2607457 43.600139787 0.0542472340106\n",
    "# # 7005 7005 260294 214744 46.4395431834 0.110949088539\n",
    "# # 6211 6211 30225 25508 6.34068587989 0.472185840259\n",
    "# # 312 312 2266 2092 9.46474358974 1.36087421586\n",
    "# # 169 169 379 293 3.01775147929 0.209680689138\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "# trace1 = go.Histogram(x=lista_ratios_all,\n",
    "#                      name=\"all plos; mean: \"+str(np.nanmean(lista_ratios_all)),\n",
    "#                       histnorm='probability',\n",
    "#                      opacity = 0.75)\n",
    "\n",
    "# trace2 = go.Histogram(x=lista_ratios_selected_year,\n",
    "#                      name=\"plos in selected year, all ref; mean: \"+str(np.nanmean(lista_ratios_selected_year)),\n",
    "#                       histnorm='probability',\n",
    "#                      opacity = 0.75)\n",
    "\n",
    "# trace3 = go.Histogram(x=lista_ratios_selected_year_young_ref,\n",
    "#                      name=\"plos in selected year, young ref only; mean: \"+str(np.nanmean(lista_ratios_selected_year_young_ref)),\n",
    "#                       histnorm='probability',\n",
    "#                      opacity = 0.75)\n",
    "\n",
    "# trace4 = go.Histogram(x=lista_ratios_selected_year_young_ref_top_plos,\n",
    "#                      name=\"TOP plos in selected year, young ref only; mean: \"+str(np.nanmean(lista_ratios_selected_year_young_ref_top_plos)),\n",
    "#                       histnorm='probability',\n",
    "#                      opacity = 0.75)\n",
    "\n",
    "# trace5 = go.Histogram(x=lista_ratios_selected_year_young_ref_top_plos_top_ref,\n",
    "#                      name=\"TOP plos in selected year, TOP young ref only; mean: \"+str(np.nanmean(lista_ratios_selected_year_young_ref_top_plos_top_ref)),\n",
    "#                       histnorm='probability',\n",
    "#                      opacity = 0.75)\n",
    "\n",
    "\n",
    "# data =[trace1,  trace2, trace3, trace4, trace5 ]\n",
    "\n",
    "# layout = go.Layout(\n",
    "#             title=str(years),\n",
    "#             xaxis = dict(title= 'Ratio final number of citations plos / final number citations ref'),\n",
    "#                 # type='log'),#, autorange=True),\n",
    "#             yaxis = dict(title= 'PDF'),\n",
    "#     )\n",
    "#                 # type='log'),#, autorange=True),        \n",
    "                  \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "\n",
    "# fig_filename='histogram_rations_num_cit_plos_num_cit_young_ref'\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=2000,\n",
    "#               filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â€‹PLOS FROM YEAR: [2006]\n",
    "#  of young ref. cited by top plos 23.6811594203   \n",
    "# of young ref. cited by bottom plos 29.4890510949\n",
    "\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2007]\n",
    "#   of young ref. cited by top plos 17.2086513995 393\n",
    "#   of young ref. cited by bottom plos 15.1619897959 784\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2008]\n",
    "#   of young ref. cited by top plos 24.7783300199 1006\n",
    "#   of young ref. cited by bottom plos 15.3371909521 1901\n",
    "    \n",
    "\n",
    "# PLOS FROM YEAR: [2009]\n",
    "#   of young ref. cited by top plos 24.2474916388 1495\n",
    "#   of young ref. cited by bottom plos 14.8216998192 2765\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2010]\n",
    "#   of young ref. cited by top plos 23.1682600382 2092\n",
    "#   of young ref. cited by bottom plos 14.1201875617 4052\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2011]\n",
    "#   of young ref. cited by top plos 18.746729461 3822\n",
    "#   of young ref. cited by bottom plos 12.3558732132 8045\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2012]\n",
    "#   of young ref. cited by top plos 17.878311915 6303\n",
    "#   of young ref. cited by bottom plos 11.0258678863 13337\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2013]\n",
    "#   of young ref. cited by top plos 19.2522829494 7337\n",
    "#   of young ref. cited by bottom plos 11.0004401408 13632\n",
    "\n",
    "\n",
    "# PLOS FROM YEAR: [2014]\n",
    "#   of young ref. cited by top plos 18.1754417598 5546\n",
    "#   of young ref. cited by bottom plos 10.3344650726 13843\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### i plot the values for number of early citations of young referneces for those cited by top non-top plos papers: values calculated in the code 'Prob_citing_ref_by_successful_plos.ipynb'\n",
    "\n",
    "# by_top_plos = [23.68, 17.208,  24.778, 24.247, 23.168, 17.878,  17.878, 19.252, 18.175 ]\n",
    "# by_bottom_plos = [ 29.489, 15.16 , 15.33, 14.82, 14.12, 11.025, 11.025, 11.00, 10.334 ]\n",
    "\n",
    "# x_labels=[2006,2007,2008,2009,2010,2011,2012,2013,2014]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# trace1 = go.Scatter(\n",
    "#     x=x_labels,\n",
    "#     y=by_top_plos,\n",
    "#     mode = 'lines + marks',\n",
    "#     name='if cited by top plos',\n",
    "#     marker=dict(\n",
    "#         color='rgb(133, 224, 133, 0.5)'\n",
    "#     )\n",
    "# )\n",
    "# trace2 = go.Scatter(\n",
    "#     x=x_labels,\n",
    "#     y=by_bottom_plos,\n",
    "#     mode = 'lines + marks',\n",
    "#     name='if cited by bottom plos',\n",
    "#     marker=dict(\n",
    "#         color='rgb(102, 163, 255, 0.5)'\n",
    "#     )\n",
    "# )\n",
    "\n",
    "\n",
    "# data = [trace1, trace2]\n",
    "# layout = go.Layout(  \n",
    "#     yaxis=dict(\n",
    "#         title= 'Avg number of early citations' , #v1_string.replace(\"_\",\" \"),   \n",
    "#         titlefont=dict(            \n",
    "#             size=30,\n",
    "#             color='black'\n",
    "#         #    color='lightgrey'\n",
    "#         ),  \n",
    "#         tickfont=dict(               \n",
    "#             size=20,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         #type='log'\n",
    "#     ) ,\n",
    "#     xaxis=dict(             \n",
    "#         tickfont=dict(               \n",
    "#             size=30,\n",
    "#             color='black'\n",
    "#         ),\n",
    "#         #type='log'\n",
    "#     ),\n",
    "#     legend=dict(       \n",
    "#         font=dict(           \n",
    "#             size=20),\n",
    "#             #color='#000'\n",
    "#         ),\n",
    "    \n",
    "# )\n",
    "\n",
    "# fig = go.Figure(data=data, layout=layout)\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename='line_plot' ,image_width=2000, image_height=1400,\n",
    "#               filename='stacked_bar_plot.html', validate=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###### DOES THE EFFECT GO AWAY OVER TIME??\n",
    "# #### I LOOK AT THE USAGE OF YOUNG REFERENCES FROM (EXAMPLE) 2010, IN THE NEXT YEARS 2011, 2012, 2013,... TO SEE IF BOTTOM PAPERS CATCH UP TO CITING THE GOOD YOUNG PAPERS AFTER THEY BECOME MORE MAINSTREAM\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "# years=[2009]\n",
    "  \n",
    "# string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "# string_self_ref=0#\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. records: 6.9M\n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# print (\"original size:\",df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# #print (years)\n",
    "# preselection_df = df_merged[df_merged['plos_pub_year'].isin(years)]  \n",
    "# #print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "     \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#    # print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         #print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "# #    print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# #print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# #print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "# time_window_young = 1\n",
    "# string_age_selection=\"only young references from >=\"+ str((min(years)-time_window_young))\n",
    "# preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "# #print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "# N_all=len(preselection_df3)\n",
    "\n",
    "  \n",
    "# ############## i get the list of focus (young) references, to look at its usage in next years:\n",
    "    \n",
    "    \n",
    "# list_focus_young_ref = list(preselection_df3.reference_UT.unique())\n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(list_focus_young_ref),'\\n')\n",
    "\n",
    "   \n",
    " \n",
    "\n",
    "\n",
    "\n",
    "# ##############################################  all this was just to select the focus young references\n",
    "# ####################################################\n",
    "# ####################################################\n",
    "# ####################################################\n",
    "# ####################################################\n",
    "       \n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    " \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# dict_group_subset_data={}\n",
    "# dict_group_quantiles_size={}\n",
    "\n",
    "# ######### in this cell I SELECT the data i want to plot (by multiple criteria), as well as the variable that will encode with color:\n",
    "# ######### ######### ######### ######### ######### ######### ######### \n",
    "\n",
    "\n",
    "# v1_string =  'cite_count'\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "# later=4\n",
    "# years=[int(years[-1])+later]  # I look at the focus references in the next following years\n",
    "\n",
    "\n",
    "# string_references_age = \"\"  ## ojo! here i want all references (from the previous selection: young ref)\n",
    "\n",
    "\n",
    "# string_isolated_ref=\"\"  #\"\"   # 0  or 1 (or empty string, to include all ref)\n",
    "  \n",
    "# string_self_ref=0#\"\"#1   # 0  or 1 (or empty string, to include all ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### plos ONE categories. records: 6.9M\n",
    "# string_code_categ=\"\" #  ojo!!! the codes are strings, not integers. if i want to include multiple subjects:  \"1 2 8\"\n",
    "\n",
    "# #  '0': 'Biology and life sciences'             6,032,537\n",
    "# #  '1': 'Computer and information sciences'     1,207,799\n",
    "# #  '10': 'Social sciences'                      755,899\n",
    "# #  '2': 'Earth sciences'                        533,155\n",
    "# #  '3': 'Ecology and environmental sciences'    624,142\n",
    "# #  '4': 'Engineering and technology'            382,247 \n",
    "# #  '5': 'Medicine and health sciences'          4,535,926   \n",
    "# #  '6': 'People and places'                     691,523\n",
    "# #  '7': 'Physical sciences'                     2,100,827\n",
    "# #  '8': 'Research and analysis methods'         3,871,470\n",
    "# #  '9': 'Science policy'                        43,360 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_journal=\"\"\n",
    "\n",
    "#     # PLOS ONE       6,367,070\n",
    "#     # PLOS GENET      149,923\n",
    "#     # PLO NE TR D     138,289   # (neglected tropical diseases)\n",
    "#     # PLOS PATHOG     109,803\n",
    "#     # PLOS COMPUT      77,924\n",
    "#     # PLOS BIOL        56,754\n",
    "#     # PLOS MED         24,506\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# string_plos_field=\"\"#['D CU BIOLOGY']\"\n",
    "\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES']                                                                                                       4464540\n",
    "# # ['D CU BIOLOGY']                                                                                                                          1055045\n",
    "# # ['D RO MULTIDISCIPLINARY SCIENCES', 'D CU BIOLOGY']                                                                                        847485\n",
    "# # ['D KM GENETICS & HEREDITY']                                                                                                               149923\n",
    "# # ['D YU TROPICAL MEDICINE', 'D TI PARASITOLOGY']                                                                                            138289\n",
    "# # ['D ZE VIROLOGY', 'D QU MICROBIOLOGY', 'D TI PARASITOLOGY']                                                                                109803\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY']                                                          77687\n",
    "# # ['D CQ BIOCHEMISTRY & MOLECULAR BIOLOGY', 'D CU BIOLOGY']                                                                                   56754\n",
    "# # ['D PY MEDICINE, GENERAL & INTERNAL']                                                                                                       24506\n",
    "# # ['D CO BIOCHEMICAL RESEARCH METHODS', 'D MC MATHEMATICAL & COMPUTATIONAL BIOLOGY', 'D PO MATHEMATICS, INTERDISCIPLINARY APPLICATIONS']        237\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "# new_selection_df = df_merged[df_merged['reference_UT'].isin(list_focus_young_ref)]  \n",
    "\n",
    "# print (new_selection_df.shape)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# ##### preselection by plos year\n",
    "# print (years)\n",
    "# preselection_df = new_selection_df[new_selection_df['plos_pub_year'].isin(years)]  \n",
    "# print (\"size of preselection1 (by plos years):\",preselection_df.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #### i remove self-citations\n",
    "# if (string_self_ref==0) or  ( string_self_ref == 1 ): \n",
    "#     preselection_df = preselection_df[preselection_df['self_citation']== string_self_ref ]  \n",
    "#     if string_self_ref ==0:\n",
    "#         string_self_ref = \", no self-cit\"\n",
    "#     elif string_self_ref ==1:\n",
    "#         string_self_ref = \", only self-cit\"\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "# ######### preselection by isolated or group references:\n",
    "# if (string_isolated_ref==0) or  ( string_isolated_ref == 1 ): \n",
    "#     preselection_df0 = preselection_df[preselection_df['isolated_citation']== string_isolated_ref ]  \n",
    "    \n",
    "#     if string_isolated_ref ==0:\n",
    "#         string_isolated_ref = \", group ref\"\n",
    "#     elif string_isolated_ref ==1:\n",
    "#         string_isolated_ref = \", isolated ref\"\n",
    "# else:    \n",
    "#     preselection_df0 = preselection_df   \n",
    "#     print (\"size of preselection1 (by isolated/group ref):\",preselection_df0.shape, string_isolated_ref)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# ######### preselection by plos ONE subject category:\n",
    "# if string_code_categ==\"\": \n",
    "#     preselection_df111 = preselection_df0\n",
    "# else:    \n",
    "#     if \" \" not in string_code_categ:  # to include one single category\n",
    "#         preselection_df111 = preselection_df0[preselection_df0['categ_codes'].str.contains(string_code_categ)]        \n",
    "#         string_code_categ = \" \"+dict_code_categ[string_code_categ]  \n",
    "        \n",
    "#     else:  # if multiple codes-categories\n",
    "#         list_codes = string_code_categ.split(\" \")\n",
    "#         print (list_codes)\n",
    "\n",
    "#         if len(list_codes) >= 2:              \n",
    "#             preselection_df111 = preselection_df0[ preselection_df0['categ_codes'].str.contains('|'.join(list_codes)) ]  # to look for partial matches from a list of strings!!!!!\n",
    "            \n",
    "       \n",
    "#         string_code_categ = \"\" \n",
    "#         for code in list_codes:\n",
    "#             string_code_categ += \"-\"+dict_code_categ[code] \n",
    "            \n",
    "            \n",
    "#     print (\" size of preselection (by plos ONE subject category):\",preselection_df111.shape, string_code_categ)\n",
    "           \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "# ######### preselection by plos journal:\n",
    "# if string_journal==\"\": \n",
    "#     preselection_df1 = preselection_df111\n",
    "# else:    \n",
    "#     preselection_df1 = preselection_df111[preselection_df111['plos_j1']== string_journal ]  \n",
    "# print (\" size of preselection2 (by plos journal):\",preselection_df1.shape, string_journal)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ######### preselection by plos field:\n",
    "# if string_plos_field==\"\": \n",
    "#     preselection_df2 = preselection_df1\n",
    "# else:    \n",
    "#     preselection_df2 = preselection_df1[preselection_df1['plos_field']== string_plos_field ]  \n",
    "# print (\" size of preselection2 (by plos field):\",preselection_df2.shape, string_plos_field)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# preselection_df3 = preselection_df2\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "# fig_font_colors=''\n",
    "\n",
    "\n",
    "\n",
    "# if v1_string ==  'cite_count'  or       v1_string ==  'log_num_cit_ref'   or v1_string == 'log2_num_cit_ref':\n",
    "     \n",
    "      \n",
    "    \n",
    "#     string_age_selection=''\n",
    "\n",
    "#     ##### preselection only young/old references:        \n",
    "#     if string_references_age == \"young\":\n",
    "#         time_window = 1\n",
    "#         string_age_selection=\"only young references from >=\"+ str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] >= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape, string_age_selection)\n",
    "        \n",
    "#     elif string_references_age == \"old\":\n",
    "#         time_window = 10\n",
    "#         string_age_selection=\"only old references from <=\"+str((min(years)-time_window))\n",
    "#         preselection_df3 = preselection_df2[preselection_df2['ref_pub_year'] <= (min(years)-time_window) ]   \n",
    "#         print (\"  size of preselection3 (only young references):\",preselection_df3.shape,string_age_selection )\n",
    "        \n",
    "#     else:\n",
    "#         string_age_selection=\"young&old\"       \n",
    "#         print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "          \n",
    "    \n",
    "    \n",
    "\n",
    "#     N_plos=len(preselection_df3.paper_UT.unique())        ## this values are overall, for the title   \n",
    "#     N_all=len(preselection_df3)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     fig_colorscale = [[0, '#ffece6'], [1, '#ff0000']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#ff0000', '#ffece6']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "    \n",
    "    \n",
    "#     fig_colorscale = \"Reds\"\n",
    "#     fig_title_plot = \"Median (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename =   '../plots/annotated-heatmap_median_citations_of_references_for_sections_and_fract_subsection_by_citations_of_plos'\n",
    "  \n",
    "#     if  v1_string ==  'log_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log10 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     elif  v1_string ==  'log2_num_cit_ref' :\n",
    "#         fig_title_plot = \"Median log2 of (final) number of citations of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+\", \"+string_age_selection+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# elif v1_string =='ref_pub_year':\n",
    "#     fig_colorscale = \"Viridis\"\n",
    "#     fig_title_plot = \"Median age of references in \"+string_journal+string_plos_field+\"<br> papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_of_references_for_sections_and_fract_subsection_by_citations_of_plos'   \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "   \n",
    "# elif v1_string =='diff_year_plos_ref':\n",
    "\n",
    "#     fig_colorscale = [[0, 'dcf0d2'], [1, '#205803']]   # if i give it a min and a max colors in HEX, it creates a gradient from one to another\n",
    "#     fig_font_colors = ['#205803', '#dcf0d2']      # same for the annotation of the boxes (to make sure they are readable)\n",
    "#     fig_title_plot = \"Median difference between publication year of plos and references in \"+string_journal+string_plos_field+\" papers from \"+str(years)+string_isolated_ref+string_self_ref+string_code_categ+\"<br>Number of occurrences: \"+str(N_all)#+\",   Number plos: \"+str(N_plos)\n",
    "#     fig_filename = '../plots/annotated-heatmap_median_age_difference_plos_publ_year_vs_references_for_sections_and_subsect_by_citations_of_plos'\n",
    "           \n",
    "#     print (\"  No preselection by age of references:\",preselection_df3.shape )\n",
    "    \n",
    "     \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# print (\"\\nTot # records included:\",len(preselection_df3),\"   # number of plos papers:\",len(preselection_df3.paper_UT.unique()), \"   # unique ref:\", len(preselection_df3.reference_UT.unique()),'\\n')\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    "# #### i get the bins number of citation of the plos papers OJO!!!!! i want the same bins for all papers (so i calculate them before separating into sections but after all the preselections)\n",
    "# #list_q=[0.5,0.9,.99,1]\n",
    "# #list_q=[0.25,0.5,0.9,.99,1]\n",
    "# list_q=[0.3,0.6,.9,.99,1]\n",
    "\n",
    "# #quantiles=sorted(list(df_plos[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "# quantiles=sorted(list(preselection_df3[string_filtering_x].quantile(list_q).to_dict().items())) #mean 10.68 \n",
    "\n",
    "# lista_bins_plos_citations=[]\n",
    "# old_value=0\n",
    "# for item in quantiles:\n",
    "#     pair=[old_value, int(item[1])]\n",
    "#     lista_bins_plos_citations.append(pair)\n",
    "#     old_value = int(item[1])\n",
    "\n",
    "# print (lista_bins_plos_citations)\n",
    "\n",
    "\n",
    "\n",
    "# ### i modify the bins to separete the zero-one\n",
    "# # lista_bins_plos_citations[0][0]=2       \n",
    "# # lista_bins_plos_citations = [[0,2]] + lista_bins_plos_citations    \n",
    "# #print (lista_bins_plos_citations)\n",
    "\n",
    "# ################################################\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "       \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_sections = [\"intro\",\"methods\",\"results\",\"discussion\"]\n",
    "\n",
    "# cont=0\n",
    "# for item in lista_bins_plos_citations:\n",
    "\n",
    "#     minimo = item[0]\n",
    "#     maximo = item[1]\n",
    "\n",
    "\n",
    "#     preselection_df4 = preselection_df3[(preselection_df3[string_filtering_x] >= minimo)  &  (preselection_df3[string_filtering_x] < maximo)]\n",
    "#     #print (\"size of preselection3 (by cit bin plos):\",df_select.shape, item)\n",
    "\n",
    "\n",
    "\n",
    "#     x1_All = list(preselection_df4[v1_string])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "    \n",
    "#     for string_section in lista_sections:\n",
    "\n",
    "\n",
    "#         ##### preselection to include only occurences in a section of the paper\n",
    "#         if  string_section == \"intro\":\n",
    "#             section=0\n",
    "#         elif  string_section == \"methods\":\n",
    "#             section=1\n",
    "#         elif  string_section == \"results\":\n",
    "#             section=2\n",
    "#         elif  string_section == \"discussion\":\n",
    "#             section=3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         df_select = preselection_df4[preselection_df4['regex_sect_index']== section]   \n",
    "#         #print (\"size of preselection2 (by section):\",preselection_df3.shape, string_section)\n",
    "\n",
    "#         x1 = list(df_select[v1_string])       \n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "#         if cont ==0:            \n",
    "#             group=string_section+\" Bottom 30%\" \n",
    "#         elif cont ==1:            \n",
    "#             group=string_section+\" 31% to 60%\"            \n",
    "#         elif cont==2:\n",
    "#              group=string_section+\" 61% to 90%\"            \n",
    "#         elif cont==3: \n",
    "#             group=string_section+\" 91% to 99%\"            \n",
    "#         elif cont==4:\n",
    "#              group=string_section+\" Top 1%\"    \n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "#         ######### i get also quantiles for each cell:    \n",
    "#         list_quantiles_cell=[.25,.5,.75]\n",
    "\n",
    "#         values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "\n",
    "#         tupla=values_quantiles + [len(x1)]\n",
    "\n",
    "#         dict_group_quantiles_size[group] = tupla\n",
    "\n",
    "#         dict_group_subset_data[group]=x1\n",
    "               \n",
    "\n",
    "\n",
    "#     cont +=1\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "# ################ i also add the median values for the section across all data in the preselection\n",
    "# for string_section in lista_sections:\n",
    "\n",
    "        \n",
    "#     if  string_section == \"intro\":\n",
    "#         section=0\n",
    "#     elif  string_section == \"methods\":\n",
    "#         section=1\n",
    "#     elif  string_section == \"results\":\n",
    "#         section=2\n",
    "#     elif  string_section == \"discussion\":\n",
    "#         section=3\n",
    "\n",
    "\n",
    "#     df_select = preselection_df3[preselection_df3['regex_sect_index']== section]   \n",
    "        \n",
    "#     list_quantiles_cell=[.25,.5,.75]\n",
    "#     values_quantiles=list(df_select[v1_string].quantile(list_quantiles_cell))#sorted(list(df_select[v1_string].quantile(list_quantiles_cell).to_dict().items()))      \n",
    "#     tupla=values_quantiles + [len(df_select)]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     dict_group_quantiles_size[string_section+\" ALL PLOS\"]=tupla\n",
    "#     dict_group_subset_data[string_section+\" ALL PLOS\"]=x1_All    \n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# ########  I create the x, y, z lists of values for the heatmap\n",
    "\n",
    "# lista_y=lista_sections\n",
    "# #lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\",\" ALL\"]\n",
    "# lista_bin_names=[\" ALL PLOS\",\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]\n",
    "# #lista_x=[\" Bottom 30%\",\" 31% to 60%\",\" 61% to 90%\" ,\" 91% to 99%\",\" Top 1%\"]   \n",
    "\n",
    "# lista_x=lista_bin_names\n",
    "\n",
    "# lista_z25=[]\n",
    "# lista_z50=[]\n",
    "# lista_z75=[]\n",
    "# lista_z_sizes=[]\n",
    "\n",
    "# for x_value in lista_x:    \n",
    "#     aux_lista25=[]\n",
    "#     aux_lista50=[]\n",
    "#     aux_lista75=[]\n",
    "#     aux_lista_sizes=[]\n",
    "    \n",
    "#     for y_value in lista_y:       \n",
    "\n",
    "#         llave=y_value+x_value\n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][0])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][0]\n",
    "#         aux_lista25.append(value)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][1])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][1]\n",
    "#         aux_lista50.append(value)\n",
    "      \n",
    "    \n",
    "    \n",
    "    \n",
    "#         try:\n",
    "#             value=int(dict_group_quantiles_size[llave][2])\n",
    "#         except:  # if it is a nan:\n",
    "#             value=dict_group_quantiles_size[llave][2]\n",
    "#         aux_lista75.append(value)\n",
    "      \n",
    "        \n",
    "        \n",
    "        \n",
    "#         value_size=dict_group_quantiles_size[llave][3]\n",
    "#         aux_lista_sizes.append(value_size)\n",
    "        \n",
    "        \n",
    "#         #print (y_value,\" \",x_value, value, value_size)\n",
    "#     lista_z25.append(aux_lista25)\n",
    "#     lista_z50.append(aux_lista50)\n",
    "#     lista_z75.append(aux_lista75)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     lista_z_sizes.append(aux_lista_sizes)\n",
    "    \n",
    "   \n",
    "\n",
    "# # print (\"lista values 25%-quantile:\",lista_z25)\n",
    "# # print (\"lista sizes\",lista_z_sizes)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# lista_text_z=[]\n",
    "# for i in range(len(lista_z_sizes)):\n",
    "#     aux=[]\n",
    "#     for j in range(len(lista_z_sizes[0])):\n",
    "#         value=\"q:(\"+str(lista_z25[i][j])+\",\"+str(lista_z50[i][j])+\",\"+str(lista_z75[i][j])+\")\"+\"<br> N:\"+str(lista_z_sizes[i][j])            #\"Median:\"+str(lista_z[i][j])+\"<br> N:\"+str(lista_z_sizes[i][j])\n",
    "#         aux.append(value)\n",
    "#     lista_text_z.append(aux)\n",
    "# # print (lista_text_z)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##### using a different library\n",
    "# path=   '/home/staff/julia/at_Northwestern/In_Text_Citations/In-Text-Citations-New/plots/'\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names, annotation_text=lista_text_z, colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "# #fig = ff.create_annotated_heatmap(z=lista_z50, x=lista_sections, y=lista_bin_names,  colorscale=fig_colorscale, font_colors=fig_font_colors,showscale=True)#, reversescale=True)\n",
    "# fig.layout.title = fig_title_plot\n",
    "\n",
    "# fig['layout']['xaxis']['side'] = 'bottom'\n",
    "# # fig.layout.xaxis.update({'title': 'Section'})\n",
    "# # fig.layout.yaxis.update({'title': 'Citation percentile of plos paper'})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# font_gral=20   # 20 if i wanna see it on the browser, 40 if i care about the png output\n",
    "# fig['layout']['font']['size'] = font_gral\n",
    "\n",
    "\n",
    "# # Altering x axis\n",
    "# #fig['layout']['xaxis']['tickfont']['family'] = 'Gill Sans MT'\n",
    "# fig['layout']['xaxis']['tickfont']['size'] = font_gral -5\n",
    "# fig['layout']['yaxis']['tickfont']['size'] = font_gral -15\n",
    "\n",
    "# fig['layout']['xaxis']['tickangle'] = 0\n",
    "# fig['layout']['yaxis']['tickangle'] = -90\n",
    "# fig['layout']['xaxis']['titlefont']['size'] = font_gral\n",
    "# fig['layout']['yaxis']['titlefont']['size'] = font_gral\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# offline.plot(fig, auto_open=True, image = 'png', image_filename=fig_filename ,image_width=2000, image_height=1400,\n",
    "#               filename=fig_filename+'.html', validate=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for pair in itertools.combinations(dict_group_subset_data.keys(), 2):    \n",
    "#     set1=dict_group_subset_data[pair[0]]\n",
    "#     set2=dict_group_subset_data[pair[1]]\n",
    "#     print (\"comparison\",pair, \"\\t\\t\",stats.ks_2samp(set1, set2),\"\\n\"  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['occurence', 'reference_UT', 'reference_rank', 'regex_sect_index',\n",
       "       'cite_count', 'ref_pub_year', 'paper_cite_count', 'plos_pub_year',\n",
       "       'sect_char_pos', 'sect_char_total', 'plos_j1', 'ref_field', 'ref_j1',\n",
       "       'log2_num_cit_ref', 'log2_num_cit_paper', 'diff_year_plos_ref',\n",
       "       'rel_loc_in_sect', 'plos_field', 'isolated_citation', 'paper_UT',\n",
       "       'total_refs', 'categ_codes', 'self_citation', 'number_authors',\n",
       "       'plos_article_type', 'num_cit_young_ref_by2009',\n",
       "       'num_cit_young_ref_by2010', 'num_cit_young_ref_by2011',\n",
       "       'num_cit_young_ref_by2012', 'num_cit_young_ref_by2013',\n",
       "       'num_cit_young_ref_by2009after8', 'num_cit_young_ref_by2008after8',\n",
       "       'num_cit_young_ref_by2010after8', 'num_cit_young_ref_by2007after8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
