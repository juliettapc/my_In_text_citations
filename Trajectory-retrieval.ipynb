{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import datetime\n",
    "import pickle\n",
    "import gzip\n",
    "import os,glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import operator\n",
    "\n",
    "#sys.path\n",
    "from tqdm import tqdm\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "\n",
    "import regex as re\n",
    "import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "from IPython.core.display import display,HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))  # to make the notebook use the entire width of the browser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is the MongoConnection class from the Amaral lab LabTools folder.\n",
    "\n",
    "from __future__ import print_function, unicode_literals\n",
    "import sys\n",
    "from pymongo import MongoClient\n",
    "\n",
    "\n",
    "class MongoConnection(object):\n",
    "    def __init__(self, cxnSettings, **kwargs):\n",
    "        self.settings = cxnSettings\n",
    "        self.mongoURI = self._constructURI()\n",
    "        self.connect(**kwargs)\n",
    "        self.ensure_index()\n",
    "\n",
    "    def _constructURI(self):\n",
    "        '''\n",
    "        Construct the mongo URI\n",
    "        '''\n",
    "        mongoURI = 'mongodb://'\n",
    "        #User/password handling\n",
    "        if 'user'in self.settings and 'password' in self.settings:\n",
    "            mongoURI += self.settings['user'] + ':' + self.settings['password']\n",
    "            mongoURI += '@'\n",
    "        elif 'user' in self.settings:\n",
    "            print('Missing password for given user, proceeding without either')\n",
    "        elif 'password' in self.settings:\n",
    "            print('Missing user for given passord, proceeding without either')\n",
    "        #Host and port\n",
    "        try:\n",
    "            mongoURI += self.settings['host'] + ':'\n",
    "        except KeyError:\n",
    "            print('Missing the hostname. Cannot connect without host')\n",
    "            sys.exit()\n",
    "        try:\n",
    "            mongoURI += str(self.settings['port'])\n",
    "        except KeyError:\n",
    "            print('Missing the port. Substituting default port of 27017')\n",
    "            mongoURI += str('27017')\n",
    "        return mongoURI\n",
    "\n",
    "    def connect(self, **kwargs):\n",
    "        '''\n",
    "        Establish the connection, database, and collection\n",
    "        '''\n",
    "        self.connection = MongoClient(self.mongoURI, **kwargs)\n",
    "        #########\n",
    "        try:\n",
    "            self.db = self.connection[self.settings['db']]\n",
    "        except KeyError:\n",
    "            print(\"Must specify a database as a 'db' key in the settings file\")\n",
    "            sys.exit()\n",
    "        #########\n",
    "        try:\n",
    "            self.collection = self.db[self.settings['collection']]\n",
    "        except KeyError:\n",
    "            print('Should have a collection.', end='')\n",
    "            print('Starting a collection in database', end='')\n",
    "            print(' for current connection as test.')\n",
    "            self.collection = self.db['test']\n",
    "\n",
    "    def tearDown(self):\n",
    "        '''\n",
    "        Closes the connection\n",
    "        '''\n",
    "        self.connection.close()\n",
    "\n",
    "    def ensure_index(self):\n",
    "        '''\n",
    "        Ensures the connection has all given indexes.\n",
    "        indexes: list of (`key`, `direction`) pairs.\n",
    "            See docs.mongodb.org/manual/core/indexes/ for possible `direction`\n",
    "            values.\n",
    "        '''\n",
    "        if 'indexes' in self.settings:\n",
    "            for index in self.settings['indexes']:\n",
    "                self.collection.ensure_index(index[0], **index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_papers_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"merged_papers\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "issues_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"issues\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "journal_settings = {\n",
    "    \"host\": \"chicago.chem-eng.northwestern.edu\",\n",
    "    \"port\": \"27017\",\n",
    "    \"db\": \"web_of_science_aux\",\n",
    "    \"collection\": \"journals\",\n",
    "    \"user\": \"mongoreader\",\n",
    "    \"password\": \"emptycoffeecup\"\n",
    "}\n",
    "papers_con = MongoConnection(merged_papers_settings)\n",
    "issue_con = MongoConnection(issues_settings)\n",
    "journal_con = MongoConnection(journal_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List_papers=['271721400012', '268637800016', '275809700007', '266872900016', '273779000031', '276311600005', '265514200012', '269229800016', '269415800006', '279140800001', '275257300012',\\\n",
    "#              '280557200015', '269229800011', '265837000004', '266214700013', '274590500027', '271685800011', '280371800009', '275620600001', '273338500021', '272828400001', '266320000010',\\\n",
    "#              '280243300011', '278599600024', '275063400012', '283216400020', '283573800004', '276454000014', '266490000014', '265510800008', '268637700007', '269220900003', '265490800004',\\\n",
    "#              '266415000008', '278601000002', 273414100007, 266716900003, 273896300006, 276952500022, 270594300016, 265505300002, 277776500010, 277079500001, 273554600021, 279259900008, 277773700011, 268739100017, 275620900007, 270160900010, 273896500011, 271721900009, 284087800017, 268035600004, 266107500006, 280520400008, 267081300004, 282869800004, 268773300004, 282748100005, 265513800007, 285578000036, 285575200067, 285041800017, 265505700011, 274231500013, 274442400021, 276454000008, 265514400020, 265482400006, 275328800002, 285579200021, 284231800014, 265513800010, 271936700019, 276418200050, 274442800021, 274139100002, 282676700018, 267806300015, 277239500014, 276418200009, 280243800032, 275894500009, 281687100001, 269267400001, 278125500010, 266221100026]\n",
    "\n",
    "\n",
    "# new_list= [ '000'+str(item) for item in List_papers]\n",
    "\n",
    "# new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for UT in new_list:\n",
    "#     doc = papers_con.collection.find_one({\"UT\":UT})  # i look at a given ref_ut paper\n",
    "#         #if 'citations' in doc:  # if the paper ut has recieved any citations \n",
    "        \n",
    "#     print  (UT,doc['TI'] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime_filler_traj(traj_dict):\n",
    "    # Generate a new dictionary from a trajectory dictionary with datetime objects. Includes the plos_flag value\n",
    "    \n",
    "\n",
    "    numeric = re.compile('[0-9]+')\n",
    "    alpha = re.compile('[A-Za-z]+')\n",
    "    month_dict = {'JAN':1, 'FEB': 2, 'MAR':3, 'APR':4, 'MAY':5, 'JUN': 6, \n",
    "                  'JUL': 7, 'AUG': 8, 'SEP': 9, 'OCT':10, 'NOV': 11, 'DEC': 12,\n",
    "                 'SPR': 3, 'SUM': 6, 'FAL': 9, 'WIN': 12}\n",
    "    formatted_traj_dict = {}\n",
    "    \n",
    "    for ref_ut in traj_dict:\n",
    "        formatted_traj_dict[ref_ut] = {}\n",
    "        for cite_ut in traj_dict[ref_ut]:\n",
    "            date = traj_dict[ref_ut][cite_ut][0]\n",
    "            year = traj_dict[ref_ut][cite_ut][1]\n",
    "            plos_flag = traj_dict[ref_ut][cite_ut][2]\n",
    "            \n",
    "            date_field = []\n",
    "\n",
    "            if type(date) == str:\n",
    "                \n",
    "                for i in re.findall(alpha, date):\n",
    "                    date_field.append(i)\n",
    "                for i in re.findall(numeric, date):\n",
    "                    date_field.append(i)\n",
    "                \n",
    "                if len(date_field) > 2: # No weirdness here, only 2 values max for date field\n",
    "                    print(date_field)\n",
    "                \n",
    "                if len(date_field[0])==1: # There's a single date that is just '0'. Likely a database error/nan\n",
    "                    print(date_field)\n",
    "\n",
    "                    \n",
    "            else:\n",
    "                date_field = ['JUNE', '15']\n",
    "                \n",
    "            month = -1\n",
    "            day = -1\n",
    "            \n",
    "            if date_field[0] in month_dict:\n",
    "                month = month_dict[date_field[0]]\n",
    "                if len(date_field)==2:\n",
    "                    if len(date_field[1])<3:\n",
    "                        day = int(date_field[1])\n",
    "            \n",
    "                        \n",
    "            if not math.isnan(year):\n",
    "                if month != -1:\n",
    "                    if day != -1:\n",
    "                        dt = datetime.datetime(year, month, day)\n",
    "                    else:\n",
    "                        dt = datetime.datetime(year, month, 15)\n",
    "                    \n",
    "                else:\n",
    "                    dt = datetime.datetime(year, 6, 15)\n",
    "\n",
    "\n",
    "            else:\n",
    "                dt = -1\n",
    "            \n",
    "            formatted_traj_dict[ref_ut][cite_ut] = [dt, plos_flag]\n",
    "\n",
    "    return formatted_traj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df_zero_citations(df_pap_,df_ref_,df_pap_ref_):\n",
    "\n",
    "    list_ref_ut_ = list(df_ref_[\\\n",
    "                              (df_ref_['cite_count']>0)\\\n",
    "                             ].index.values)\n",
    "    df_ref_sel_ = df_ref_[df_ref_.index.isin(list_ref_ut_)]\n",
    "    df_pap_ref_sel_ = df_pap_ref_[df_pap_ref_['reference_UT'].isin(list_ref_ut_)]\n",
    "    ## identify the remaining pap-uts and cut the df_pap dataframe\n",
    "    list_pap_ut_ = df_pap_ref_sel_['paper_UT'].drop_duplicates().tolist()\n",
    "    df_pap_sel_ = df_pap_[df_pap_.index.isin(list_pap_ut_)]\n",
    "\n",
    "    ## in case we want to get rid again of the 'unique_occurrences'-column\n",
    "    # df_ref_sel_.drop('occurrences_unique', axis=1, inplace=True)\n",
    "    return df_pap_sel_,df_ref_sel_,df_pap_ref_sel_\n",
    "\n",
    "def filter_df_plos(df_pap_,df_ref_,df_pap_ref_, y1_,y2_, list_pap_art_type_, list_pap_field_ = None):\n",
    "    '''filter our three dataframes\n",
    "        - publication-year-interval >=y1_ <= y2_\n",
    "        - article-type: list_pap_art_type_\n",
    "\n",
    "    TODO:\n",
    "        - scientific field \n",
    "        - journal (plosone,plosbio, ....)\n",
    "    '''\n",
    "    ## identify paper-uts that satisfy the constraints\n",
    "    if list_pap_field_ == None: ## no contraint no field\n",
    "        list_pap_ut_ = list(df_pap_[\\\n",
    "                              (df_pap_['plos_pub_year']>=y1_)&(df_pap_['plos_pub_year']<=y2_)\\\n",
    "                                &(df_pap_['plos_article_type'].isin(list_pap_art_type_))\\\n",
    "                             ].index.values)\n",
    "    else: ## constraint on field\n",
    "        ## create list-field as string\n",
    "        x_tmp = df_pap_['plos_field'].values\n",
    "        y_tmp = [str(h) for h in x_tmp]\n",
    "        df_pap_.loc[:,'plos_field_str'] = y_tmp\n",
    "        field_sel_str_ = '|'.join(list_pap_field_)\n",
    "        list_pap_ut_ = list(df_pap_[\\\n",
    "                      (df_pap_['plos_pub_year']>=y1_)&(df_pap_['plos_pub_year']<=y2_)\\\n",
    "                        &(df_pap_['plos_article_type'].isin(list_pap_art_type_))\\\n",
    "                        &(df_pap_['plos_field_str'].str.contains(field_sel_str_))\\\n",
    "                     ].index.values)\n",
    "\n",
    "    ## cut the df_pap dataframe\n",
    "    df_pap_sel_ = df_pap_[df_pap_.index.isin(list_pap_ut_)]\n",
    "    try:\n",
    "        df_pap_sel_.drop('plos_field_str', axis=1, inplace=True)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "    ## cut the df_pap_ref dataframe\n",
    "    df_pap_ref_sel_ = df_pap_ref_[df_pap_ref_['paper_UT'].isin(list_pap_ut_)]\n",
    "\n",
    "    ## identify the remaining ref-uts and cut the df_ref dataframe\n",
    "    list_ref_ut_ = df_pap_ref_sel_['reference_UT'].drop_duplicates().tolist()\n",
    "    df_ref_sel_ = df_ref_[df_ref_.index.isin(list_ref_ut_)]\n",
    "    return df_pap_sel_,df_ref_sel_,df_pap_ref_sel_\n",
    "\n",
    "def filter_df_ref(df_pap_,df_ref_,df_pap_ref_, n_occ_min_,n_occ_max_, y1_,y2_, list_ref_art_type_, list_ref_field_ = None):\n",
    "    '''filter our three dataframes\n",
    "        - number of unique occurrences in plos-paers: n_thresh\n",
    "        - publication-year-interval >=y1_ <= y2_\n",
    "        - article-type: list_pap_art_type_\n",
    "\n",
    "    TODO:\n",
    "        - scientific field \n",
    "    '''\n",
    "    ## create a new column in df_ref with unique number of occurrences\n",
    "    df_tmp = df_pap_ref_[['paper_UT', 'reference_UT']].drop_duplicates() # remove multi-occurence of the same ref in the same paper\n",
    "    x_ = df_tmp.groupby(['reference_UT'])['reference_UT'].value_counts()\n",
    "    x_ut_ = [h[0] for h in x_.index.tolist()]\n",
    "    x_series_tmp = pd.Series(data=x_.values,index=x_ut_)\n",
    "    df_ref_.loc[:,'occurrences_unique'] = x_series_tmp\n",
    "\n",
    "    ## filter ref-uts and cut the df_ref dataframe\n",
    "    if list_ref_field_ == None: ## no constraint on field\n",
    "\n",
    "        list_ref_ut_ = list(df_ref_[\\\n",
    "                                  (df_ref_['occurrences_unique']>=n_occ_min_)&(df_ref_['occurrences_unique']<=n_occ_max_)\\\n",
    "                                  &(df_ref_['ref_pub_year']>=y1_)&(df_ref_['ref_pub_year']<=y2_)\\\n",
    "                                  &(df_ref_['ref_article_type'].isin(list_ref_art_type_))\\\n",
    "                                 ].index.values)\n",
    "\n",
    "    else: ## constraint on field\n",
    "        ## create list-field as string\n",
    "        x_tmp = df_ref_['ref_field'].values\n",
    "        y_tmp = [str(h) for h in x_tmp]\n",
    "        df_ref_.loc[:,'ref_field_str'] = y_tmp\n",
    "        field_sel_str_ = '|'.join(list_ref_field_)\n",
    "        list_ref_ut_ = list(df_ref_[\\\n",
    "                          (df_ref_['occurrences_unique']>=n_occ_min_)&(df_ref_['occurrences_unique']<=n_occ_max_)\\\n",
    "                          &(df_ref_['ref_pub_year']>=y1_)&(df_ref_['ref_pub_year']<=y2_)\\\n",
    "                          &(df_ref_['ref_article_type'].isin(list_ref_art_type_))\\\n",
    "                          &(df_ref_['ref_field_str'].str.contains(field_sel_str_))\\\n",
    "                         ].index.values)\n",
    "\n",
    "    df_ref_sel_ = df_ref_[df_ref_.index.isin(list_ref_ut_)]\n",
    "\n",
    "    ## cut the df_pap dataframe\n",
    "    try:\n",
    "        df_ref_sel_.drop('ref_field_str', axis=1, inplace=True)\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    ## cut the df_pap_ref dataframe\n",
    "    df_pap_ref_sel_ = df_pap_ref_[df_pap_ref_['reference_UT'].isin(list_ref_ut_)]\n",
    "\n",
    "    ## identify the remaining pap-uts and cut the df_pap dataframe\n",
    "    list_pap_ut_ = df_pap_ref_sel_['paper_UT'].drop_duplicates().tolist()\n",
    "    df_pap_sel_ = df_pap_[df_pap_.index.isin(list_pap_ut_)]\n",
    "\n",
    "    ## in case we want to get rid again of the 'unique_occurrences'-column\n",
    "    # df_ref_sel_.drop('occurrences_unique', axis=1, inplace=True)\n",
    "    return df_pap_sel_,df_ref_sel_,df_pap_ref_sel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_df_min = pickle.load(open('../data/ref_dataframe_min.pkl', 'rb'))\n",
    "plos_df = pickle.load(open('../data/plos_paper_dataframe.pkl', 'rb'))\n",
    "cite_df = pickle.load(open('../data/citation_dataframe.pkl', 'rb'))\n",
    "#ref_df = pd.concat([ref_df_p1, ref_df_p2])\n",
    "#suppl_dict= pickle.load(open('../suppl_dict.txt', 'rb'))\n",
    "print (\"done loading pickles\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "result = ref_df_min.join(cite_df, on='reference_UT')\n",
    "ref_df = result.join(plos_df, on='paper_UT')\n",
    "print (\"done joining dfs\")\n",
    "\n",
    "\n",
    "# Remove null reference columns (8907763 rows vs 10848620 rows)\n",
    "ref_df = ref_df.loc[(ref_df['reference_UT']!='-1')]\n",
    "print (\"done dropping observations\")\n",
    "\n",
    "\n",
    "del ref_df_min\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###############################\n",
    "###### filtering plos-articles\n",
    "\n",
    "pap_pub_year_min = 2000\n",
    "pap_pub_year_max = 2016\n",
    "pap_art_type = ['@ Article','L Letter']   # ['@ Article','L Letter','K Article','N Note']  # we dont include the Review papers for now (it can be argued their behavior is different)\n",
    "\n",
    "\n",
    "## the most common  types:\n",
    "# @ Article               156824\n",
    "# R Review                  1469\n",
    "# E Editorial Material       746\n",
    "# C Correction                10\n",
    "# I Biographical-Item          4\n",
    "# L Letter                     4\n",
    "\n",
    "list_pap_field = None ## if you want to keep all fields\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "###### filtering reference-article\n",
    "\n",
    "n_thresh = 10   # min and max number of occurences of any given ut_ref paper on plos papers\n",
    "n_max = 100000\n",
    "\n",
    "ref_pub_year_min = 1900\n",
    "ref_pub_year_max = 2000\n",
    "list_ref_art_type = ['@ Article','L Letter','N Note']  # ['@ Article','L Letter','K Article','N Note']    # we dont include the Review papers for now (it can be argued their behavior is different)\n",
    "\n",
    "### the most common types:\n",
    "# @ Article                       2268122\n",
    "# R Review                         260165\n",
    "# E Editorial Material              36944\n",
    "# N Note                            18231\n",
    "# L Letter                          17953\n",
    "# M Meeting Abstract                 3376\n",
    "# 5 News Item                        1967\n",
    "# C Correction                        116\n",
    "\n",
    "\n",
    "list_ref_field = None ## if you want to keep all fields    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_pap, df_ref, df_pap_ref = filter_df_plos(plos_df, cite_df, ref_df,\\\n",
    "                                            pap_pub_year_min, pap_pub_year_max,\\\n",
    "                                            pap_art_type,\\\n",
    "                                           list_pap_field_ = list_pap_field\\\n",
    "                                           ) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df_pap, df_ref, df_pap_ref = filter_df_ref(df_pap, df_ref, df_pap_ref,\\\n",
    "                                           n_thresh, n_max,\\\n",
    "                                           ref_pub_year_min, ref_pub_year_max,\\\n",
    "                                           list_ref_art_type,\\\n",
    "                                          list_ref_field_ = list_ref_field\\\n",
    "                                          )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"tot. numb. ref. occurences:\",ref_df.shape, \"   # unique ref UTs\", len(ref_df['reference_UT'].unique() ), \"   mean # citations:\",ref_df['cite_count'].mean() )\n",
    "print(\"tot. numb. ref. occurences:\",df_pap_ref.shape, \"   # unique ref UTs\", len(df_pap_ref['reference_UT'].unique() ), \"   # unique plos UTs\", len(df_pap_ref['paper_UT'].unique() ), \"   mean # citations:\",df_pap_ref['cite_count'].mean() )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lista_ref_uts=list(df_pap_ref.reference_UT.unique())\n",
    "lista_plos_uts=list(df_pap_ref.paper_UT.unique())\n",
    "\n",
    "with open('../data/lista_selected_ref_uts.pkl', 'wb') as handle:\n",
    "     pickle.dump(lista_ref_uts, handle, protocol = 2)\n",
    "        \n",
    "with open('../data/lista_selected_plos_uts.pkl', 'wb') as handle:\n",
    "     pickle.dump(lista_plos_uts, handle, protocol = 2)\n",
    "\n",
    "\n",
    "del df_pap\n",
    "del df_ref\n",
    "del plos_df\n",
    "del cite_df\n",
    "del ref_df\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# OR  (for selected UTs and selected plos)\n",
    "try:\n",
    "    lista_ref_uts = pickle.load(open('../data/lista_selected_ref_uts.pkl', 'rb'))\n",
    "    print (\"read lista_ref_uts\", len(lista_ref_uts))\n",
    "except:\n",
    "     print (\"lista_ref_uts not found\")\n",
    "\n",
    "    \n",
    "try:\n",
    "    lista_plos_uts = pickle.load(open('../data/lista_selected_plos_uts.pkl', 'rb'))\n",
    "    print (\"read lista_plos_uts\", len(lista_plos_uts))\n",
    "except:\n",
    "    print (\"list_plos_uts not found\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read lista_ref_uts 2607457\n",
      "read lista_plos_uts 158813\n"
     ]
    }
   ],
   "source": [
    "#################  OR (for ALL trajectories for all UTs)\n",
    "try:\n",
    "    lista_all_reference_UTs = pickle.load(open('../data/lista_all_reference_UTs.pkl', 'rb'))\n",
    "    print (\"read lista_ref_uts\", len(lista_all_reference_UTs))\n",
    "except:\n",
    "     print (\"lista_ref_uts not found\")\n",
    "\n",
    "    \n",
    "try:\n",
    "    lista_all_plos_UTs = pickle.load(open('../data/lista_all_plos_UTs.pkl', 'rb'))\n",
    "    print (\"read lista_plos_uts\", len(lista_all_plos_UTs))\n",
    "except:\n",
    "    print (\"list_plos_uts not found\")\n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_merged = pickle.load(open('../data/df_reference_cite_plos_merged_simplified_cols.pkl', 'rb'))\n",
    "# print (\"done loading pickle\", df_merged.shape)\n",
    "\n",
    "\n",
    "\n",
    "# # df_merged.columns   # ['occurence', 'paper_UT', 'reference_UT', 'reference_rank',  'regex_sect_index', 'cite_count', 'ref_pub_year', 'paper_cite_count',  'total_refs', 'plos_pub_year']\n",
    "\n",
    "# lista_all_reference_UTs=list(df_merged.reference_UT.unique())\n",
    "# with open('../data/lista_all_reference_UTs.pkl', 'wb') as handle:\n",
    "#      pickle.dump(lista_all_reference_UTs, handle, protocol = 2)\n",
    "\n",
    "# len(lista_all_reference_UTs)\n",
    "\n",
    "\n",
    "\n",
    "# lista_all_plos_UTs=list(df_merged.paper_UT.unique())\n",
    "# with open('../data/lista_all_plos_UTs.pkl', 'wb') as handle:\n",
    "#      pickle.dump(lista_all_plos_UTs, handle, protocol = 2)\n",
    "\n",
    "# len(lista_all_plos_UTs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def retrieve_trajectory(ut, plos_uts):\n",
    "#     # Retrieve a citation trajectory for a single reference UT. This version collects -all- WoS citations for a given UT,\n",
    "#     # PLOS or not, so this one takes a lot more time, but shows the underlaying shape of the citation profile.\n",
    "#     # Requires inputting the list of all plos UTs ('paper_UT' column in the plos dataframe)\n",
    "#     count = 0\n",
    "#     cite_dates = {}\n",
    "    \n",
    "#     doc = papers_con.collection.find_one({\"UT\":ut})\n",
    "#     if 'citations' in doc:\n",
    "#         for ref_ut in doc['citations']:\n",
    "#             count+=1\n",
    "#             ref = papers_con.collection.find_one({\"UT\":ref_ut})\n",
    "#             year = float('nan')\n",
    "#             date = float('nan')\n",
    "#             plos_flag = False\n",
    "#             if ref:\n",
    "#                 if 'issue' in ref:\n",
    "#                     if 'PY' in ref['issue']:\n",
    "#                         year = ref['issue']['PY']\n",
    "#                     if 'PD' in ref['issue']:\n",
    "#                         date = ref['issue']['PD']\n",
    "#                     if ref_ut in plos_uts:\n",
    "#                         plos_flag = True\n",
    "                    \n",
    "# #             if count%1000==0:\n",
    "# #                 print(count)   \n",
    "#             cite_dates[ref_ut] = [date, year, plos_flag]\n",
    "\n",
    "#     return cite_dates\n",
    "\n",
    "\n",
    "\n",
    "# ###############################3\n",
    "# def retrieve_trajectory_new_old(ut, plos_uts, list_citing_papers, dict_citing_ut_year):\n",
    "    \n",
    "#      # Retrieve a citation trajectory for a single reference UT. This version collects -all- WoS citations for a given UT,\n",
    "#     # PLOS or not, so this one takes a lot more time, but shows the underlaying shape of the citation profile.\n",
    "#     # Requires inputting the list of all plos UTs ('paper_UT' column in the plos dataframe) to flag those papers that are Plos\n",
    "#     #count = 0\n",
    "#     cite_dates = {}\n",
    "    \n",
    "#     doc = papers_con.collection.find_one({\"UT\":ut})  # i look at a given ref_ut paper\n",
    "#     if 'citations' in doc:\n",
    "#         for ref_ut in doc['citations']:   # i iterate over all papers that have CITED this ref_ut paper\n",
    "            \n",
    "#             list_citing_papers.append(ref_ut)\n",
    "            \n",
    "#             try:\n",
    "#                 dict_citing_ut_year[ref_ut]  # if i have encounter this citing paper before, i dont need to go query it again for its publication year\n",
    "#                 cite_dates[ref_ut]=dict_citing_ut_year[ref_ut]\n",
    "                \n",
    "#             except KeyError:\n",
    "                                    \n",
    "#                 #count+=1\n",
    "#                 ref = papers_con.collection.find_one({\"UT\":ref_ut})\n",
    "#                 year = float('nan')\n",
    "#                 date = float('nan')\n",
    "#                 plos_flag = False\n",
    "#                 if ref:\n",
    "#                     if 'issue' in ref:\n",
    "#                         if 'PY' in ref['issue']:\n",
    "#                             year = ref['issue']['PY']\n",
    "#                         if 'PD' in ref['issue']:\n",
    "#                             date = ref['issue']['PD']\n",
    "#                         if ref_ut in plos_uts:\n",
    "#                             plos_flag = True\n",
    "\n",
    "#                 cite_dates[ref_ut] = [date, year, plos_flag]\n",
    "#                 dict_citing_ut_year[ref_ut] = [date, year, plos_flag]\n",
    "            \n",
    "            \n",
    "#     return cite_dates\n",
    "    \n",
    "\n",
    "###########################\n",
    "\n",
    "\n",
    "def retrieve_trajectory_new(ref_ut, plos_uts, list_citing_papers, dict_citing_ut_year):\n",
    "    \n",
    "    # Retrieve a citation trajectory for a single reference UT. This version collects -all- WoS citations for a given UT,\n",
    "    # PLOS or not, so this one takes a lot more time, but shows the underlaying shape of the citation profile.\n",
    "    # Requires inputting the list of all plos UTs ('paper_UT' column in the plos dataframe) to flag those papers that are Plos\n",
    "    #count = 0\n",
    "    \n",
    "    \n",
    "    cite_dates = {}\n",
    "    \n",
    "    doc = papers_con.collection.find_one({\"UT\":ref_ut})  # i look at a given ref_ut paper\n",
    "    if 'citations' in doc:  # if the paper ut has recieved any citations \n",
    "        \n",
    "        list_citations= doc['citations'] \n",
    "        print (len(list_citations))\n",
    "            \n",
    "        list_citing_papers += list_citations#_all\n",
    "           \n",
    "        query = papers_con.collection.find({\"UT\":{\"$in\":list_citations}},{\"UT\":1,\"issue.PY\":1,\"issue.PD\":1}, no_cursor_timeout=True)  # it returns an iterator (it gets empty after iterating over it once) # second {} to select the fields it returns, if i dont want them all\n",
    "               \n",
    "            \n",
    "        for item in query:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict\n",
    "\n",
    "            UT=item[\"UT\"]\n",
    "            \n",
    "            \n",
    "            try:\n",
    "                                                \n",
    "                cite_dates[UT] = dict_citing_ut_year[UT]\n",
    "                \n",
    "            except KeyError:  # if i haven't encountered this citing paper before\n",
    "                     \n",
    "                year = float('nan')\n",
    "                date = float('nan')\n",
    "                plos_flag = False\n",
    "            \n",
    "                issue=item['issue']\n",
    "                year=issue['PY']\n",
    "                try:\n",
    "                    date=issue['PD']\n",
    "                except KeyError: # if there is no PD field\n",
    "                    pass               \n",
    "\n",
    "\n",
    "                if UT in plos_uts:\n",
    "                    plos_flag = True\n",
    "\n",
    "\n",
    "                cite_dates[UT] = [date, year, plos_flag]\n",
    "                dict_citing_ut_year[UT] = [date, year, plos_flag]\n",
    "\n",
    "        query.close()  # because i am using the no_cursor_timeout=True, i need also this, or cursor keeps waiting so ur resources are used up\n",
    "        \n",
    "            \n",
    "    return cite_dates\n",
    "    \n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "############################3\n",
    "\n",
    "\n",
    "# def retrieve_multiple_trajectories(ut_list, plos_uts):\n",
    "#     # Retrieve multiple trajectories for all reference UTs in ut_list. Returns a nested dictionary\n",
    "#     # {Reference_UT_1: {Citation_UT_1: [Month, Year], ...}}\n",
    "#     trajectories = {}\n",
    "#     for ut in ut_list:\n",
    "#         print(ut)\n",
    "#         #cite_dates = retrieve_trajectory_new(ut, plos_uts)  # with some changes i added\n",
    "#         cite_dates = retrieve_trajectory(ut, plos_uts)\n",
    "#         trajectories[ut] = cite_dates\n",
    "#     return trajectories\n",
    "\n",
    "\n",
    "\n",
    "#######################\n",
    "\n",
    "# def retrieve_multiple_trajectories_new(ut_list, plos_uts,dict_ref_UT_dict_traj):\n",
    "    \n",
    "#     for ut in ut_list:\n",
    "#         print(ut)\n",
    "#         #cite_dates = retrieve_trajectory_new(ut, plos_uts)  # with some changes i added\n",
    "#         cite_dates = retrieve_trajectory(ut, plos_uts)\n",
    "#         dict_ref_UT_dict_traj[ut] = cite_dates\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#########################\n",
    "\n",
    "\n",
    "# def retrieve_trajectory_plos_only(ut, plos_uts):\n",
    "#     # Retrieve a citation trajectory for a single reference UT. This version collects only PLOS citations, so it's much faster\n",
    "#     # Requires inputting the list of all plos UTs ('paper_UT' column in the plos dataframe)\n",
    "\n",
    "#     count = 0\n",
    "#     cite_dates = {}\n",
    "#     if ut in plos_uts:\n",
    "#         doc = papers_con.collection.find_one({\"UT\":ut})\n",
    "#         if 'citations' in doc:\n",
    "#             for ref_ut in doc['citations']:\n",
    "#                 count+=1\n",
    "#                 ref = papers_con.collection.find_one({\"UT\":ref_ut})\n",
    "#                 year = float('nan')\n",
    "#                 date = float('nan')\n",
    "#                 plos_flag = False\n",
    "#                 if ref:\n",
    "#                     if 'issue' in ref:\n",
    "#                         if 'PY' in ref['issue']:\n",
    "#                             year = ref['issue']['PY']\n",
    "#                         if 'PD' in ref['issue']:\n",
    "#                             date = ref['issue']['PD']\n",
    "#                         if ref_ut in plos_uts:\n",
    "#                             plos_flag = True\n",
    "#                 if count%1000==0:\n",
    "#                     print(count)   \n",
    "#                 cite_dates[ref_ut] = [date, year, plos_flag]\n",
    "\n",
    "#     return cite_dates\n",
    "\n",
    "# #######################\n",
    "\n",
    "# def retrieve_multiple_trajectories_plos_only(ut_list, plos_uts):\n",
    "#     # Retrieve multiple trajectories for all reference UTs in ut_list. Returns a nested dictionary\n",
    "#     # {Reference_UT_1: {Citation_UT_1: [Month, Year], ...}}\n",
    "\n",
    "#     trajectories = {}\n",
    "#     for ut in ut_list:\n",
    "#         print(ut)\n",
    "#         cite_dates = retrieve_trajectory_plos_only(ut, plos_uts)\n",
    "#         trajectories[ut] = cite_dates\n",
    "#     return trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #rm ../data/test_trajectories.pkl\n",
    "\n",
    "\n",
    "# try:\n",
    "#     master_dict_ref_UT_dict_traj = pickle.load(open('../data/test_trajectories.pkl', 'rb'))\n",
    "#     print (\"read master dict\")\n",
    "# except:\n",
    "#     master_dict_ref_UT_dict_traj={}\n",
    "\n",
    "    \n",
    "# try:\n",
    "#     list_citing_papers = pickle.load(open('../data/list_citing_papers.pkl', 'rb'))\n",
    "#     print (\"read list citing papers\")\n",
    "# except:\n",
    "#     list_citing_papers=[]\n",
    "    \n",
    "    \n",
    "# try:\n",
    "#     dict_citing_ut_year = pickle.load(open('../data/dict_citing_ut_year.pkl', 'rb'))\n",
    "#     print (\"read dict citing ut-year\")\n",
    "# except:\n",
    "#     dict_citing_ut_year={}\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for ref_ut in lista_ref_uts:\n",
    "#     try:\n",
    "#         master_dict_ref_UT_dict_traj[ref_ut]\n",
    "#         print (ref_ut, \"already in dict. of trajectories\")\n",
    "#     except KeyError:\n",
    "   \n",
    "#         cite_dates = retrieve_trajectory_new(ref_ut, lista_plos_uts, list_citing_papers,dict_citing_ut_year)\n",
    "#         master_dict_ref_UT_dict_traj[ref_ut] = cite_dates\n",
    "\n",
    "\n",
    "#         ### dump in a pickle after i process every ref_ut\n",
    "#         with open('../data/test_trajectories.pkl', 'wb') as handle:\n",
    "#             pickle.dump(master_dict_ref_UT_dict_traj, handle, protocol = 2)   \n",
    "#             print (ref_ut, len(master_dict_ref_UT_dict_traj), len(dict_citing_ut_year), len(list_citing_papers))\n",
    "\n",
    "\n",
    "#         with open('../data/list_citing_papers.pkl', 'wb') as handle:\n",
    "#              pickle.dump(list_citing_papers, handle, protocol = 2)\n",
    "\n",
    "#         with open('../data/dict_citing_ut_year.pkl', 'wb') as handle:\n",
    "#              pickle.dump(dict_citing_ut_year, handle, protocol = 2)\n",
    "\n",
    "        \n",
    "        \n",
    "# print (\"done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " ################################ new, faster way:\n",
    "    \n",
    "\n",
    "def get_all_citing_UTs_at_once_and_dict_ref_UT_citing_UTs(lista_ref_uts, list_all_citing_UTs, dict_ref_UT_list_its_citing_papers):\n",
    "                                                          \n",
    "    \n",
    "    cursor = papers_con.collection.find({\"UT\":{\"$in\":lista_ref_uts}},{\"UT\":1,'citations':1}, no_cursor_timeout=True)  # it returns an iterator (it gets empty after iterating over it once) # second {} to select the fields it returns, if i dont want them all\n",
    "   # cont =0\n",
    "    tot=len(lista_ref_uts)\n",
    "    for item in cursor:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict\n",
    "\n",
    "        UT=item[\"UT\"]               \n",
    "    #    cont +=1\n",
    "        \n",
    "        list_citing_papers=item['citations']\n",
    "        \n",
    " \n",
    "        dict_ref_UT_list_its_citing_papers[UT]=list_citing_papers\n",
    "        list_all_citing_UTs +=list_citing_papers\n",
    "    \n",
    "    cursor.close()  # because i am using the no_cursor_timeout=True, i need also this, or cursor keeps waiting so ur resources are used up\n",
    "    \n",
    "    \n",
    "    return list(set(list_all_citing_UTs)), dict_ref_UT_list_its_citing_papers \n",
    "\n",
    "#####################################\n",
    "\n",
    "  \n",
    "    \n",
    "\n",
    "def  get_publ_years_for_all_citing_UTs(list_citing_UTs,lista_plos_uts,dict_citing_UT_publ_year) :\n",
    "       # active_users = db.users.find({active:True}).batch_size(2000)\n",
    "       \n",
    "    print (len(dict_citing_UT_publ_year), len(list_citing_UTs))\n",
    "        \n",
    "    cursor = papers_con.collection.find({\"UT\":{\"$in\":list_citing_UTs}},{\"UT\":1,\"issue.PY\":1,\"issue.PD\":1}, no_cursor_timeout=True).batch_size(2000)  # it returns an iterator (it gets empty after iterating over it once) # second {} to select the fields it returns, if i dont want them all   \n",
    "    #query = papers_con.collection.find({\"UT\":{\"$in\":list_citing_UTs}},{\"UT\":1,\"issue.PY\":1,\"issue.PD\":1})\n",
    "    cont=0\n",
    "    tot=len(list_citing_UTs)   \n",
    "   \n",
    "    for item in cursor:  # query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict\n",
    "\n",
    "       \n",
    "        UT=item[\"UT\"]\n",
    "        print (cont, tot)                  \n",
    "        cont +=1\n",
    "                                           \n",
    "        year = float('nan')\n",
    "        date = float('nan')\n",
    "        plos_flag = False\n",
    "            \n",
    "            \n",
    "        issue=item['issue']\n",
    "       \n",
    "        year=issue['PY']\n",
    "        try:\n",
    "            date=issue['PD']\n",
    "        except KeyError: # if there is no PD field\n",
    "            pass               \n",
    "\n",
    "\n",
    "        if UT in lista_plos_uts:\n",
    "            plos_flag = True\n",
    "\n",
    "\n",
    "        dict_citing_UT_publ_year[UT] = [date, year, plos_flag]\n",
    "      \n",
    "    \n",
    "    \n",
    "    cursor.close()  # because i am using the no_cursor_timeout=True, i need also this, or cursor keeps waiting so ur resources are used up\n",
    "    print (len(dict_citing_UT_publ_year))\n",
    "    return dict_citing_UT_publ_year\n",
    "\n",
    " \n",
    "\n",
    "###########################\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ######  ORIGINAL query to get trajecteries of a SELECTION of ref UTs\n",
    "# ####################################3333\n",
    "\n",
    "\n",
    "\n",
    "# print (\"length first list:\", len(lista_ref_uts))\n",
    "\n",
    "# try:   # this part is fast\n",
    "#     list_all_citing_UTs = pickle.load(open('../data/list_all_citing_UTs.pkl', 'rb'))\n",
    "#     print (\"read list_all_citing_UTs\", len(list_all_citing_UTs))    \n",
    "    \n",
    "#     dict_ref_UT_list_its_citing_papers = pickle.load(open('../data/dict_ref_UT_list_its_citing_papers.pkl', 'rb'))\n",
    "#     print (\"read dict_ref_UT_list_its_citing_papers\", len(dict_ref_UT_list_its_citing_papers))    \n",
    "    \n",
    "# except:     \n",
    "#     print (\"1st query: getting list all citing UTs that i will need.........\")\n",
    "\n",
    "#     list_all_citing_UTs, dict_ref_UT_list_its_citing_papers  = get_all_citing_UTs_at_once_and_dict_ref_UT_citing_UTs(lista_ref_uts)\n",
    "\n",
    "#     with open('../data/list_all_citing_UTs.pkl', 'wb') as handle:\n",
    "#         pickle.dump(list_all_citing_UTs, handle, protocol = 2)   \n",
    "#         print ( len(list_all_citing_UTs), '../data/list_all_citing_UTs.pkl')\n",
    "\n",
    "#     with open('../data/dict_ref_UT_list_its_citing_papers.pkl', 'wb') as handle:\n",
    "#         pickle.dump(dict_ref_UT_list_its_citing_papers, handle, protocol = 2)   \n",
    "#         print ( len(dict_ref_UT_list_its_citing_papers), '../data/dict_ref_UT_list_its_citing_papers.pkl')        \n",
    "    \n",
    "# print (\"done.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print (\"\\n\\nlength second list:\", len(list_all_citing_UTs))\n",
    "# print (\"2nd query: getting dict UT publication year ...................\")   # too large for Mongo to do all at once!\n",
    "\n",
    "\n",
    "# try:\n",
    "#     dict_citing_UT_publ_year = pickle.load(open('../data/partial_dict_citing_UT_publ_year_.pkl', 'rb'))\n",
    "#     print (\"read partial master dict, length:\", len(dict_citing_UT_publ_year))\n",
    "# except:\n",
    "#     dict_citing_UT_publ_year = {}\n",
    "\n",
    "    \n",
    "# stop= len(list_all_citing_UTs)\n",
    "# initial=len(dict_citing_UT_publ_year)\n",
    "# delta=int(stop/1000.)\n",
    "# final=initial + delta\n",
    "# while final-delta <=  stop:    \n",
    "#     print (\" \",initial, final, \"....\")\n",
    "\n",
    "#     partial_list=list_all_citing_UTs[initial:final]  # slicing beyond the length of the list is NOT a problem\n",
    "\n",
    "#     get_publ_years_for_all_citing_UTs(partial_list,lista_plos_uts,dict_citing_UT_publ_year)    #where  dict_citing_UT_publ_year[UT] = [date, year, plos_flag]\n",
    "\n",
    "#     initial += delta\n",
    "#     final = initial +delta\n",
    "\n",
    "#     with open('../data/partial_dict_citing_UT_publ_year.pkl', 'wb') as handle:\n",
    "#         pickle.dump(dict_citing_UT_publ_year, handle, protocol = 2)   \n",
    "#         print (\"# papers in master dict:\", len(dict_citing_UT_publ_year), '../data/partial_dict_citing_UT_publ_year.pkl')\n",
    "\n",
    "    \n",
    "# #     print (\"   done\")\n",
    "# print(\"done.\")\n",
    "\n",
    "\n",
    "\n",
    "# print (\"putting together master dict ref_UT trajectories.............\")\n",
    "# list_missing =[]\n",
    "# master_dict_ref_UT_dict_traj={}\n",
    "# for ref_UT in dict_ref_UT_list_its_citing_papers:  # the 9K ref_UT papers i focus on\n",
    "#     cite_dates={}\n",
    "#     dict_aux = dict_ref_UT_list_its_citing_papers[ref_UT]\n",
    "#     print (ref_UT, len(dict_ref_UT_list_its_citing_papers[ref_UT]))\n",
    "#     for citing_UT in dict_aux:\n",
    "#         try:\n",
    "#             cite_dates[citing_UT]=dict_citing_UT_publ_year[citing_UT]\n",
    "#         except KeyError:\n",
    "#             list_missing.append(citing_UT)\n",
    "    \n",
    "#     master_dict_ref_UT_dict_traj[ref_UT]=cite_dates\n",
    "# print (\"missing UTs:\", len(set(list_missing)))\n",
    "# print (\"done.\")    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # final dict:   master_dict_ref_UT_dict_traj[ref_ut] = cite_dates   where is a dict: cite_dates[citing_UT] = [date, year, plos_flag]\n",
    "    \n",
    "    \n",
    "\n",
    "# ### dump in a pickle \n",
    "# with open('../data/new_master_dict_ref_UT_trajectories.pkl', 'wb') as handle:\n",
    "#     pickle.dump(master_dict_ref_UT_dict_traj, handle, protocol = 2)   \n",
    "#     print (\"# papers in master dict:\", len(master_dict_ref_UT_dict_traj), '../data/new_master_dict_ref_UT_trajectories.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lista_all_reference_UTs)\n",
    "\n",
    "\n",
    "len(lista_all_plos_UTs)\n",
    "                                                            \n",
    "dict_citing_UT_publ_year = pickle.load(open('../data/dict_citing_UT_publ_year_.pkl', 'rb'))\n",
    "print (\"read partial master dict, length:\", len(dict_citing_UT_publ_year))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st query: getting list all citing UTs that i will need.........\n",
      "  0 26074 ....\n",
      "26074 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "8376382 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  26074 52148 ....\n",
      "52148 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "14572178 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  52148 78222 ....\n",
      "78222 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "20044755 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  78222 104296 ....\n",
      "104296 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "24783508 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  104296 130370 ....\n",
      "130370 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "29635101 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  130370 156444 ....\n",
      "156444 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "33793291 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  156444 182518 ....\n",
      "182518 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "37980007 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  182518 208592 ....\n",
      "208592 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "41779812 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  208592 234666 ....\n",
      "234666 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "45466469 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  234666 260740 ....\n",
      "260739 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "48954335 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  260740 286814 ....\n",
      "286813 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "52331229 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  286814 312888 ....\n",
      "312887 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "55542364 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  312888 338962 ....\n",
      "338960 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "58729807 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  338962 365036 ....\n",
      "365034 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "61878757 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  365036 391110 ....\n",
      "391108 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "64907940 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  391110 417184 ....\n",
      "417182 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "67853583 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  417184 443258 ....\n",
      "443256 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "70799313 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  443258 469332 ....\n",
      "469330 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "73709079 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  469332 495406 ....\n",
      "495404 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "76431152 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  495406 521480 ....\n",
      "521478 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "79189951 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  521480 547554 ....\n",
      "547552 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "81831290 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  547554 573628 ....\n",
      "573626 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "84482576 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  573628 599702 ....\n",
      "599700 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "86933775 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  599702 625776 ....\n",
      "625774 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "89462870 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  625776 651850 ....\n",
      "651848 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "91996598 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  651850 677924 ....\n",
      "677922 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "94510667 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  677924 703998 ....\n",
      "703996 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "97051882 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  703998 730072 ....\n",
      "730070 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "99474188 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  730072 756146 ....\n",
      "756144 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "101816794 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  756146 782220 ....\n",
      "782218 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "104133255 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  782220 808294 ....\n",
      "808292 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "106476122 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  808294 834368 ....\n",
      "834366 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "108695373 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  834368 860442 ....\n",
      "860440 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "110871545 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  860442 886516 ....\n",
      "886513 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "113068766 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  886516 912590 ....\n",
      "912587 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "115252182 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  912590 938664 ....\n",
      "938661 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "117409713 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  938664 964738 ....\n",
      "964735 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "119563997 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  964738 990812 ....\n",
      "990809 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "121686049 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  990812 1016886 ....\n",
      "1016883 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "123713643 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  1016886 1042960 ....\n",
      "1042957 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "125775033 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  1042960 1069034 ....\n",
      "1069031 ../data/dict_ref_UT_list_its_citing_papers.pkl\n",
      "127823396 ../data/list_all_citing_UTs_of_ref_papers.pkl\n",
      "  1069034 1095108 ....\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-550cdbf4f6d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpartial_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlista_all_reference_UTs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minitial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# slicing beyond the length of the list is NOT a problem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mget_all_citing_UTs_at_once_and_dict_ref_UT_citing_UTs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_all_citing_UTs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_ref_UT_list_its_citing_papers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/dict_ref_UT_list_its_citing_papers.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-ce8853320760>\u001b[0m in \u001b[0;36mget_all_citing_UTs_at_once_and_dict_ref_UT_citing_UTs\u001b[0;34m(lista_ref_uts, list_all_citing_UTs, dict_ref_UT_list_its_citing_papers)\u001b[0m\n\u001b[1;32m      9\u001b[0m    \u001b[0;31m# cont =0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mtot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlista_ref_uts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# query (cursor) is an iterator (once i iterate over it once, it is empty), and every item is a dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mUT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"UT\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__data\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__manipulate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m                 \u001b[0m_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m_refresh\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m                                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__collection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                                         self.__max_await_time_ms)\n\u001b[0;32m-> 1110\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__send_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Cursor id is zero nothing else to return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36m__send_message\u001b[0;34m(self, operation)\u001b[0m\n\u001b[1;32m    922\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                 response = client._send_message_with_response(operation,\n\u001b[0;32m--> 924\u001b[0;31m                                                               **kwargs)\n\u001b[0m\u001b[1;32m    925\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__exhaust\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_send_message_with_response\u001b[0;34m(self, operation, read_preference, exhaust, address)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__all_credentials\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_listeners\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             exhaust)\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset_on_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_reset_on_error\u001b[0;34m(self, server, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \"\"\"\n\u001b[1;32m   1036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNetworkTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# The socket has been closed. Don't reset the server.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/server.py\u001b[0m in \u001b[0;36msend_message_with_response\u001b[0;34m(self, operation, set_slave_okay, all_credentials, listeners, exhaust)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpublish\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mreceive_message\u001b[0;34m(self, request_id)\u001b[0m\n\u001b[1;32m    527\u001b[0m                                    self.max_message_size)\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlegacy_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_doc_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_last_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36m_raise_connection_failure\u001b[0;34m(self, error)\u001b[0m\n\u001b[1;32m    647\u001b[0m             \u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/pool.py\u001b[0m in \u001b[0;36mreceive_message\u001b[0;34m(self, request_id)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m             return receive_message(self.sock, request_id,\n\u001b[0;32m--> 527\u001b[0;31m                                    self.max_message_size)\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_connection_failure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36mreceive_message\u001b[0;34m(sock, request_id, max_message_size)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;31m# Ignore the response's request id.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     length, _, response_to, op_code = _UNPACK_HEADER(\n\u001b[0;32m--> 147\u001b[0;31m         _receive_data_on_socket(sock, 16))\n\u001b[0m\u001b[1;32m    148\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0m_OpReply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOP_CODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         raise ProtocolError(\"Got opcode %r but expected \"\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/envs/psy/lib/python3.6/site-packages/pymongo/network.py\u001b[0m in \u001b[0;36m_receive_data_on_socket\u001b[0;34m(sock, length)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIOError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_errno_from_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEINTR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print (\"1st query: getting list all citing UTs that i will need.........\")\n",
    "\n",
    "\n",
    "try:  \n",
    "    list_all_citing_UTs = pickle.load(open('../data/list_all_citing_UTs_of_ref_papers.pkl', 'rb'))\n",
    "    print (\"read list_all_citing_UTs_of_ref_papers\", len(list_all_citing_UTs_of_ref_papers))    \n",
    "    \n",
    "    dict_ref_UT_list_its_citing_papers = pickle.load(open('../data/dict_ref_UT_list_its_citing_papers.pkl', 'rb'))\n",
    "    print (\"read dict_ref_UT_list_its_citing_papers\", len(dict_ref_UT_list_its_citing_papers))    \n",
    "except:\n",
    "    list_all_citing_UTs=[]\n",
    "    dict_ref_UT_list_its_citing_papers ={}\n",
    "    \n",
    "\n",
    "stop= len(lista_all_reference_UTs)\n",
    "initial=len(dict_ref_UT_list_its_citing_papers)\n",
    "delta=int(stop/100.)\n",
    "final=initial + delta\n",
    "while final-delta <=  stop:    \n",
    "    print (\" \",initial, final, \"....\")\n",
    "\n",
    "    partial_list=lista_all_reference_UTs[initial:final]  # slicing beyond the length of the list is NOT a problem\n",
    "\n",
    "    get_all_citing_UTs_at_once_and_dict_ref_UT_citing_UTs(partial_list, list_all_citing_UTs, dict_ref_UT_list_its_citing_papers)\n",
    "\n",
    "    with open('../data/dict_ref_UT_list_its_citing_papers.pkl', 'wb') as handle:\n",
    "        pickle.dump(dict_ref_UT_list_its_citing_papers, handle, protocol = 2)   \n",
    "        print ( len(dict_ref_UT_list_its_citing_papers), '../data/dict_ref_UT_list_its_citing_papers.pkl')        \n",
    "\n",
    "        \n",
    "        \n",
    "    with open('../data/list_all_citing_UTs_of_ref_papers.pkl', 'wb') as handle:\n",
    "        pickle.dump(list_all_citing_UTs, handle, protocol = 2)   \n",
    "        print ( len(list_all_citing_UTs), '../data/list_all_citing_UTs_of_ref_papers.pkl')\n",
    "\n",
    "\n",
    "    initial += delta\n",
    "    final = initial +delta\n",
    "    \n",
    "print (\"done.\")\n",
    "print (len(list_all_citing_UTs), len(lista_all_reference_UTs))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ############# for ALL trajectories for all UTs, i expand the queries for trajectories to ALL UT papers referenced in the plos corpus\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# try:  \n",
    "#     list_all_citing_UTs = pickle.load(open('../data/lista_all_reference_UTs.pkl', 'rb'))\n",
    "#     print (\"read list_all_citing_UTs\", len(lista_all_reference_UTs))    \n",
    "    \n",
    "#     dict_ref_UT_list_its_citing_papers = pickle.load(open('../data/dict_ref_UT_list_its_citing_papers_.pkl', 'rb'))\n",
    "#     print (\"read dict_ref_UT_list_its_citing_papers\", len(dict_ref_UT_list_its_citing_papers))    \n",
    "    \n",
    "# except:     \n",
    "print (\"1st query: getting list all citing UTs that i will need.........\")\n",
    "\n",
    "list_all_citing_UTs, dict_ref_UT_list_its_citing_papers  = get_all_citing_UTs_at_once_and_dict_ref_UT_citing_UTs(lista_all_reference_UTs)\n",
    "\n",
    "#     with open('../data/list_all_citing_UTs.pkl', 'wb') as handle:\n",
    "#         pickle.dump(list_all_citing_UTs, handle, protocol = 2)   \n",
    "#         print ( len(list_all_citing_UTs), '../data/list_all_citing_UTs_.pkl')\n",
    "\n",
    "with open('../data/dict_ref_UT_list_its_citing_papers.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_ref_UT_list_its_citing_papers, handle, protocol = 2)   \n",
    "    print ( len(dict_ref_UT_list_its_citing_papers), '../data/dict_ref_UT_list_its_citing_papers_.pkl')        \n",
    "\n",
    "    \n",
    "print (\"done.\")\n",
    "print (len(list_all_citing_UTs), len(lista_all_reference_UTs))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########### FALTA AGNADIR LA EXCLUSION DE UTS Q YA ESTAN BUSCAOS\n",
    "\n",
    "\n",
    "\n",
    "print (\"\\n\\nlength second list:\", len(lista_all_reference_UTs))\n",
    "print (\"2nd query: getting dict UT publication year ...................\")   # too large for Mongo to do all at once!\n",
    "\n",
    "\n",
    "try:\n",
    "    dict_citing_UT_publ_year = pickle.load(open('../data/partial_dict_citing_UT_publ_year_.pkl', 'rb'))\n",
    "    print (\"read partial master dict, length:\", len(dict_citing_UT_publ_year))\n",
    "except:\n",
    "    dict_citing_UT_publ_year = {}\n",
    "\n",
    "    \n",
    "stop= len(list_all_citing_UTs)\n",
    "initial=len(dict_citing_UT_publ_year)\n",
    "delta=int(stop/1000.)\n",
    "final=initial + delta\n",
    "while final-delta <=  stop:    \n",
    "    print (\" \",initial, final, \"....\")\n",
    "\n",
    "    partial_list=list_all_citing_UTs[initial:final]  # slicing beyond the length of the list is NOT a problem\n",
    "\n",
    "    get_publ_years_for_all_citing_UTs(partial_list,lista_plos_uts,dict_citing_UT_publ_year)    #where  dict_citing_UT_publ_year[UT] = [date, year, plos_flag]\n",
    "\n",
    "    initial += delta\n",
    "    final = initial +delta\n",
    "\n",
    "    with open('../data/partial_dict_citing_UT_publ_year.pkl', 'wb') as handle:\n",
    "        pickle.dump(dict_citing_UT_publ_year, handle, protocol = 2)   \n",
    "        print (\"# papers in master dict:\", len(dict_citing_UT_publ_year), '../data/partial_dict_citing_UT_publ_year.pkl')\n",
    "\n",
    "    \n",
    "#     print (\"   done\")\n",
    "print(\"done.\")\n",
    "\n",
    "\n",
    "\n",
    "print (\"putting together master dict ref_UT trajectories.............\")\n",
    "list_missing =[]\n",
    "master_dict_ref_UT_dict_traj={}\n",
    "for ref_UT in dict_ref_UT_list_its_citing_papers:  # the 9K ref_UT papers i focus on\n",
    "    cite_dates={}\n",
    "    dict_aux = dict_ref_UT_list_its_citing_papers[ref_UT]\n",
    "    print (ref_UT, len(dict_ref_UT_list_its_citing_papers[ref_UT]))\n",
    "    for citing_UT in dict_aux:\n",
    "        try:\n",
    "            cite_dates[citing_UT]=dict_citing_UT_publ_year[citing_UT]\n",
    "        except KeyError:\n",
    "            list_missing.append(citing_UT)\n",
    "    \n",
    "    master_dict_ref_UT_dict_traj[ref_UT]=cite_dates\n",
    "print (\"missing UTs:\", len(set(list_missing)))\n",
    "print (\"done.\")    \n",
    "    \n",
    "    \n",
    "    \n",
    "# final dict:   master_dict_ref_UT_dict_traj[ref_ut] = cite_dates   where is a dict: cite_dates[citing_UT] = [date, year, plos_flag]\n",
    "    \n",
    "    \n",
    "\n",
    "### dump in a pickle \n",
    "with open('../data/new_master_dict_ref_UT_trajectories.pkl', 'wb') as handle:\n",
    "    pickle.dump(master_dict_ref_UT_dict_traj, handle, protocol = 2)   \n",
    "    print (\"# papers in master dict:\", len(master_dict_ref_UT_dict_traj), '../data/new_master_dict_ref_UT_trajectories.pkl')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list_all_citing_UTs)  # list_all_citing_UTs: 6901130\n",
    "\n",
    "print (len(dict_citing_UT_publ_year)+ len(missing_citing_UTs) )  # dict_citing_UT_publ_year:  6898862   \n",
    "\n",
    "len(dict_citing_UT_publ_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_citing_UTs=[]   # it turns out, this UTs are NOT in the WoS db\n",
    "for item in list_all_citing_UTs:\n",
    "    if item not in dict_citing_UT_publ_year:\n",
    "        missing_citing_UTs.append(item)\n",
    "        \n",
    "print (\"missing UTs:\", len(missing_citing_UTs))\n",
    "\n",
    "\n",
    "print (len(dict_citing_UT_publ_year))\n",
    "\n",
    "# dict_citing_UT_publ_year=get_publ_years_for_all_citing_UTs(missing_citing_UTs,lista_plos_uts,dict_citing_UT_publ_year)    #where  dict_citing_UT_publ_year[UT] = [date, year, plos_flag]\n",
    "# print (len(dict_citing_UT_publ_year))\n",
    "\n",
    "# with open('../data/partial_dict_citing_UT_publ_year.pkl', 'wb') as handle:\n",
    "#     pickle.dump(dict_citing_UT_publ_year, handle, protocol = 2)   \n",
    "#     print (\"# papers in master dict:\", len(dict_citing_UT_publ_year), '../data/partial_dict_citing_UT_publ_year.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_a=set((dict_citing_UT_publ_year.keys()))\n",
    "list_b=set(list_all_citing_UTs)\n",
    "list_c=set(missing_citing_UTs)\n",
    "interseccion = list_c.intersection(list_a)\n",
    "len(interseccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_traject = datetime_filler_traj(master_dict_ref_UT_dict_traj)\n",
    "print (\"done converting datetimes\")\n",
    "\n",
    "with open('../data/test_trajectories_datetimes.pkl', 'wb') as handle:\n",
    "    pickle.dump(dict_traject, handle, protocol = 2)\n",
    "print (\"written: ../data/test_trajectories_datetimes.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_traject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i create a simpler dictionary (because sorting it by value when the value is a tuple (date, flag) is problematic)\n",
    "master_dict_traject={}\n",
    "for llave1 in dict_traject:\n",
    "    master_dict_traject[llave1]={}\n",
    "   # print( dict_traject[llave1])\n",
    "    for llave2 in dict_traject[llave1]:        \n",
    "        master_dict_traject[llave1][llave2]=dict_traject[llave1][llave2][0]\n",
    "    #print (llave1, len(master_dict_traject[llave1]))\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "### i create a new dict (ut as keys) of dicts: where dates are keys and values are total count of citations    \n",
    "new_dict={}\n",
    "for ut in master_dict_traject:\n",
    "  #  print (ut, len(master_dict_traject[ut]))\n",
    "   \n",
    "    new_dict[ut]={}\n",
    "    dict_citating_paper_date=master_dict_traject[ut]\n",
    "       \n",
    "    cont_cit =1\n",
    "    sorted_dict = sorted(dict_citating_paper_date.items(), key=operator.itemgetter(1))\n",
    "    for pair in sorted_dict:        \n",
    "        date=pair[1]\n",
    "\n",
    "#         try:                  \n",
    "#             new_dict[ut][date] += cont_cit\n",
    "#         except KeyError:\n",
    "        new_dict[ut][date] = cont_cit\n",
    "        cont_cit +=1\n",
    "        \n",
    "#     print (sum(new_dict_ut_dict_citation_dates_counts[ut].values()))\n",
    "    #input()\n",
    "\n",
    "print (\"done\")\n",
    "    \n",
    "with open('../data/dict_ut_dict_dates_citations_trajectories_datetimes.pkl', 'wb') as handle:\n",
    "    pickle.dump(new_dict, handle, protocol = 2)\n",
    "print (\"written: ../data/dict_ut_dict_dates_citations_trajectories_datetimes.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## dont need for now\n",
    "\n",
    "# # Non-time-filtered regex-based exploration\n",
    "# import numpy as np\n",
    "# i=0\n",
    "# #k = np.log2(i)\n",
    "# bin_points = []\n",
    "# while i<16:\n",
    "#     k = 2**i\n",
    "#     i+=1\n",
    "#     bin_points.append(k)\n",
    "# bin_points\n",
    "\n",
    "\n",
    "# fig = plt.figure(figsize=(20,20))\n",
    "# fig.suptitle('Citation distribution across regex sections (normalized, log2 bins)', size=30)\n",
    "# bin_num = 8\n",
    "# subplot_num = 1\n",
    "# hist_list_raw = []\n",
    "# #hist_list = np.array([])\n",
    "# hist_list = []\n",
    "\n",
    "# for i in range(len(bin_points)-1):\n",
    "#     print(subplot_num)\n",
    "#     #print(i)\n",
    "#     if i == len(bin_points)-2:\n",
    "#         upper_bound = int(max(ref_df['cite_count'])+1)\n",
    "#         print(upper_bound)\n",
    "#     else:\n",
    "#         upper_bound = bin_points[i+1]\n",
    "#         #print('asdfasdf')\n",
    "        \n",
    "\n",
    "#     cite_count_log_list = ref_df.loc[ref_df['cite_count'].isin(range(bin_points[i], upper_bound))]\n",
    "#     title_text = str(bin_points[i]) + '-' + str(upper_bound-1) + ' citations'\n",
    "#     unique_refs = len(cite_count_log_list['reference_UT'].unique())\n",
    "#     unique_papers = len(cite_count_log_list['paper_UT'].unique())\n",
    "#     total_occs = len(cite_count_log_list)\n",
    "    \n",
    "#     ax = fig.add_subplot(4,4,subplot_num)\n",
    "    \n",
    "    \n",
    "#     hist1, bins1 = np.histogram(cite_count_log_list['sect_index'], bins= bin_num, range=[0,8])\n",
    "#     hist_list_raw.append(hist1)\n",
    "#     widths1 = np.diff(bins1)\n",
    "#     hist1 = hist1/float(len(cite_count_log_list))\n",
    "#     hist_list.append(hist1)\n",
    "#     #hist_list = np.append(hist_list, hist1, axis=1)\n",
    "#     ax.bar(bins1[:-1], hist1, widths1)\n",
    "    \n",
    "#     ax.title.set_text(title_text)\n",
    "    \n",
    "#     subtext1 = 'unique refs:  ' + str(unique_refs)\n",
    "#     subtext2 = 'total occs:   ' + str(total_occs)\n",
    "#     subtext3 = 'total papers:' + str(unique_papers)\n",
    "\n",
    "\n",
    "#     ax.text(7.3, 0.83, subtext1 + '\\n' + subtext2 + '\\n' + subtext3, style='italic', horizontalalignment='right',\n",
    "#         bbox={'facecolor':'white', 'alpha':0.6, 'pad':10})\n",
    "\n",
    "#     ax.grid()\n",
    "#     ax.set_ylim([0,1])\n",
    "#     ax.set_xlim([0,7])\n",
    "#     ax.set_xticks([0,1,2,3,4,5,6,7,8])\n",
    "#     subplot_num +=1\n",
    "    \n",
    "\n",
    "# # Plotting the distribution for all papers    \n",
    "# cite_count_all = ref_df.loc[(ref_df['cite_count']>=0)]\n",
    "# title_text = 'All citations'\n",
    "# unique_refs = len(cite_count_all['reference_UT'].unique())\n",
    "# unique_papers = len(cite_count_all['paper_UT'].unique())\n",
    "# total_occs = len(cite_count_all)\n",
    "\n",
    "# ax = fig.add_subplot(4,4,subplot_num)\n",
    "# hist1, bins1 = np.histogram(cite_count_all['sect_index'], bins= bin_num, range=[0,8])\n",
    "# hist_all_raw = hist1\n",
    "# widths1 = np.diff(bins1)\n",
    "# hist1 = hist1/float(len(cite_count_all))\n",
    "# hist_all = hist1\n",
    "# ax.bar(bins1[:-1], hist1, widths1)\n",
    "# ax.title.set_text(title_text)\n",
    "# subtext1 = 'unique refs:   ' + str(unique_refs)\n",
    "# subtext2 = 'total occs:   ' + str(total_occs)\n",
    "# subtext3 = 'total papers:   ' + str(unique_papers)\n",
    "\n",
    "# ax.text(7.3, 0.83, subtext1 + '\\n' + subtext2 + '\\n' + subtext3, style='italic', horizontalalignment='right',\n",
    "#     bbox={'facecolor':'white', 'alpha':0.6, 'pad':10})\n",
    "# ax.grid()\n",
    "# ax.set_ylim([0,1])\n",
    "# ax.set_xlim([0,7])\n",
    "# ax.set_xticks([0, 1, 2, 3, 4, 5, 6, 7, 8],['i','m','r','d','rd','c','mx','na',''])\n",
    "\n",
    "\n",
    "# fig.text(0.5, 0.08, 'Section Label', ha='center', size = 30)\n",
    "# fig.text(0.07, 0.55, 'Fraction of Total Occurences', va='center', rotation='vertical', size = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_cites = ref_df.loc[ref_df['cite_count'].isin(range(100, 110))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mid_cites['reference_UT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cites = ref_df[ref_df['cite_count']>=16384]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ref_df[ref_df['paper_UT'] == '000271022300001']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(high_cites['paper_UT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(high_cites['reference_UT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_df['paper_UT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ref_df['reference_UT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_cites = ref_df[ref_df['cite_count']>=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(any_cites['reference_UT'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any_cites['paper_UT'].unique()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = papers_con.collection.find_one({\"UT\":'000324515600133'})\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_traj = retrieve_trajectory('000089825700038')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = papers_con.collection.find_one({\"UT\":'000181970800023'})\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_ref_uts = list(df_pap_ref['reference_UT'].unique()[0:10])\n",
    "filtered_trajs = retrieve_multiple_trajectories(filtered_ref_uts, all_plos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_traj_dict = datetime_filler_traj(filtered_trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_traj_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_traj_dict['000187449400012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(range(len(dates)), key=lambda k: dates[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in test_traj_dict:\n",
    "    dates = []\n",
    "    count = []\n",
    "    plos_dates = []\n",
    "    plos_count = []\n",
    "    for index, cite in enumerate(test_traj_dict[paper]):\n",
    "        #print(type(test_traj_dict[paper][cite][0]))\n",
    "        if type(test_traj_dict[paper][cite][0]) != int:\n",
    "            dates.append(test_traj_dict[paper][cite][0])#if test_traj_dict[paper][cite][1] == True:    \n",
    "            count.append(index)\n",
    "            if test_traj_dict[paper][cite][1] == True:\n",
    "                #plos_dates.append(test_traj_dict[paper][cite][0])#if test_traj_dict[paper][cite][1] == True:    \n",
    "                plos_count.append(index)\n",
    "                \n",
    "    sorted_ind = sorted(range(len(dates)), key=lambda k: dates[k])\n",
    "    plos_dates = [dates[x] for x in plos_count]\n",
    "    plos_inds = [sorted_ind.index(i) for i in plos_count]\n",
    "\n",
    "    #print(plos_dates)\n",
    "    dates.sort()\n",
    "    #sorted(range(len(plos_dates)), key=lambda k: plos_dates[k])\n",
    "    #plos_dates.sort()\n",
    "    plt.plot(dates, count, color = 'b')\n",
    "    \n",
    "    plt.scatter(plos_dates, plos_inds, color='r')\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates[0,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_plos = ref_df['paper_UT'].unique()\n",
    "mid_cite_ref_uts = mid_cites['reference_UT'].unique()[0:10]\n",
    "mid_cites_trajs = retrieve_multiple_trajectories(mid_cite_ref_uts, all_plos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_cites_trajs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_df.loc[:,'paper_UT'] = ref_.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def section_regex_parser(ref_df):\n",
    "# # Using regular expressions to sort sections. Reach a concensus on section identification by scanning both section_title and section_title_alt\n",
    "#     import regex as re\n",
    "\n",
    "#     sect_index_dict = {'intro': 0, 'methods': 1, 'results': 2, 'disc': 3, 'res_disc':4, 'concl':5, 'mixed':6, 'na':7}\n",
    "\n",
    "#     intro_re = re.compile(r'(intro)')\n",
    "#     method_re = re.compile(r'(method)')\n",
    "#     results_re = re.compile(r'(results)')\n",
    "#     disc_re = re.compile(r'(disc)')\n",
    "#     concl_re = re.compile(r'(conclu)')\n",
    "#     backgr_re = re.compile(r'(backgr)')\n",
    "#     mater_re = re.compile(r'(mater)')\n",
    "#     count = 0\n",
    "#     judgement = []\n",
    "\n",
    "#     for i in range(len(ref_df)):\n",
    "#         count += 1\n",
    "#         if count%100000 == 0:\n",
    "#             print(count)\n",
    "#         sect = ref_df.iloc[i]['section_title']\n",
    "#         sect_alt = ref_df.iloc[i]['section_title_alt']\n",
    "#         sect_tag = -1\n",
    "#         sect_alt_tag = -1\n",
    "#         sect_final = -1\n",
    "\n",
    "#         if sect == None:\n",
    "#             sect_tag = 'na'\n",
    "#         else:\n",
    "#             if re.search(intro_re, sect.lower()) or re.search(backgr_re, sect.lower()):\n",
    "#                 sect_tag = 'intro'\n",
    "\n",
    "#             elif re.search(method_re,sect.lower()) or re.search(mater_re, sect.lower()):\n",
    "#                 if re.search(results_re, sect.lower()):\n",
    "#                     sect_tag = 'mixed'\n",
    "\n",
    "#                 elif re.search(disc_re, sect.lower()):\n",
    "#                     sect_tag = 'mixed'\n",
    "\n",
    "#                 else:\n",
    "#                     sect_tag = 'methods'\n",
    "\n",
    "#             elif re.search(results_re, sect.lower()):\n",
    "#                 if re.search(disc_re, sect.lower()):\n",
    "#                     sect_tag = 'res_disc'\n",
    "#                 else:\n",
    "#                     sect_tag = 'results'\n",
    "\n",
    "#             elif re.search(disc_re, sect.lower()):\n",
    "#                 sect_tag = 'disc'\n",
    "\n",
    "#             elif re.search(concl_re, sect.lower()):\n",
    "#                 sect_tag = 'concl'\n",
    "\n",
    "#             else:\n",
    "#                 sect_tag = 'na'\n",
    "\n",
    "\n",
    "\n",
    "#         if sect_alt == None:\n",
    "#             sect_alt_tag = 'na'\n",
    "#         else:\n",
    "#             if re.search(intro_re, sect_alt.lower()) or re.search(backgr_re, sect_alt.lower()):\n",
    "#                 sect_alt_tag = 'intro'\n",
    "\n",
    "#             elif re.search(method_re, sect_alt.lower()) or re.search(mater_re, sect_alt.lower()):\n",
    "#                 if re.search(results_re, sect_alt.lower()):\n",
    "#                     sect_alt_tag = 'mixed'\n",
    "\n",
    "#                 elif re.search(disc_re, sect_alt.lower()):\n",
    "#                     sect_alt_tag = 'mixed'\n",
    "\n",
    "#                 else:\n",
    "#                     sect_alt_tag = 'methods'\n",
    "\n",
    "#             elif re.search(results_re, sect_alt.lower()):\n",
    "#                 if re.search(disc_re, sect_alt.lower()):\n",
    "#                     sect_alt_tag = 'res_disc'\n",
    "#                 else:\n",
    "#                     sect_alt_tag = 'results'\n",
    "\n",
    "#             elif re.search(disc_re, sect_alt.lower()):\n",
    "#                 sect_alt_tag = 'disc'\n",
    "\n",
    "#             elif re.search(concl_re, sect_alt.lower()):\n",
    "#                 sect_alt_tag = 'concl'\n",
    "\n",
    "#             else:\n",
    "#                 sect_alt_tag = 'na'\n",
    "\n",
    "\n",
    "\n",
    "#         if sect_tag == sect_alt_tag: # Confident on label\n",
    "#             sect_final = sect_tag\n",
    "#             #ref_df.iloc[i]['regex_sect_index'] = sect_index_dict[sect_tag]\n",
    "\n",
    "#         else:\n",
    "#             if sect_tag == 'na':\n",
    "#                 sect_final = sect_alt_tag\n",
    "#                 #ref_df.iloc[i]['regex_sect_index'] = sect_index_dict[sect_alt_tag]\n",
    "#             elif sect_alt_tag == 'na':\n",
    "#                 sect_final = sect_tag\n",
    "\n",
    "#             elif sect_tag == 'mixed':\n",
    "#                 sect_final = sect_alt_tag\n",
    "\n",
    "#             elif sect_alt_tag == 'mixed':\n",
    "#                 sect_final = sect_tag\n",
    "\n",
    "#             elif sect_alt_tag == 'concl': # Conclusion is a plos-based standard that includes discussion/results and discussion\n",
    "#                 sect_final = sect_tag\n",
    "#             else:\n",
    "#                 sect_final = sect_alt_tag\n",
    "\n",
    "#         judgement.append(sect_final)\n",
    "#     return judgement\n",
    "#         #ref_df.iloc[i]['regex_sect_index'] = sect_index_dict[sect_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# judgement = section_regex_parser(ref_df)\n",
    "# judgement_2 = []\n",
    "# sect_index_dict = {'intro': 0, 'methods': 1, 'results': 2, 'disc': 3, 'res_disc':4, 'concl':5, 'mixed':6, 'na':7}\n",
    "# for i in judgement:\n",
    "#     judgement_2.append(sect_index_dict[i])\n",
    "# ref_df['regex_sect_index'] = judgement_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
